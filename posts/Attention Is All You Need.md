---
title: Attention Is All You Need
date: 2021-05-21 10:35:00
tags:
 - CS
 - transformer
 - NeurIPS 2017
typora-root-url: ..
typora-copy-images-to: ..\img\transformer
---

  Transformer 注意力就是您所需要的
 ---

会议： NeurIPS 2017

论文地址：https://arxiv.org/abs/1706.03762

[TOC]

## 摘要

主流的序列转换模型多是基于复杂的循环或卷积神经网络，网络包括编码器和解码器。性能最好的模型还通过注意机制连接编码器和解码器。我们提出了一个新的简单且完全基于注意力机制的网络结构，Transformer，摒弃了循环和卷积。在两个机器翻译任务上的实验表明，这些模型具有更高的并行性，更少的训练时间。我们的模型在WMT 2014 Englishto-German 的翻译任务中达到28.4 BLEU，比现有的最佳结果（包括集成模型）提高了2 BLEU以上。在WMT 2014 English-to-French翻译任务中，我们的模型在8个GPU上训练3.5天后，达到了新的单模型BLEU 41.8的最新分数，这只相当于文献中最佳模型训练成本的一小部分。通过将Transformer成功地应用于大规模和有限训练数据下的英语选区分析，证明了Transformer具有良好的泛化能力。

<!--more-->



## 1 简介

循环神经网络，特别是长-短期记忆(LSTM)[13]和门控循环(GRN))[7]神经网络，已经被确定为序列建模和转换问题（如语言建模和机器翻译）的最新方法[35，2，5]。自那以后，许多努力继续推动循环语言模型和编解码器架构的(性能)边界[38，24，15]。

循环模型通常沿着输入序列和输出序列的符号位置进行因子计算。将序列中每位置数据与RNN计算时刻中的步骤对齐，它们生成一系列隐藏状态$h_t$，作为先前隐藏状态$h_{t−1}$和位置t输入的函数。这种固有的顺序性质阻碍了训练样本中的并行化，这(并行)在较长的序列长度下变得很关键，因为内存限制了跨样本的批处理。最近的工作通过因子分解技巧[21]和条件计算[32]在计算效率方面取得了显著的改进，同时也提高了后者的模型性能。然而，顺序计算的基本限制仍然存在。

注意机制已经成为各种任务中序列建模和转换模型的一个重要组成部分，允许建模依赖性[2，19]，而不考虑其在输入或输出序列中的距离。然而，在除少数情况[27]外的所有情况下，这种注意机制都与循环网络结合使用。

在这项工作中，我们提出了Transformer，一种避免循环结构的模型架构，而完全依赖于一种注意机制来绘制输入和输出之间的全局依赖关系。Transformer允许更明显的并行化，并可以在在8个P100 GPU上经过训练后，只12小时达到新的最先进的翻译效果。



## 2 背景

以减少顺序计算目的为基础形成了扩展神经GPU〔16〕、ByteNet〔18〕和ConvS2S〔9〕，所有这些都使用卷积神经网络作为基本构建块，并行的计算输入输出所有位置的隐藏表示。在这些模型中，将两个任意输入或输出位置的信号关联起来所需的操作数随着位置之间的距离而增长，ConvS2S是线性的，ByteNet是对数的。这使得学习远距离位置之间的依赖关系变得更加困难[12]。在Transformer中，这被减少到一个恒定的操作数，尽管由于平均注意加权位置(averaging attention-weighted positions)而降低了有效分辨率，我们用多头部注意抵消了这种影响，如第3.2节所述。

自注意，有时被称为内部注意，是一种注意机制，将单个序列的不同位置联系起来，以计算序列的表示形式。自我注意在阅读理解、抽象总结、文本蕴涵和学习任务无关的句子表征等任务中得到了成功的应用[4,27,28,22]。

端到端记忆网络是基于一种循环注意机制而不是序列对齐的循环，并且在简单的语言问答和语言建模任务中表现良好[34]。

然而，据我们所知，Transformer是第一个完全依靠自注意来计算其输入和输出表示的转换模型，而不使用序列对齐RNN或卷积。在下面的章节中，我们将描述Transformer，自注意的动机，并讨论其相对于[17,18]和[9]等模型的优势。



## 3 模型结构

大多数竞争神经序列转换模型都有编码器-解码器结构[5，2，35]。这里，编码器将符号表示 $(x_1, …, x_n)$ 的输入序列映射到连续表示 $z=(z_1, …, z_n)$ 的序列。给定z，然后解码器生成符号的输出序列 $(y_1, …, y_m)$ ，一次生成一个元素。在每一步中，模型都是自递归的[10]，在下一步生成时，使用先前生成的符号作为额外的输入。

Transformer遵循这个整体架构，使用堆叠的自注意和逐点，编码器和解码器使用全连接层连接，分别如图1的左半部分和右半部分所示。

![1706](/img/transformer/1706.jpg)

### 3.1 编码器和解码器堆叠

**编码器**：编码器由N=6个相同的层组成。每层有两个子层。第一子层是多头自注意机制，第二子层是简单的、位置相关的全连接前馈网络。我们在两个子层的每一个子层周围使用残差连接[11]，然后使用层归一化[1]。也就是说，每个子层的输出是 $LayerNorm(x+Sublayer(x))$ ，其中 $Sublayer(x)$ 是子层本身实现的函数。为了便于使用残差连接，模型中的所有子层和嵌入层都生成维数为 $d_{model}=512$ 的输出。

**解码器**：解码器也由N=6个相同层的堆叠组成。除了在每个编码器层中都存在的两个子层之外，解码器还插入第三个子层，该子层对编码器堆叠的输出执行多头部注意力。与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化。我们还修改了解码器堆叠中的自注意子层，以防止当前位置关注后续位置。这种掩码与输出嵌入偏移一个位置相结合，确保位置$i$的预测只能依赖于位置小于$i$的已知输出。



### 3.2 注意力

注意力函数可以描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。输出作为值的加权和进行计算，其中分配给每个值的权重由查询与相应键的兼容性(相似性)函数计算。

![image-20210521105517778](/img/transformer/image-20210521105517778.png)

`图2：（左）缩放点乘注意力。（右）多头注意力由几个平行运行的注意力层组成。`

#### 3.2.1  缩放点积注意力

我们称我们的特别注意力为“标度点积注意力”（图2）。输入包括$d_k$维度的查询和键，以及$d_v$维度的值。我们用所有键计算与查询的点积，将每个点积除以$\sqrt{d_k}$，然后应用$softmax$函数获得值的权重。



在实践中，我们同时计算一组查询的注意函数，并将其打包到矩阵Q中。键和值也被打包到矩阵K和V中。我们将输出矩阵计算为：


$$
Attention(Q, K, V) = softmax \left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$


最常用的两个注意函数是加性注意力(additive attention)[2]和点积（多重复制）注意力。点积注意力与我们的算法相同，[我们]除了比例因子$\frac{1}{\sqrt{d_k}}$。加性注意力(additive attention)利用一个具有单个隐层的前馈网络来计算相容函数。虽然两者在理论复杂度上相似，但由于可以使用高度优化的矩阵乘法码来实现，因此在实践中，点积关注速度更快，空间效率更高。

而对于较小的$d_k$值，这两种机制表现相似，对于$d_k$值，加法注意优于点积注意[3]。我们怀疑对于较大的 $d_k$ 值，点积在数量级上增长很大，将softmax函数推到梯度非常小的区域^4。为了抵消这种影响，我们用$\frac{1}{\sqrt{d_k}}$缩放点积。

#### 3.2.2 多头注意力

取代使用 $d_{model}$ 维的键、值和查询执行单一注意函数，我们发现，使用不同的线性投影将查询、键和值线性投影h次，分别投影到 $d_k$、$d_k$ 和 $d_v$ 维是有益的。在查询、键和值的每个投影版本上，我们并行执行注意函数，产生 $d_v$ 维输出值。它们被连接起来并再次投影，从而得到最终值，如图2所示。

多头部注意使得模型能够在不同位置，连带地关注来自不同表示子空间的信息。单注意头，平均值会抑制这一点。
$$
\begin{aligned}
\operatorname{MultiHead}(Q, K, V) &=\text { Concat }\left(\text { head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\

\text { where head }_{\mathrm{i}} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)
\end{aligned}
$$
其中投影为参数矩阵  $
W_{i}^{Q} \in \mathbb{R}^{d_{\text {model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text {model }} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text {model }} \times d_{v}}, W^{O} \in \mathbb{R}^{ {hd_v} \times d_{\text{model}} }
$  

在这项工作中，我们采用 $h=8$ 平行注意层，或头。对于每一层，我们使用 $d_k=d_v=d_{model}/h=64$ 。由于每个头的维数减小，总的计算量与全维度单头注意力的计算量相似。

#### 3.2.3 注意力在我们模型中的应用

Transformer以三种不同的方式使用多头注意：

+ 在“编码器-解码器注意”层中，查询来自前一个解码器层，而记忆键和值来自编码器的输出。这使得解码器中的每个位置都可以注意输入序列中的所有位置。这模仿了典型的编码器-解码器在序列到序列模型中的注意机制，如[38，2，9]。
+ 编码器包含自注意层。在自注意层中，所有的键、值和查询都来自同一个地方，在本例中为编码器中前一层的输出。编码器中的每个位置都可以关注编码器前一层中的所有位置。
+ 类似地，解码器中的自注意层允许解码器中的每个位置关注到解码器中所有位置，直到并且包括当前位置。我们需要防止解码器中的信息流向左流动，以保持自回归特性。我们通过屏蔽softmax输入中与非法连接相对应的所有值（设置为−∞），实现了这种缩放点积注意力。见图2。



### 3.3 位置前馈网络

除了注意力子层之外，我们的编码器和解码器中的每一层都包含一个完全连接的前馈网络，该网络单独且相同地应用于每个位置。这包括两个线性变换，中间有一个ReLU激活函数。
$$
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
$$


虽然线性变换在不同的位置上是相同的，但它们在层与层之间使用不同的参数。另一种描述前馈网络的方法是两个核大小为1的卷积。输入和输出的维数为 $d_{model}=512$ ，内层的维数为 $d_{ff}=2048$ 。



### 3.4 嵌入和Softmax

与其他序列转换模型类似，我们使用可学习的嵌入将输入标记和输出标记转换为维度 $d_{model}$ 的向量。我们还使用常用的可学习线性变换和softmax函数将解码器输出转换为预测的下一个词符(next-token)概率。在我们的模型中，我们在两个嵌入层和pre-softmax线性变换之间共享相同的权重矩阵，类似于[30]。在嵌入层，我们将这些权重乘以 $\sqrt{d_{model}}$ 。



### 3.5 位置编码

由于我们的模型不包含循环和卷积，为了使模型能够利用序列的顺序，我们必须注入一些关于符号在序列中的相对或绝对位置的信息。为此，我们将“位置编码”添加到编码器和解码器堆栈底部的输入嵌入中。位置编码与嵌入具有相同的维度$d_{model}$，因此可以将两者相加。有许多位置编码的选择，学习的和固定的[9]。

`表1：不同层类型的最大路径长度、每层复杂度和最小顺序操作数。n是序列长度，d是表示维数，k是卷积的核大小，r是限制自我注意的邻域大小。`

![image-20210521111900969](/img/transformer/image-20210521111900969.png)

在这项工作中，我们使用不同频率的正弦和余弦函数：
$$
PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}})
$$

$$
PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})
$$



其中pos是位置，i是尺寸。也就是说，位置编码的每个维度对应于一个正弦曲线。波长呈几何级数从2π到10000·2π。我们之所以选择这个函数，是因为我们假设它可以让模型很容易地通过相对位置来学习，因为对于任何固定的偏移量k，$PE_{pos+k}$ 可以表示为 $PE_{pos}$ 的线性函数。

我们还尝试使用学习到的位置嵌入[9]，发现这两个版本产生了几乎相同的结果（见表3第（E）行）。我们选择正弦版本，因为它可能允许模型外推序列长度比训练中遇到的更长。



## 4 为什么(使用)自注意力

在这一节中，我们将自注意层与循环和卷积层的各个方面进行比较，循环和卷积层通常用于映射变长序列的符号表示 $(x_1, …, x_n)$ 到另一个等长序列 $(z_1, …, z_n)$ ，其中 $x_i, z_i \in \mathbb{R}^d，$  例如在一个典型的序列转换编码器或解码器的隐藏层。促使我们使用自注意的动机，我们考虑了三个条件。

第一是每层的总计算复杂度。另一个是可以并行化的计算量，以所需的最小序列操作数来衡量。

第三个是网络中远程依赖之间的路径长度。在许多序列转换任务中，学习长程依赖是一个关键的挑战。影响学习这种依赖关系能力的一个关键因素是前向和后向信号在网络中必须经过的路径长度。输入和输出序列中任何位置组合之间的路径越短，就越容易学习长期依赖关系[12]。因此，我们也比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。

如表1所示，自注意层连接所有位置都使用常数个顺序执行操作，而循环层需要 $O(n)$ 个顺序操作。在计算复杂性方面，当序列长度n小于表示维数d时，自注意层比循环层快，这通常是机器翻译中最先进模型使用的句子表示的情况，例如词条[38]和字节对[31]表示。为了提高包含很长序列的任务的计算性能，可以将自我注意限制为只考虑以相应输出位置为中心的输入序列中大小为 $r$ 的邻域。这会将最大路径长度增加到$O(n/r)$  。我们计划在今后的工作中进一步研究这种方法。

核宽 $k<n$ 的单个卷积层并不连接所有的输入和输出位置对。这样做需要一个 $O(n/k)$ 卷积层的堆栈（对于连续的核contiguous kernels），或者$O(logk(n))$  的堆栈(对于空洞卷积dilated convolutions）[18]，增加网络中任意两个位置之间的最长路径的长度。卷积层通常比循环层的成本高出 $k$ 倍。然而，可分离卷积(Separable convolutions)[6]将复杂性大大降低到$O(k·n·d+n·d^2)$ 。然而，即使在 $k=n$ 的情况下，可分离卷积的复杂度也等于我们在模型中采用的自注意层和逐点前馈层的组合。

作为副作用，自注意可以产生更多可解释的模型。我们从我们的模型中检查注意分布，并在附录中给出和讨论示例。不仅个体的注意力头清楚地学会执行不同的任务，更多的头似乎表现出与句子的句法和语义结构有关的行为。



## 5 训练

本节描述了模型的训练机制。

### 5.1 训练数据和批处理

我们在标准WMT 2014 English-German数据集上进行了训练，该数据集由大约450万个句子对组成。句子是用字节对编码的[3]，它有一个大约37000个标记的共享源-目标词汇表。对于English-French，我们使用了更大的WMT 2014 English-French数据集，该数据集由3600万个句子和拆分标记组成，包含32000个单词的词汇量[38]。句子对按大致的序列长度分批排列在一起。每个训练批包含一组句子对，其中包含大约25000个源标记和25000个目标标记。



### 5.2 硬件和预设

我们在一台有8个NVIDIA P100 GPU的机器上训练我们的模型。对于使用本文中描述的超参数的基础模型，每个训练步骤大约需要0.4秒。我们对基础模型进行了总共10万steps或12小时的训练。对于我们的大型模型（在表3的底行中描述），step时间是1.0秒。这些大模型被训练了30万步（3.5天）。



### 5.3 优化器

我们使用了Adam优化器[20]，其中 $β1=0.9, β2=0.98, \sigma =10−9$ 。在整个训练过程中，我们根据以下公式改变了学习率：
$$
\text { lrate } = d_{\text {model }}^{-0.5} \cdot \min \left(\text { step\_num }^{-0.5}, \text { step\_num } \cdot \text { warmup\_steps }^{-1.5}\right)
$$
这相当于在第一个预热步(warmup_steps)训练步骤中线性地增加学习率，然后按步骤数的平方反比成比例地降低学习率。我们使用warmup_steps=4000。



### 5.4 正则化

在训练期间，我们采用了三种正则化方法：

**残差排除(Residual Dropout)**：我们在将每个子层的输出使用dropout(随机删除)[33]，在输出加到子层输入和归一化之前。此外，我们对编码器和解码器堆栈中的嵌入和位置编码的和也应用dropout。对于基本模型，我们使用$P_{drop}=0.1$ 的速率。

`表2 Transformer在English-German和English-French newstest2014的测试中取得了比以前最先进的模型更好的BLEU分数，而训练成本仅占一小部分。`

![image-20210521113103705](/img/transformer/image-20210521113103705.png)



**标签平滑**：在训练过程中，我们使用了值 $e_{ls} = 0.1$  [36]的标签平滑。这伤害了困惑度，因为模型学会了更加不确定，但提高了准确性和BLEU分数。

## 6 结果

### 6.1 机器翻译

在WMT 2014 English-German翻译任务中，大transformer模型（表2中的Transformer(big)）比之前报道的最佳模型（包括整体）的BLEU值高出2.0以上，建立了一个新的最先进的BLEU值28.4。这个模型的配置列于表3的底行。在8个P100 GPU上训练3.5天。即使是我们的基础模型超过了所有以前出版的模型和合奏，在训练代价只使用任何竞争模型的一小部分。

在WMT 2014 English-French的翻译任务中，我们的大模型达到了41.0的BLEU分数，优于之前发布的所有单个模型，不到之前最先进模型训练成本的1/4。为 English-French训练的Transformer(big)模型使用的dropout率$P_{drop}=0.1$，而不是0.3。

对于基本模型，我们使用一个单头模型，该模型通过平均最后5个checkpoints获得，这些checkpoints以10分钟的间隔写入。对于大型模型，我们平均了最后20个checkpoints。我们使用波束搜索，波束大小为4，长度惩罚 $α=0.6$ [38]。这些超参数是在开发集上进行实验后选择的。我们将推断过程中的最大输出长度设置为输入长度+50，但尽可能提前终止[38]。



表2总结了我们的结果，并将我们的翻译质量和训练成本与文献中的其他模型架构进行了比较。我们通过乘以训练时间、使用的GPU数量和每个GPU的持续单精度浮点容量来估计用于训练模型的浮点运算的数量。

### 6.2 模型变量

为了评估Transformer不同组件的重要性，我们以不同的方式改变了我们的基本模型，在开发集newstest2013上测量 English-German翻译的性能变化。我们使用了前一节中描述的波束搜索，但是没有checkpoints平均。我们在表3中给出了这些结果。

在表3（A）行中，我们改变了注意头的数量以及注意键和值维度，保持计算量不变，如第3.2.2节所述。虽然单头注意力比最佳设置差0.9 BLEU，但过多的头质量也会下降。



`表3：Transformer结构的变化。未列出的值与基本模型的值相同。所有指标都在English-German翻译开发集newstest2013上。根据我们的字节对编码，列出的困惑是每个单词的，不应与每个单词的困惑进行比较。`

![image-20210521113508486](/img/transformer/image-20210521113508486.png)



`表4：Transformer可以很好地推广到英语选区分析（结果见WSJ第23节）`

![image-20210521113658606](/img/transformer/image-20210521113658606.png)



在表3行（B）中，我们观察到减小注意键大小 $d_k$ 会损害模型质量。这表明确定兼容性并不容易，而且比点乘更复杂的兼容性功能可能是有益的。我们在第（C）行和第（D）行中进一步观察到，正如预期的那样，更大的模型更好，而dropout对于避免过拟合非常有帮助。在第（E）行中，我们将正弦位置编码替换为学习的位置嵌入[9]，并观察到与基本模型几乎相同的结果。

### 6.3 英语选区分析

为了评估这个Transformer是否可以推广到其他任务，我们进行了英语选区分析实验。这项任务提出了具体的挑战：产出受到强大的结构约束，而且大大长于投入。此外，RNN序列到序列模型还不能在小数据区获得最先进的结果[37]。

我们在Penn Treebank的华尔街日报（Wall Street Journal，WSJ）部分[25]上训练了一个4层的Transformer， $d_{model}=1024$　，大约有40K个训练句子。我们也在半监督的环境中训练它，使用了更大的高置信度和BerkleyParser语料库，来自大约1700万个句子[37]。我们使用了16K的词汇表用于仅限WSJ的设置，32K的词汇表用于半监督设置。

我们只进行了少量的实验，在第22节的发展集上选择辍学、注意力和剩余（第5.4节）、学习率和波束大小，所有其他参数从English-German的基础翻译模型保持不变。在推理过程中，我们将最大输出长度增加到输入长度+300。我们使用了21和α=0.3的波束大小，用于WSJ和半监督设置。

我们在表4中的结果表明，尽管缺乏特定于任务的调整，我们的模型表现得非常好，除了循环神经网络语法外，产生的结果比所有以前报道的模型都好[8]。

与RNN序列到序列模型[37]相比，Transformer的性能优于Berkeley解析器[29]，即使只在WSJ训练集上训练40K个句子。

## 7 结论

在这项工作中，我们提出了第一个完全基于注意的序列转换模型Transformer，它用多头自注意取代了编解码结构中最常用的循环层。

对于翻译任务，Transformer的训练速度比基于循环层或卷积层的架构快得多。在WMT 2014 English-German和WMT 2014 English-French的翻译任务中，我们实现了一个新的水平。在前一个任务中，我们的最佳模型甚至比所有先前报道的集成模型都要好。

我们对基于注意力模型的未来感到兴奋，并计划将其应用于其他任务。我们计划将Transformer扩展到涉及文本以外的输入和输出模式的问题，并研究局部、受限的注意机制，以有效地处理图像、音频和视频等大量输入和输出。减少世代的连续性是我们的另一个研究目标。

我们用来训练和评估模型的代码可以获取在https://github.com/tensorflow/tensor2tensor。



## 注意力可视化

![image-20210521114450660](/img/transformer/image-20210521114450660.png)

`图3：注意机制的一个例子，在编码器中遵循长距离依赖，在第5层（共6层）自我注意。许多注意力集中的人注意到动词“making”的一个遥远的依赖关系，完成短语“使…更困难”。这里的注意仅限于“使”一词。不同的颜色代表不同的头。最好看彩色。`



![image-20210521115133027](/img/transformer/image-20210521115133027.png)

`图4：两个注意头，也在第5层的6，显然涉及回指决议。上图：5号头全神贯注。底部：将注意力从注意力头5和6的“its”字中分离出来。注意，这个词的注意力非常敏锐。`



![image-20210521115353615](/img/transformer/image-20210521115353615.png)

`图5：多头注意力集中的人表现出的行为似乎与句子的结构有关。我们在上面给出了两个这样的例子，来自6层第5层编码器自我注意的两个不同的头部。很明显，这些头学会了执行不同的任务。`