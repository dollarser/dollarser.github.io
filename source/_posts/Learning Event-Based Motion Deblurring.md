---
title: Learning Event-Based Motion Deblurring
date: 2021-04-22 13:38:00
tags:
 - CS
 - CVPR2020
typora-root-url: ..
typora-copy-images-to: ..\img
---

## 学习基于事件的运动去模糊

会议：CVPR 2020

地址：https://arxiv.org/abs/2004.05794

## 摘要

由于模糊过程中丢失了大量的运动信息，从运动模糊图像中恢复清晰的视频序列是一个高度[不适定问题](https://baike.baidu.com/item/不适定问题)。然而，对于基于事件的相机，快速运动可以在高时间率上作为事件被捕捉，从而提出了探索有效解决方案的新机遇。在本文中，我们从基于事件的运动去模糊的序列表述开始，然后说明如何使用新颖的端到端深度架构来实现其优化。所提出的架构是一个卷积循环神经网络，有原则的整合了全局和局部尺度的视觉和时间知识。为了进一步改进(图像的)重建，我们提出了一种可微的定向事件过滤模块，可以有效地从事件流中提取丰富的先验边界。我们在合成的GoPro数据集和新引入的使用DAVIS240C相机捕获的大型数据集上进行了大量的实验。我们提出的方法达到了最先进的重建质量，并更好地处理现实世界的运动模糊。

<!--more-->



## 1. 概述

运动模糊通常是由于现代相机传感器需要曝光时间，在此期间，场景被记录在不同的时间戳，并累积成平均(模糊)信号。这个过程的反问题被称为“去模糊”，它揭示了运动模糊图像背后的场景动态，并生成一系列复原的清晰场景，在计算机视觉中仍然具有挑战性。虽然简单的运动模式(如相机抖动)已经被很好地建模，但是在现实世界中形成更复杂的运动模式却要困难得多。

为了模拟一般的运动模糊，最近的深度学习方法提出通过观察大量的清晰图像及其模糊版本来恢复模糊图像。尽管他们在某些场景中取得了成功，但他们可能无法合理地重建严重的运动模糊场景(如图1)，这是常见的手持，车辆或无人机装备的相机。在这种情况下，由于时间顺序和视觉信息的显著丢失，对场景细节进行幻化几乎是不可能的。

![image-20210422151736600](/img/image-20210422151736600.png)

`图1 我们方法的动机。严重的运动模糊图像（a）仅通过最先进的深度结构[50]很难通过观察其模棱两可的外观进行模糊处理[50](c)。尽管事件(b)提供了密集的时间线索，但由于事件(d)的嘈杂，物理重建方法[31]仍然呈现出未解决的模糊。我们深度运动去模糊技术可从不完美的图像和事件中恢复可能的细节(e)。`

本工作在数据捕获阶段采用事件像机来缓解这一问题，而不是单纯依靠计算架构。事件相机是一种受生物学启发的传感器，擅长记录像素强度(称为事件)的变化，具有微秒精度和非常低的功耗。这类传感器的混合模型(如[5])允许用图像在时间上校准事件。因此，这些数据自然地编码了密集的时间信息，可以促进运动去模糊。如图1 (a)和(b)所示，尽管图像经历了明显的模糊，但伴随的事件在时间上是密集的，并显示出清晰的场景移动模式。

尽管基于事件的运动去模糊的潜力很大，但一个关键问题是，事件是有损的和有噪声的信号，只有当像素强度变化到一定的阈值时才会触发，该阈值随着场景情况[35]的变化而变化。这种离散和不一致的采样使得纹理和对比度难以恢复。如图1 (d)所示，目前最先进的物理去模糊方法[31]仍然难以合理地重建图像。我们的解决方案是将深度学习的先验知识插入基于事件的去模糊过程中，从而超越数据的不完善性。

具体来说，这项工作从基于事件的去模糊的序列表述开始。通过使用深度网络重新诠释其优化，我们提出了一种可端到端训练的新颖循环架构。对于每个时间步，从之前的重建和局部时间事件得到粗重建。然后，通过网络预测来提供精细的细节，并以全局和局部尺度上的外观和时间提示为指导。为了进一步提高重建质量，我们提出了一种可微分Directional Event Filtering(方向事件滤波: DEF)模块，它有效地聚合了事件所揭示的运动边界，并产生清晰的去模糊先验效果。为了评估所提出的方法，我们编制了一个使用DAVIS240C相机[5]捕获的大型户外数据集。在这个数据集和合成的GoPro数据集[25]上进行的大量实验表明，所提出的方法优于各种最新的方法，无论是基于图像的还是基于事件的，并且能够更好地处理真实世界的运动模糊。

本文的贡献总结如下。1)我们提出了一种新的基于事件的运动去模糊循环深度架构，在两个大型基准上实现了最先进的结果。2)我们提出了定向事件过滤，从事件中生成清晰的边界，用于运动去模糊。3)我们使用真实的运动模糊编制了一个新的事件数据集，以促进未来的研究。

## 2. 相关工作

**盲去运动模糊** 目的是在不知道模糊内核的情况下解决模糊图像。早期的研究设计了各种模糊感知指标，如颜色通道统计[29,47]、块重复[22]和”离群“图像信号[8]来定义潜在图像的先验。一些研究提出从数据中学习运动核[39,28]、恢复函数[45,11]和图像先验[55,42]。由不同对象复合而成的更复杂的运动模式也被解决[16,37]。更丰富的先验知识(例如场景几何)被证明是有用的[30,32]。

最近的一个趋势是使用深度神经网络来处理所有复杂的运动去模糊。提出了各种有效的网络设计，包括扩大接收野[52]、多尺度融合[25,27]、特征分离[26]和循环细化[44]。也有研究将模糊图像的运动动力学解码为清晰的视频序列[15]。尽管取得了这些进步，但现实世界中照明，纹理和运动的大量组合（在模糊图像中严重丢失）仍然难以合理恢复。

**事件相机** 是一种特殊的传感器，它在微秒级检测场景的强度变化，功耗较小。它们在各种视觉任务中都有应用，如视觉跟踪[34,23]、立体视觉[54,1]和光流估计[20,48]。一个相关的分支是探索损坏的事件信号，以恢复高帧率图像序列[38,24,40] 。最近，Pan等人[31]用一个二重积分模型制定了基于事件的运动模糊。然而，事件摄像机的嘈杂的硬采样机制往往会引入强烈的累积噪声和场景细节/对比度的损失。

这项工作分享了最近有关事件到视频翻译的研究[33,17,36]，这些研究通过从数据中学习可能的细节，超越了不完美的事件采样。[33]解决了未来的帧预测问题，[17，36]则根据局部运动线索以流方式将事件转换为合理的强度图像。 相反，这项工作探索了长期的局部外观/运动线索以及新颖的事件边界先验，以解决运动模糊问题。



## 3. 学习基于事件的运动去模糊

给定一个运动模糊图像 $\overset{\_}{I} $ ，我们的目标是恢复一个有T帧的清晰视频序列，$\mathbb{I} = \{I_i\}^T_{i=1}$。我们假设在曝光期间，混合图像-事件传感器也捕获了一系列事件$E_{1\sim T}$，其中波浪线表示时间间隔。每个事件 $\mathcal{E}\in \mathbb{E}_{1\sim T}$ 的形式为$\mathcal{E}_{x,y,t}$，即在图像坐标(x, y)和时间点$t\in[1,T]$的触发(激活)。注意这里t不需要是整数，也可以是小数，由于事件相机的高时间分辨率(即微秒级)。对 $\mathcal{E}_{x,y,t}$ 记录一个极性 $p_{x,y,t}$ ，表示局部强度的变化。正式定义参考文献[19,5]，公式(1)：


$$
p_{x, y, t}=\left\{\begin{array}{l}
+1, \text { if } \log \left(\frac{\mathcal{I}_{t}(x, y)}{\mathcal{I}_{t-\Delta t}(x, y)}\right)>\tau \\
-1, \text { if } \log \left(\frac{\mathcal{I}_{t}(x, y)}{I_{t-\Delta t}(x, y)}\right)<-\tau
\end{array}\right.
$$



公式(1)表明，时刻t的瞬间图像，即$\mathcal{I}_t$，在一个小时间周期$\Delta t$内像素强度变化到阈值$\pm\tau$时触发事件。不失一般性，我们假设当$\log \left(\frac{\mathcal{I}_{t}(x, y)}{I_{t-\Delta t}(x, y)}\right)$在$[-τ, τ]$时，$p_{x,y,t}$取零。对于相邻的潜在图像$\mathcal{I}_i$和$\mathcal{I}_{i-1}$，可以得到如下关系，公式(2):


$$
\mathcal{I}_{i}(x, y) \approx \mathcal{I}_{i-1}(x, y) \cdot \exp \left(\tau \int_{t=i-1}^{i} p_{x, y, t} \mathbb{1}\left(\mathcal{E}_{x, y, t}\right) d t\right)
$$


如果事件 $\mathcal{E}_{x,y,t}$ 存在，则指示函数 $\mathbb{1}(·)$ 等于1，否则为0。

需要注意的是，当 $\Delta t,\tau\rightarrow 0$ 时上式(2)的近似误差越来越小，这意味着根据公式(1)是稠密事件。然而，由于受各种噪声的影响而不一致的$\tau$，这种近似在实践中大部分是不够的，导致对比度和细节的丢失。为了解决这个问题，我们提出了一个联合框架，通过重新诠释一个序列去模糊过程，来学习从数据中重建清晰的图像。

**深度序列去模糊** 事件辅助去模糊可以用**最大后验**表示，公式(3): 
$$
\mathbb{I}^* = arg\ \underset{\mathbb{I}}{max}\ P(\mathbb{I}| \overset{\_}{\mathcal{I}},\mathbb{E}_{1\sim T})
$$
读者注: 这里$\mathbb{I}$表示模糊图像生成的图像序列，$\overset{\_}{\mathcal{I}}$表示模糊图像，$\mathbb{E}_{1\sim T}$表示事件序列

为了解决组合问题(3)，我们作以下简化。对于联合后验 $P(\mathbb{I}| \overset{\_}{\mathcal{I}},\mathbb{E}_{1\sim T})$，我们利用相邻潜在图像之间的时间关系(2)，并假设一个马尔可夫链模型，公式(4):


$$
P(\mathbb{I}| \overset{\_}{\mathcal{I}},\mathbb{E}_{1\sim T}) \approx P(\mathcal{I}_T| \overset{\_}{\mathcal{I}},\mathbb{E}_{1\sim T})\times\prod_{i=1}^{T-1}P(\mathcal{I}_i| \mathcal{I}_{i+1},\overset{\_}{\mathcal{I}},\mathbb{E}_{1\sim T})
$$
其中， $P(\mathcal{I}_i|\mathcal{I}_{i+1},\overset{\_}{\mathcal{I}},\mathbb{E}_{1\sim T}) = P(\mathcal{I}_i|\mathcal{I}_{i+1},\overset{\_}{\mathcal{I}},\mathbb{E}_{i \sim i+1})$ ，具有马尔可夫假设。请注意，这个简化的模型首先估计$\mathcal{I}_T$(读者注：最后一张图)，然后按反向顺序执行序列重建。根据贝叶斯规则，一个最大化反向重构的步骤为，公式(5)：


$$
\mathcal{I}^*_i = arg\ \underset{\mathcal{I}_i}{max}\ P(\mathcal{I}_{i+1},\overset{\_}{\mathcal{I}}, \mathbb{E}_{i\sim i+1}| \mathcal{I}_i) P(\mathcal{I}_i)
$$
在这里，前面的项 $P(\mathcal{I}_i)$ 表示潜在图像的期望分布，类似在最近基于事件的图像重建中的 $\mathcal{l}_1$ 梯度[3]或流形平滑[24]。为了对似然项建模，我们假设有一个从以前的重建得到的初步估计，通过(2):


$$
\hat{\mathcal{I}}_i = \mathcal{I}_{i+1}\odot exp(-\tau S^i_{i+1})
$$
式中，对于 ${\forall}x,y$ , 有  $\mathcal{S}^i_{i+1}(x,y) = \int_{t=i}^{i+1}p_{x,y,t} 1(\mathcal{E}_{x,y,t})dt$ ，其中$\odot$表示哈达玛(Hadamard)积。由于时间间隔很小，我们假设的常数$\tau$，只引入了很小的漂移(注:可能指偏差，误差)，并提供良好的初始化。为了求解$\mathcal{I}^*_i$，有几个工作假设以 $\hat{\mathcal{I}}_i^*$ 为中心的简单分布来定义(5)中的似然项，例如在[24]中使用了泊松分布。这样公式(5)可以作为一个深入研究的去噪问题。

代替使用简单的图像先验，我们借用了最近基于学习的深度降噪先验研究[53,50]。特别地，我们插入一个深度网络 $\mathcal{N}$ 作为可学习的降噪器，公式(7):


$$
\mathcal{I}^*_i = \mathcal{N}(\hat{\mathcal{I}}_i ,\mathcal{I}_{i+1},\overset{\_}{\mathcal{I}}, \mathbb{E}_{i\sim i+1 })
$$
因此，潜在图像$P(\mathcal{I}_i)$ 的先验值不是明确定义的，而是从训练数据中隐式学习的。为了减少参数大小和防止过拟合，我们对公式(5)的每个去模糊步骤使用相同参数集控制的相同网络，形成一个循环架构。

解问题(4)的剩余问题是如何得到初始潜在图像，即$\mathcal{I}_T$。我们使用的事实是，模糊图像$\overset{\_}{\mathcal{I}}$大致等于瞬时图像在曝光过程中的平均值。结合这个事实和公式(6)，我们有，公式(8)：


$$
\overline{\mathcal{I}} \approx \frac{1}{T} \sum_{i=1}^{T} \mathcal{I}_{i}=\mathcal{I}_{T} \odot \frac{1}{T}\left(1+\sum_{t=2}^{T} \prod_{i=1}^{t-1} \mathcal{B}_{T-i+1}^{T-i}\right)
$$
式中$\mathcal{B}^{i}_{i+1} = exp(-\tau\mathcal{S}^i_{i+1})$，$\mathcal{S}^i_{i+1}$在公式(6)中定义。它使用模糊图像$\overline{\mathcal{I}}$和事件对$\mathcal{I}_T$进行初始估计，称为$\hat{\mathcal{I}}_T$。因此，我们也把$\mathcal{I}_T$作为一个去噪问题来解决，以$\hat{\mathcal{I}}_T$为中心，并使用网络来近似它。然而，我们注意到，公式(8)中的累加运算符引入了更多的漂移，而不同于序列的去模糊步骤。因此，我们纠正$\hat{\mathcal{I}}_T$通过一个独立的、更强大的网络 $\mathcal{I}^*_T = \mathcal{N}_0(\hat{\mathcal{I}}_T ,\overset{\_}{\mathcal{I}}, \mathbb{E}_{1\sim T})$ 。Alg.1 中总结了整个去模糊过程。注意，根据设计(7)，潜在图像以来自图像和事件的局部和长期线索为条件。



`Alg.1 事件辅助的深度运动去模糊: 通过公式8计算原始模糊图像恢复出的最后一帧图像，用一个好的网络对该帧去模糊；之后根据公式2，已知一帧求前一帧，对求出的帧用待训练的网络去模糊`

![image-20210422143259391](/img/image-20210422143259391.png)



## 4. 网络架构

图2展示了本文提出的基于事件的运动去模糊架构，其中包括：(1)一个读取网络，它遍历事件并生成全局场景运动的单个表示；(2)一个初始化网络，它将外观和运动结合起来以生成初始潜在图像；(3)以及一个循环处理网络，依次对所有潜在图像去模糊。读取和初始化网络实例化算法Alg. 1中的 $\mathcal{N}_0$ ，而处理网络实现算法Alg. 1中的 $\mathcal{N}$ 。

读取网络 读取所有事件数据并生成一个联合表示，代表全局的事件运动。为了实现这一点，在曝光期间的事件首先被存储为等长的时间间隔(在图2中有3个间隔)。在每个时间间隔中，事件通过stacked(堆叠)事件帧[17]来表示，进一步将时间间隔~~划分为8个大小相等的块~~，概括每个块中事件的极性，并沿着通道维度stacking(积分)结果。读网络是一个由卷积块和顶部的卷积LSTM[41]组成的循环编码器，用来长期累积特征。

![image-20210422143446090](/img/image-20210422143446090.png)

`图2.本文基于事件的运动去模糊学习框架。为了更好的可视化，我们只假设从模糊图像中恢复4个清晰的帧。详细的层和参数配置参考补充材料。注意由于空间不够，运动补偿(MC)模块没有被画出。有关架构的详细描述，请参阅文本。`



初始化网络(InitializeNet) 从模糊图像中解码出图像的外观，并将其与全局运动相结合，求解出潜在图像 $\mathcal{I}^*_T$  。它将模糊图像 $\overline{\mathcal{I}}$ 和初始估计 $\hat{\mathcal{I}}_T$ (由公式(8)得到)作为输入，用卷积编码器对其进行处理，将得到的编码与读取网络中累积的全局运动特征拼接(Concat)起来，并将联合特征输入解码器得到结果。

给定初始结果，然后处理网络依次对剩余的潜在图像去模糊。在第i步，它需要图像和基于事件的观测(作为输入)。图像部分包括：1)用前一帧的重建 ${\mathcal{I}_{i+1}}$ 通过等式(6)获得的初始估计 $\hat{\mathcal{I}}_i$ ；2)对前一帧的结果${\mathcal{I}_{i+1}}$ 使用运动补偿模块（图2中的MC）的变换，得到的局部+历史图像；3)由定向事件滤波模块（图2中的"DEF''）给出的边界引导映射(map)。稍后将进一步解释这两个模块。输入图像通过卷积层处理，并拼接(concatenated)上通过潜在融合从读取网络中提取的每一步事件的特征。融合的特征被处理并反馈给另一个卷积LSTM，以沿时间方向传播时间知识。最后，解码器使用联合特征并生成去模糊图像。

**运动补偿(Motion compensation)** 我们使用一个运动补偿模块来扭曲(Warp)前一帧的去模糊结果 ${\mathcal{I}_{i+1}}$ ，来生成第i个时间步的初始化。尽管公式(6)通过事件整合来实现这一点（即获取第i帧的初始化），我们发现直接扭曲清晰结果 ${\mathcal{I}_{i+1}}$ 作为**附加**指导来预测流场(flow ﬁeld)更有效。事件的运动补偿已经在论文[10]中讨论过了。为了提高效率，我们采用了FlowNetS架构[9]，以事件 $\mathbb{E}_{i\sim i+1 }$ 为输入，直接返回从 $i$ 到 $i+1$ 的前向流(forward ﬂows)。扭曲(Warping)是通过一个可微的空间变换层来实现的[18,14]。

**定向事件过滤(Directional event filtering)** 由于天然的模糊模型(8)和事件的噪声，初始估计 $\hat{\mathcal{I}}_i$ 可能遭受未处理的模糊。我们借助锐利的边界先验来缓解这个问题，这是一种被广泛探索的图像先验[7,46]，它从事件$\mathbb{E}_{i\sim i+1 }$中提取用于盲去模糊。

事件表明场景照明的局部变化，并揭示物理边界。然而，随着场景边界的移动，在特定的时间，它们仅与在其位置触发的最新事件在空间上对齐。作为一个简单的例子，图(3)显示了成像后的顶线和底线对应于两个不同时间点的事件。它给出了通过在适当的时空位置对事件进行采样，可以预先生成场景边界的方法。注意，由于场景深度的变化，场景的不同部分可能会有不同的运动，**位置自适应采样**(position-adaptive sampling )是必不可少的。

此外，由于事件是稀疏的、有噪声的、非均匀分布的信号，鲁棒采样过程应该决定采样的地点(即中心)和采样的数量(即尺度)。我们通过可微采样和滤波从数据中学习这个任务。对于每个图像位置p，用小网络从事件中预测一个时间中心 $c (p)$ 和一组 $2k + 1$ 滤波系数 $\{α_i\}^k_{i=−k}$ ，其中k是滤波核的支持(support)，满足 $∀i, α_i≥0, \sum^k_{i=−k} α_k = 1$ 。过滤后的结果由下式获得，公式(9)：


$$
\mathcal{G}(\mathbf{p})=\sum_{i=-k}^{k} \alpha_{k} s(\mathbf{p}+\lambda k \mathbf{d}(\mathbf{p}, c(\mathbf{p})), c(\mathbf{p})+\lambda k)
$$
`读者注：d(p, c(p))表示像素点p在时间c(p)时的速度`

式中λ定义采样步长(我们用 k = 2, λ = 1)， s(·, ·)表示**时空域**的采样函数。对于事件 $\mathbb{E}^{i+1}_i$ 的堆叠事件帧表示，可以应用连续采样[21]的三线性核(trilinear kernal)。注意，速度d应该遵循事件在时空点 $(p, c (p))$ 的局部运动方向，沿着事件的密集面过滤，而不是穿过它。

![image-20210422143957672](/img/image-20210422143957672.png)

`图3. 自适应事件采样的动机。(a)一个玩具场景，顶部的线先向下移动，然后底部的线向上移动。事件的正极和负极的分别用红点和绿点表示。(b)成像过程后场景的投影图像。场景边界对应于最近触发的事件，这些事件可能会因不同的位置而变化，如箭头所示。(c)事件累积映射(map)。`



为了得到局部速度，再利用了运动补偿模块预测的流向量。我们假设物体速度保持不变，在这里大致是正确的，因为只有一小部分(很短的)持续时间(只有曝光时间的 $\frac{1}{T-1}$ )。运动补偿给出了所有位置 $p_0\in \mathbb{P}$ 在时间 $i$ 的速度 $d (p_0, i)$ 。在时间 $c(p)$，像素 $p_0$ 将通过流动移动到一个新的位置，公式(10): 
$$
n(p_0)=p_0+(c(p)−i) d(p_0, i)
$$
注意，$n(p_0)$在局部常量假设下继承$p_0$的速度，即：$d(n(p_0)，c(p)) = d(p_0, i)$。



但是，在时间平面$c(p)$的相交位置，即$\{ n(p_0) | p_0 \in \mathbb{P} \}$，并不能保证图像空间的完整采样。因此，我们用Nadaraya-Watson估计器重新采样给定目标p处的速度[4]，公式(11): 


$$
\mathbf{d}(\mathbf{p}, c(\mathbf{p}))=\frac{\sum_{\mathbf{p}_{0} \in \mathbb{P}} \kappa\left(\mathbf{n}\left(\mathbf{p}_{0}\right)-\mathbf{p}\right) \mathbf{d}\left(\mathbf{n}\left(\mathbf{p}_{0}\right), c(\mathbf{p})\right)}{\sum_{\mathbf{p}_{0} \in \mathbb{P}} \kappa\left(\mathbf{n}\left(\mathbf{p}_{0}\right)-\mathbf{p}\right)}
$$
`读者注：n(p0)表示在第i帧的p0像素使用光流法估计出该点在第i+1帧时的位置，用周围的点预测一个点，清除噪声`

式中核κ的定义是标准高斯函数。这在本质上与计算机图形学中用于表面渲染的收集("gather")方法[49]有相似之处。

公式(11)使用所有 $p_0$s来估计每个位置p，是低效的。在实践中，我们只使用位于以p为中心的局部$L\times L$窗口内的样本。窗口大小L应该考虑像素的最大空间位移，我们发现L = 20就足够了。所有提出的步骤都是可微的，可以插入到网络中进行端到端的训练。

**损失函数** 我们使用下面的联合损失函数 


$$
\mathcal{L}_{\text {total }}=\mathcal{L}_{\text {content }}+\lambda_{a} \mathcal{L}_{a d v}+\mathcal{L}_{\text {flow }}+\lambda_{t} \mathcal{L}_{t v}
$$
式中，$\mathcal{L}_{\text {content}}$ 是亮度$\mathcal{l}_1$损失 $\frac{1}{T} \sum_{i=1}^T ||\mathcal{I}_i^*-\mathcal{I}_i^g||$ ，式中 $\mathcal{I}_i^g$ 是清晰图像的groundtruth(标签)。为了提高结果的清晰度，我们也纳入了对抗性损失 $\mathcal{L}_{adv}$ 。我们使用相同的**PatchGAN**鉴别器[13]，并严格遵循它原来的损失定义。

流(flow)网络引入了另外两个损失项。第一个 $\mathcal{L}_{flow}$ 是光度重建损失


$$
\mathcal{L}_{\text {flow }}=\frac{1}{T-1} \sum_{i=1}^{T-1}\left\|\omega\left(\mathcal{I}_{i+1}^{*}, \mathcal{F}_{i \rightarrow i+1}\right)-\mathcal{I}_{i}^{g}\right\|
$$
式中 $\omega(.,.)$ 是一个使用前向流 $\mathcal{F}_{i\rightarrow i+1}$ 的后向扭曲函数(backward warping function)， $\mathcal{L}_{tv} = \frac{1}{T-1} \sum_{i=1}^{T-1}||\triangledown \mathcal{F}_{i\rightarrow i+1}||$ 是流场平滑的总变化损失。 对于这些术语，我们遵循论文**[14]**相同的定义。设$λ_a$和$λ_t$的权值分别为0.01和0.05。

## 5. 实验

### 5.1 实验设置

**数据准备** 我们使用两个数据集进行评估。首先，我们对在GoPro[25]数据集上进行评估，该数据集被广泛用于图像运动去模糊，最近[31]使用它来测试(benchmark)基于事件的去模糊。为了可靠地合成事件，我们使用了开放ESIM事件模拟器[35]。我们遵循训练和测试分离的建议。官方也提供了平均附近(数字从7到13)帧的模糊图像。

由于缺乏在真实场景中评估基于事件的运动去模糊的大规模数据集，我们用DAVIS240C相机捕获了一个新的城市环境数据集，称为Blur-DVS。它混合了一个高速事件传感器和一个记录强度为 $180 \times 240$ 的低帧率有源像素传感器(APS)。因此，APS在快速移动时可能会出现运动模糊。我们收集两个子集进行评估。慢速子集由15246幅相对静态场景的缓慢而稳定的相机运动捕获的图像组成，因此很少发生运动模糊。我们通过对附近7帧的平均来合成运动模糊，得到2178对模糊图像和清晰序列。通过这种方式，我们可以进行定量的基准测试。我们选择1782对用于训练，396对用于测试。快速子集由额外的8个序列组成，共740帧，在快速运动场景的快速相机运动下捕获，以研究如何将所提出的方法推广到真正的运动模糊。然而，没有关于这个子集的groundtruth数据可用。

**方法对比** 我们用现有的结果和/或代码与最近的运动去模糊方法进行了广泛的比较。它们包括基于图像的方法:DCP [29]， MBR [42]， FLO [11]， DMS [25]， EVS [15]， SRN [43]， SVR[52]和MPN[50]，以及最先进的基于事件的运动去模糊方法BHA[31]。我们还比较了三种基于事件的视频重建方法，包括CIE [38]， MRL[24]和最先进的基于学习的方法ETV[36]。采用PSNR和SSIM指标进行定量评价。



![image-20210429134035625](/img/image-20210429134035625.png)



![image-20210429134149352](/img/image-20210429134149352.png)

`图4. 在GoPro数据集上的可视化比较。从左到右分别展示了MPN[50]、BHA[31]和我们方法的模糊图像和groundtruth清晰图像的结果。放大看得更清楚。`



![image-20210429134728267](/img/image-20210429134728267.png)

`一种混合基线，首先采用CIE对图像进行重构，然后采用SRN对图像进行去模糊处理。详情请参见[31]。`

**实现细节** 对于这两个数据集，我们的训练采用的batch size为2个训练对的和优化器为Adam。该网络训练400个epoch，以学习速率 $10^{-4}$ 开始，并从第200个epoch开始线性衰减到零。网络的所有组件都是从零开始共同训练的。

### 5.2 与最先进方法比较

在GoPro数据集上，我们分别在表1和表2中报告了单幅图像去模糊(即只恢复中间帧)和视频重建(即恢复所有清晰帧)的结果。大部分其他方法(j结果)都直接取自论文。我们的方法在这两个任务中都达到了最高的位置，展示了事件辅助去模糊比纯粹依赖图像的优势，以及所提出的框架优于物理重建模型。我们在图4中展示了两个快速移动场景的视觉对比:虽然基于图像的方法MPN不能很好地解决这种模糊，但BHA对事件的噪声很敏感，特别是沿着物体边缘的噪声。我们的方法产生了更干净、更清晰的结果。

需要注意的是，GoPro数据集主要表现为小到中等程度的运动模糊，因此模糊输入的质量较好，从事件中得到的改进是有限的。因此，最近强大的架构SRN和MPN得到了非常有前景的结果，尽管它们没有看到事件。出于这个原因，我们将我们的方法与最先进方法在提出的Blur-DVS数据集上进行比较，在这些数据集中，严重的运动模糊更为普遍。再次，我们报告单个图像去模糊(表3)和视频重建(表4)任务的结果。请注意，为了公平比较，基于学习的方法SRN、MPN和ETV在Blur-DVS的训练集上进行了微调。我们还比较了增强版的图像和事件:对于基于图像的SRN和MPN方法，我们将输入的模糊图像与所有48个事件帧(每个时间间隔和(7 1)时间间隔内的8个帧)连接起来。对于基于事件的方法ETV，我们也将模糊图像连同事件一起提供给其每个周期性重建步骤。我们分别将这些变量表示为SRN+、MPN+和ETV+。

在表3和表4中，本文提出的方法取得了最好的结果。它的性能也优于所有增强的变体，证明了所提出的框架的有效性。从图5可以看出:1)在快速运动的情况下，仅基于图像的线索是不够的，限制了MPN的性能;2)由于事件的有损采样机制，物理模型BHA容易产生噪声，并呈现未处理的模糊;3)基于事件的重建方法CIE、MRL和ETV由于缺乏图像引导和/或物理模型简化，无法正确恢复场景对比度。我们的方法没有遇到上述问题，甚至比配备强大架构的增强图像+事件变体的结果更清晰。

最后，我们分析了现实运动模糊的泛化行为。如图6所示，本文方法获得了最好的视觉质量。我们怀疑运动去模糊的显式建模和强去模糊先验的引入可能会减轻学习困难，并避免潜在的过拟合在更多的黑箱架构中。在实践中，我们发现这种改进与真实数据一致，这一点在补充材料中提供的快速子集的更多结果中得到了证明。

![image-20210429134347374](/img/image-20210429134347374.png)



![image-20210429134535535](/img/image-20210429134535535.png)

`图5. 在模糊-分布式数据集的慢运动子集上，用不同方法生成的两个例子的代表性结果。更多的结果可以在我们的补充材料中找到。放大看得更清楚。`



`模糊分布式 数据集 的视频重构性能。`

![image-20210429134645033](/img/image-20210429134645033.png)

### 5.3 技术性能分析

**分析不同的组件** 我们将重要的算法组件分离出来，看看其他对最终性能的贡献，并将结果总结在表5和图7中。由此可见，为了提高结果的PSNR和SSIM，每个分量都是必要的。只使用图像外观而不使用事件(App.)不能很好地消除图像模糊。另一方面，仅使用事件可以恢复大量的细节，但强度对比度不能很好地恢复(见图7(b))。同时使用两种输入信号(App. + event)效果更好，但由于噪声的影响，重构图像不是很平滑(如图7 (c)中的地面)。进一步结合运动补偿(+MC)有助于这些方面，因为它强加了时间平滑。最后，进一步引入定向事件过滤模块(+DEF)，学习到的边界指导可以产生更清晰的结果和更丰富的细节。

`表5 模糊-分布式数据集的组件分析。App.和event分别表示以模糊图像外观和事件数据作为输入。MC和DEF分别为运动补偿模块和方向事件滤波模块。`

![image-20210422145324727](/img/image-20210422145324727.png)

**DEF模块的校准** 在表6中，我们证明了提出的方向事件过滤模块的必要性。在这里,"w/o guid"。在整个流程中不包括边界指导。相反，“guid only”。在每个连续的清除步骤中丢弃事件特征，而只使用边界指导作为附加线索。我们进一步设计了一个变体“+param.”，它不包含DEF，但在处理网络的编码器中有额外的卷积层，超出了当前的参数大小(即参数变多了)。结果表明，学习到的边界引导极大地提高了估计(SSIM从0.786提高到0.827)，并且在没有其他线索的情况下也可以得到很好的结果。然而，简单地扩大网络规模，并没有观察到有意义的改善。

![image-20210422145504218](/img/image-20210422145504218.png)

`图6 在Blur-DVS数据集的快速子集(真实世界运动模糊)上，通过不同方法生成代表性结果。更多的结果可以在我们的补充材料中找到。放大看得更清楚。`

![image-20210422145626221](/img/image-20210422145626221.png)

`图7 可视化地分析不同组件在DVS-Blur数据集上的贡献。详情请参见文本。`



`表6 分析了DVS-Blur数据集上的方向事件过滤模块。详情请参见text。`

![image-20210422145728340](/img/image-20210422145728340.png)

在图8中，我们将学习边界引导的作用可视化。关注网络如何学习根据场景的运动选择不同的时间中心(图8 (c))。边界引导显著提高了场景的清晰度，并恢复了缺失的细节(图8 (e)和(f))。

![image-20210422145909881](/img/image-20210422145909881.png)

`图8 可视化学习的边界指导。请注意图(c)中如何选择来自不同时间戳的运动边界(红色代表大值，蓝色代表小值)。`



**微光摄影** 如图9所示，该方法的一个潜在应用是微光摄影。短曝光(13ms)图像缺乏光线。然而，长曝光(104ms)的相机可能会出现严重的运动模糊。利用事件线索，我们的方法产生自然的结果，没有这种模糊。

![image-20210422150033834](/img/image-20210422150033834.png)

`图9 使用我们的方法进行微光摄影。在室内场景中，用DAVIS240C相机捕捉图像和事件。`

## 6. 结论

在这项工作中，我们提出在事件的帮助下从严重的运动模糊图像中提取视频。为此，提出了一种新的深度学习体系结构，在全局和局部粒度上有效融合外观和运动线索。在此基础上，利用新的定向事件滤波模块提取清晰的事件边界引导来改善重构细节。广泛的评估表明，与各种现有的基于图像和事件的方法相比，所提出的方法在合成和真实数据集上取得了更好的性能。