---
title: pytorch-激活函数
date: 2025-08-5 10:14:00
tags:
 - PyTorch
 - 深度学习
typora-root-url: ..
typora-copy-images-to: ../img/pytorch
---



# 常用的激活函数介绍

激活函数的原则：

- 单调函数（或有极小一部分不单调）
- 非线性函数
- 具有良好的梯度

---

<!--more-->

## 1. Sigmoid 函数

Sigmoid 是早期神经网络中常用的激活函数，其数学表达式为：


$$
f(x) = \frac{1}{1 + e^{-x}}
$$


- **优点**：输出值在 (0, 1) 区间内，适合用于二分类问题的概率预测。
- **缺点**：容易出现梯度消失问题，计算量相对较大。

## 2. Tanh（双曲正切）函数

Tanh 的数学表达式如下：


$$
 f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} 
$$


- **优点**：将输入值压缩到 (-1, 1)，对于后续的优化过程较为友好。
- **缺点**：与 Sigmoid 类似，两端的导数接近于零，可能导致梯度消失问题。

## 3. ReLU（修正线性单元）

ReLU 是当前深度学习中最常用的激活函数之一，定义为：


$$
 f(x) = \max(0, x) 
$$


- **优点**：计算简单，有效缓解了梯度消失问题；允许模型稀疏表示。
- **缺点**：可能导致“死亡ReLU”问题，即某些神经元永远不会再激活。

## 4. Leaky ReLU

Leaky ReLU 是对 ReLU 的改进，尝试解决“死亡ReLU”问题，定义为：


$$
 f(x) = \begin{cases} 
x & \text{if } x > 0 \\
\alpha x & \text{if } x \leq 0 
\end{cases} 
$$


其中，$\alpha$ 是一个小的常数。

- **优点**：避免了部分神经元失效的问题。
- **缺点**：相比 ReLU 更复杂一些，但差异不大。

## 5. ELU（指数线性单元）

ELU 在 Leaky ReLU 的基础上进一步改进，定义为：


$$
 f(x) = \begin{cases} 
x & \text{if } x > 0 \\
\alpha(e^x - 1) & \text{if } x \leq 0 
\end{cases} 
$$


- **优点**：在负数区域有非零输出，有助于加速学习。
- **缺点**：计算成本较 ReLU 稍高。

## 6. Swish

Swish 是一种新型激活函数，由 Google 提出，定义为：


$$
 f(x) = x \cdot \sigma(x) 
$$


这里，$\sigma(x)$ 是 Sigmoid 函数。

- **优点**：平滑、非单调，可能提供更好的性能。
- **缺点**：相较于 ReLU 计算稍微复杂。

## 7. Softmax

Softmax 主要用于多分类问题的输出层，将一个 K 维向量转化为另一个 K 维概率分布。


$$
f(\mathbf{x})_i = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}}
$$


- **优点**：适用于多分类任务，直接给出每个类别的概率估计。
- **缺点**：仅适用于输出层，在隐藏层使用效果不佳。

## 8.GELU (Gaussian Error Linear Unit)

**GELU** 是一种平滑的激活函数，它基于随机正态分布的概率计算。具体来说，GELU 可以看作是神经元在输入值大于某个阈值时被激活的概率，这个阈值遵循标准正态分布。它的数学定义如下：
$$
GELU(x)=xΦ(x)
$$
这里，$Φ(x)$表示标准正态分布 $N(0,1)$的累积分布函数（CDF）。为了便于实际计算，通常使用近似公式：
$$
GELU(x)≈0.5x(1+tanh⁡(\sqrt{2/π}(x+0.044715x3)))
$$

![gelu](/img/pytorch/gelu.png)

详情参考苏神文章：[GELU的两个初等函数近似是怎么来的 ](https://kexue.fm/archives/7309)

- **优点**: 相比于 ReLU 和其变种，GELU 提供了更平滑的非线性变换，有助于改善梯度流和模型的泛化能力。它特别适合用于深度学习模型中的隐藏层。
- **缺点**: 计算上相对复杂一些，但现代硬件可以很好地处理这种额外的计算开销。

## 9.SwiGLU (Switched GLU)

**SwiGLU** 是 Google 在2023年提出的一种新型激活函数，结合了 Swish 激活函数的优点与门控机制(GLU)的思想。不同于传统激活函数直接作用于输入，SwiGLU 通过引入可学习的参数来调整输入，从而试图保留 GLU 的优点同时改善其性能。然而，关于 SwiGLU 的具体定义和实现细节可能有所不同或有所发展，下面提供一个基于公开信息的理解框架：
$$
SwiGLU(x,W,V,b,d)=Swish(Wx+b)⊗(Vx+d)
$$
其中，

- $x$ 是输入向量，
- $W$、$V$ 分别为权重矩阵，
- $b$、$d$ 为偏置项，
- $⊗$ 表示逐元素相乘，
- $Swish(z)=z⋅σ(z)$，这里的 $σ(z)$是 Sigmoid 函数。
- **优点**: 结合了 Swish 的非单调性和 GLU 的门控特性，旨在提高模型的表达能力和训练效率。理论上，SwiGLU 能够更好地适应复杂的模式，特别是在自然语言处理任务中显示出潜力。
- **缺点**: 作为一种较新的激活函数，其实用性和优势还在探索之中。相对于一些简单激活函数（如ReLU），其计算成本更高。
