---
title: Path Aggregation Network for Instance Segmentation
date: 2024-07-29 11:49:00
tags:
 - CS
 - CVPR 2018
typora-root-url: ..
typora-copy-images-to: ..\img\panet
---

用于实例分割的路径聚合网络
 ---


会议: CVPR 2018

论文地址：https://arxiv.org/abs/1803.01534

github: https://github.com/ShuLiu1993/PANet

[TOC]

## 摘要

信息在神经网络中的传播方式至关重要。本文提出了路径聚合网络 (PANet)，旨在提升基于候选框的实例分割框架中的信息流。具体来说，我们通过自底向上的路径增强，将低层中的精确定位信号引入整个特征层次，从而缩短了低层和顶层特征之间的信息路径。我们提出了自适应特征池化，将特征网格和所有特征级别连接起来，使每个特征级别中的有用信息可以直接传播到后续的候选框子网络中。我们创建了一个互补分支，用于捕获每个候选框的不同视角，从而进一步提高掩码预测的精度。这些改进易于实现，且计算开销微小。我们的 PANet 在 COCO 2017 挑战赛的实例分割任务中取得了第一名，并在没有使用大批次训练的情况下，在目标检测任务中取得了第二名。它也是 MVD 和 Cityscapes 上的最先进技术。代码地址: https://github.com/ShuLiu1993/PANet

<!--more-->



## 1. 引言

实例分割是最重要和最具挑战性的任务之一。它旨在预测类别标签和像素级的实例掩码，以定位图像中出现的不同数量的实例。这项任务广泛应用于自动驾驶、机器人、视频监控等领域。

在深度卷积神经网络的帮助下，已经提出了几种实例分割框架，例如 [21, 33, 3, 38]，其性能迅速提升 [12]。Mask R-CNN [21] 是一种简单而有效的实例分割系统。基于 Fast/Faster R-CNN [16, 51]，使用全卷积网络 (FCN) 进行掩码预测，并辅以框回归和分类。为了获得高性能，FPN [35] 被用于提取网络内部的层次特征，其中添加了一个自顶向下的路径和横向连接，以传播语义信息强的特征。

最近发布的几个数据集 [37, 7, 45] 为算法改进提供了很大的空间。COCO [37] 包含 20 万张图像。每张图像中都有大量具有复杂空间布局的实例。Cityscapes [7] 和 MVD [45] 提供了包含大量交通参与者的街景，每张图像中都有模糊、遮挡严重和极其微小的实例。

在图像分类中设计网络的一些原则也被证明对目标识别有效。例如，通过直接的残差连接 [23, 24] 和密集连接 [26] 缩短信息路径和简化信息传播，以及通过创建遵循“分割-变换-合并”策略的并行路径来增加信息路径的灵活性和多样性 [61, 6]，都是有益的。

**本文发现**

我们的研究表明，最先进的 Mask R-CNN 中的信息传播可以进一步改进。具体来说，低层特征对于大型实例识别很有帮助。但从低层结构到顶层特征的路径很长，增加了获取精确定位信息的难度。此外，每个候选框都是基于从单个特征级别池化的特征网格进行预测的，这个级别是启发式地分配的。由于在其他级别中丢弃的信息可能对最终预测有帮助，因此这个过程可以进行优化。最后，掩码预测是在单个视角上进行的，失去了收集更多多样化信息的机会。

**本文贡献**

受这些原则和观察结果的启发，我们提出了 PANet，如图 1 所示，用于实例分割。

![image-20240729151551834](/img/panet/image-20240729151551834.png)

`图1. 展示了网络框架插图。(a) FPN骨干网。(b)自下而上的路径聚合。(c)自适应特征池化。(d)检测框分支。(e)全连接融合。注意图中为了简洁起见，省略了(a)和(b)中特征映射的通道维度。`

首先，为了缩短信息路径并使用低层中存在的精确定位信号增强特征金字塔，我们创建了自底向上的路径增强。实际上，低层特征已经在 [44, 42, 13, 46, 35, 5, 31, 14] 系统中使用。但将低层特征传播以增强整个特征层次以进行实例识别尚未被探索。

其次，为了恢复每个候选框与所有特征级别之间中断的信息路径，我们开发了自适应特征池化。这是一个简单的组件，用于为每个候选框聚合来自所有特征级别的特征，避免任意分配的结果。通过此操作，与 [4, 62] 中的路径相比，创建了更直接的路径。

最后，为了捕获每个候选框的不同视角，我们通过添加小型全连接 (fc) 层来增强掩码预测，这些层具有与 Mask R-CNN 原本使用的 FCN 互补的特性。通过融合来自这两个视角的预测，增加了信息多样性，并产生了质量更好的掩码。

前两个组件由对象检测和实例分割共享，从而显著提高了这两个任务的表现。

**实验结果**

使用 PANet，我们在几个数据集上取得了最先进的性能。以 ResNet-50 [23] 作为初始网络，我们的 PANet 在单个尺度上测试的性能已经超过了 COCO 2016 挑战赛中对象检测 [27] 和实例分割 [33] 任务的冠军。请注意，这些先前结果是通过更大型的模型 [23, 58] 以及多尺度测试和水平翻转测试获得的。

我们在没有使用大批次训练的情况下，在 COCO 2017 挑战赛的实例分割任务中取得了第一名，并在目标检测任务中取得了第二名。我们还将在 Cityscapes 和 MVD 上对系统进行基准测试，这同样会产生排名靠前的结果，表明我们的 PANet 是一个非常实用且性能优异的框架。

## 2. 相关工作

**实例分割**

实例分割主要有两种方法。最流行的是基于候选框的方法。这种方法与目标检测有很强的联系。在 R-CNN [17] 中，来自 [60, 68] 的对象候选框被输入到网络中，以提取用于分类的特征。而 Fast/Faster R-CNN [16, 51] 和 SPPNet [22] 通过从全局特征图中池化特征来加速过程。早期的工作 [18, 19] 从 MCG [1] 中提取掩码候选框作为输入，以提取特征，而 CFM [9]、MNC [10] 和 Hayder 等人 [20] 将特征池化与网络合并，以提高速度。更新的设计是在网络中生成掩码作为候选框 [48, 49, 8] 或最终结果 [10, 34, 41]。Mask R-CNN [21] 是一个属于该类别的有效框架。我们的工作基于 Mask R-CNN，并从不同的方面对其进行了改进。

另一类方法主要是基于分割的。它们学习了专门设计的变换 [3, 33, 38, 59] 或实例边界 [30]。然后从预测的变换中解码实例掩码。

**多级特征**

在图像识别中使用了来自不同层的特征。SharpMask [49]、Peng 等人 [47] 和 LRR [14] 通过融合特征图来进行分割，以获得更精细的细节。FCN [44]、U-Net [54] 和 Noh 等人 [46] 通过跳跃连接将来自较低层的信息融合在一起。TDM [56] 和 FPN [35] 通过横向连接增强自顶向下的路径，用于目标检测。与 TDM 不同，它将具有最高分辨率的融合特征图用于池化特征，SSD [42]、DSSD [13]、MS-CNN [5] 和 FPN [35] 将候选框分配到适当特征级别进行推理。我们将 FPN 作为基线，并对其进行了大幅改进。

ION [4]、Zagoruyko 等人 [62]、Hypernet [31] 和 Hypercolumn [19] 通过连接来自不同层的特征网格来进行更好的预测。但需要进行一系列操作，即归一化、连接和降维，以获得可行的新特征。相比之下，我们的设计要简单得多。

**更大的上下文区域**

[15, 64, 62] 中的方法使用类似于中间凹的结构为每个候选框池化特征，以利用具有不同分辨率的区域中的上下文信息。来自更大区域的特征提供了周围的环境信息。PSPNet [67] 和 ParseNet [43] 中使用了全局池化，极大地提高了语义分割的质量。Peng 等人 [47] 观察到类似的趋势，其中使用了全局卷积。我们的掩码预测分支也支持访问全局信息，但技术完全不同。

## 3 模型框架

我们的框架如图 1 所示。路径增强和聚合是为了提高性能。创建了一个自底向上的路径，以使低层信息更容易传播。我们设计了自适应特征池化，以允许每个候选框访问来自所有级别的信息进行预测。一个互补路径被添加到掩码预测分支中。这个新的结构导致了良好的性能。与 FPN 类似，这种改进与 CNN 结构无关，例如 [57, 32, 23]。

### 3.1. 自底向上的路径增强

**动机**

论文[63] 中的深刻见解表明，高层神经元对整个目标有强烈的响应，而底层神经元更可能被局部纹理和图案激活，这表明了增强自顶向下路径以传播语义信息强的特征并增强所有特征以具有合理的分类能力的必要性。我们的框架通过基于高边缘或实例部分响应是对准确定位实例的强指标的事实，进一步增强了整个特征层次的空间定位能力。为此，我们构建了一条从低层到顶层的干净（指不经过变换直接连接）横向连接路径。因此，在这些层之间有一个“捷径”(图 1 中的虚线绿色线)，它由不到 10 层组成，横跨这些层。相比之下，FPN 中的 CNN 主干提供了一个从低层到最顶层的长路径(图 1 中的虚线红线)，它甚至经过 100 多层。

**增强的自底向上结构**

我们的框架首先完成自底向上的路径增强。我们遵循 FPN 来定义具有相同空间大小的特征图的层位于相同的网络阶段。每个特征级别对应一个阶段。我们还采用 ResNet [23] 作为基本结构，并使用 {P2, P3, P4, P5} 来表示 FPN 生成的特征级别。我们的增强路径从最低级别 P2 开始，并逐渐接近 P5，如图 1(b) 所示。

从 P2 到 P5，空间大小逐渐以 2 为因子下采样。我们使用 {N2, N3, N4, N5} 来表示新产生的特征图，对应于 {P2, P3, P4, P5}。请注意，N2 就是 P2，没有经过任何处理。

如图 2 所示，每个构建块从更高分辨率的特征图 $N_i$ 和更粗的特征图 $P_{i+1}$ 通过横向连接接收，并生成新的特征图 $N_{i+1}$。每个特征图 $N_i$ 首先通过一个步长为 2 的 3x3 卷积层来减少空间大小。然后，特征图 $P_{i+1}$ 的每个元素和下采样的图通过横向连接相加。然后，融合后的特征图经过另一个 3x3 卷积层来生成 $N_{i+1}$，以供后续子网络使用。这是一个迭代过程，并在接近 P5 时终止。

![image-20240729155815876](/img/panet/image-20240729155815876.png)

`图2. 自底向上路径扩展的构建块示意图。`

在这些构建块中，我们始终使用特征图的通道 256。所有卷积层后面都跟着一个 ReLU [32]。然后，每个候选框的特征网格从新的特征图中池化，即 {N2, N3, N4, N5}。

### 3.2. 自适应特征池化

**动机**

在 FPN [35] 中，根据候选框的大小将候选框分配到不同的特征级别。这使得小候选框被分配到低特征级别，而大候选框被分配到高特征级别。虽然简单有效，但它仍然可能产生非最优结果。例如，两个具有 10 像素差异的候选框可以被分配到不同的级别。实际上，这两个候选框相当相似。此外，特征的重要性可能与其所属的级别没有很强的相关性。高层特征是由具有大感受野生成的，并捕获更丰富的上下文信息。允许小候选框访问这些特征可以更好地利用有用的上下文信息进行预测。同样，低层特征包含许多精细的细节和高定位精度。让大候选框访问它们显然是有益的。基于这些想法，我们提出了为每个候选框从所有级别池化特征并进行融合，以供后续预测。我们将这个过程称为自适应特征池化。

我们现在分析自适应特征池化中来自不同级别的特征池化的比例。我们使用最大操作来融合来自不同级别的特征，这允许网络选择逐元素的有用信息。我们根据候选框在 FPN 中被分配到的级别将候选框聚类为四类。对于每组候选框，我们计算从不同级别选择的特征的比例。在符号表示中，1-4 级别代表从低到高的级别。如图 3 所示，蓝色线代表最初在 FPN 中被分配到第 1 级别的小候选框。令人惊讶的是，近 70% 的特征来自其他更高的级别。我们还使用黄色线来表示最初在 FPN 中被分配到第 4 级别的大候选框。同样，50%+ 的特征是从其他较低的级别池化的。这个观察清楚地表明，多个级别的特征共同有助于准确的预测。这也是设计自底向上路径增强的强有力支持。

![image-20240729160144008](/img/panet/image-20240729160144008.png)

`图3. 自适应特征池化从不同特征层池化的特征比例。每条线代表一组应分配到FPN中相同特征级别的提案，即具有相似规模的提案。横轴表示汇集特征的来源。结果表明，不同规模的提案都利用了几个不同层次的特征。`

**自适应特征池化结构**

自适应特征池化实际上在实现上很简单，如图 1(c) 所示。首先，对于每个候选框，我们将它们映射到不同的特征级别，如图 1(b) 中的深灰色区域所示。遵循 Mask R-CNN [21]，ROIAlign 被用于从每个级别池化特征网格。然后，使用元素级最大值或求和的融合操作被用于融合来自不同级别的特征网格。

在后续子网络中，池化的特征网格独立地通过一个参数层，该层后面跟着融合操作，以允许网络适应特征。例如，FPN 的框分支中有两个 fc 层。我们在第一层之后应用融合操作。由于 Mask R-CNN 中的掩码预测分支使用了四个连续的卷积层，我们将融合操作放置在第一个和第二个卷积层之间。关于自适应特征池化在框分支上的详细说明，请参见附录中的图 6。

我们的设计侧重于融合网络内部特征层次的信息，而不是来自输入图像金字塔中不同特征图的 [52]。与 [4, 62, 31] 中的过程相比，它更简单，在这些过程中需要 L-2 归一化、连接和降维。

### 3.3. 全连接融合

**动机**

全连接层(MLP) ，在实例分割 [10, 41, 34] 中的掩码预测和掩码候选框生成 [48, 49] 中被广泛使用。结果 [8, 33] 表明 FCN 也能够预测实例的像素级掩码。最近，Mask R-CNN [21] 在池化的特征网格上应用了一个小型 FCN 来预测相应的掩码，从而避免了类别之间的竞争。我们注意到 fc 层与 FCN 相比会产生不同的特性，其中后者根据局部感受野和不同空间位置的共享参数对每个像素进行预测。相反，fc 层是位置敏感的，因为不同空间位置的预测是通过不同的参数集实现的。因此，它们具有适应不同空间位置的能力。此外，每个空间位置的预测都是基于整个候选框的全局信息实现的。这有助于区分实例 [48] 并识别属于同一对象的独立部分。鉴于 fc 和卷积层之间不同的特性，我们将来自这两种类型层的预测进行融合，以获得更好的掩码预测。

**掩码预测结构**

我们的掩码预测组件轻量级且易于实现。掩码分支对每个候选框的池化特征网格进行操作。如图 4 所示，主路径是一个小型 FCN，由 4 个连续的卷积层和 1 个反转卷积层组成。每个卷积层由 256 个 3x3 滤波器组成，反转卷积层将特征上采样因子为 2。

![image-20240729160326105](/img/panet/image-20240729160326105.png)

`图4. 掩码预测分支与全连接融合。`

它为每个类别独立地预测二值像素级掩码，以解耦分割和分类，类似于 Mask R-CNN。我们进一步创建了一条从 conv3 层到 fc 层的短路径。有两个 3x3 卷积层，其中第二个卷积层将通道数量减半，以减少计算开销。

使用 fc 层预测类别无关的前景/背景掩码。它不仅效率高，而且允许 fc 层中的参数用更多样本来训练，从而获得更好的泛化能力。我们使用的掩码大小是 28x28，因此 fc 层产生一个 784x1x1 的向量。这个向量被重塑为与 FCN 预测的掩码相同的空间大小。为了获得最终的掩码预测，将 FCN 的每个类别的掩码和 fc 的前景/背景预测相加。使用一个 fc 层而不是多个 fc 层来进行最终预测，可以防止将隐藏的空间特征图坍缩成一个短的特征向量，从而丢失空间信息。

## 4. 实验

我们在具有挑战性的 COCO [37]、Cityscapes [7] 和 MVD [45] 数据集上将我们的方法与最先进的技术进行了比较。我们在所有这些数据集上都取得了排名靠前的结果。我们在 COCO 数据集上进行了全面的消融研究。我们还展示了我们在 COCO 2017 实例分割和目标检测挑战赛中的结果。

### 4.1. 实现细节

我们基于 Caffe [29] 重新实现了 Mask R-CNN 和 FPN。我们在实验中使用的所有预训练模型都是公开可用的。我们采用以图像为中心的训练 [16]。对于每张图像，我们采样 512 个感兴趣区域 (ROIs)，正负比例为 1:3。权重衰减为 0.0001，动量为 0.9。其他超参数略有不同，具体取决于数据集，我们在各自的实验中详细说明了它们。遵循 Mask R-CNN，候选框来自一个独立训练的 RPN [35, 51]，以便于消融和公平比较，即主干不与目标检测/实例分割共享。

### 4.2. 在 COCO 上的实验

**数据集和指标**

COCO [37] 数据集是实例分割和目标检测最具挑战性的数据集之一，因为数据复杂。它包含 11.5 万张训练图像和 5 千张验证图像(2017 年的新分割)。2 万张图像用于测试开发，2 万张图像用作测试挑战。测试挑战和测试开发的真实标签没有公开。有 80 个类别，带有像素级实例掩码注释。我们在 train-2017 子集上训练我们的模型，并在 val-2017 子集上报告消融研究的结果。我们还报告了测试开发集上的结果以进行比较。

我们遵循标准的评估指标，即 $AP、AP_{50}、AP_{75}、AP_S、AP_M和 AP_L$ 。最后三个指标衡量不同尺度对象的表现。由于我们的框架对实例分割和目标检测都适用，我们还训练了独立的目标检测器。我们报告了独立训练的目标检测器的掩码 AP 和框 ap $AP^{bb}$，以及多任务方式训练的框分支的 box ap $AP^{bbM}$。

**超参数**

我们在一个图像批次中使用 16 张图像进行训练。如果未特别说明，则图像的短边和长边分别为 800 和 1000。对于实例分割，我们使用学习率 0.02 训练模型 120k 次迭代，并使用学习率 0.002 训练 40k 次迭代。对于目标检测，我们训练了一个没有掩码预测分支的目标检测器。目标检测器以学习率 0.02 训练 60k 次迭代，并以学习率 0.002 训练 20k 次迭代。

这些参数是从 Mask R-CNN 和 FPN 中采用的，没有进行任何微调。

**实例分割结果**

我们报告了我们的 PANet 在测试开发集上的性能以进行比较，包括多尺度训练和不进行多尺度训练。如表 1 所示，我们的 PANet 使用在多尺度图像上训练并在单尺度图像上测试的 ResNet-50，已经超过了 Mask R-CNN 和 2016 年的冠军，其中后者使用了更大的模型集成和测试技巧 [23, 33, 10, 15, 39, 62]。在 800 的图像尺度上训练和测试，我们的方法在相同的初始模型下比单模型最先进的 Mask R-CNN 的性能提高了近 3 个百分点。

![image-20240729161430557](/img/panet/image-20240729161430557.png)

`表1：在COCO测试-验证子集中，基于PANet（COCO 2016实例分割挑战赛的获胜者）和Mask R-CNN（基准模型）的Mask AP的比较。`

**目标检测结果**

与 Mask R-CNN 采用的方式类似，我们还报告了从框分支推断的边界框结果。表 2 显示，我们的方法使用 ResNet-50，在单尺度图像上进行训练和测试，以大幅优势超过了所有其他单模型，即使使用了更大的 ResNeXt-101 [61] 作为初始模型。使用多尺度训练和单尺度测试，我们的 PANet 使用 ResNet-50 超过了 2016 年的冠军，后者使用了更大的模型集成和测试技巧。

![image-20240729161536835](/img/panet/image-20240729161536835.png)

`表2. COCO 2016目标检测挑战赛的获胜者PANet、RetinaNet和Mask R-CNN在COCO测试开发子集上的box AP比较，其中后三个为基线。`

**组件消融研究**

首先，我们分析了我们提出的每个组件的重要性。除了自底向上路径增强、自适应特征池化和全连接融合之外，我们还分析了多尺度训练、多 GPU 同步批归一化和更重的头部。对于多尺度训练，我们将长边设置为 1,400，而另一个则从 400 到 1,400。我们在一个批次中跨所有 GPU 的所有样本上计算均值和方差，在训练过程中不固定任何参数，并且当使用多 GPU 同步批归一化时，使所有新层后面跟着一个批归一化层。更重的头部使用 4 个连续的 3x3 卷积层，由框分类和框回归共享，而不是两个 fc 层。这与 [36] 中使用的头部类似，但在他们的情况下，框分类和框回归分支的卷积层不共享。

我们在 val-2017 子集上从基线逐渐添加所有组件的消融研究，结果如表 3 所示。ResNet-50 [23] 是我们的初始模型。我们报告了掩码 AP、独立训练的目标检测器的框 ap APbb 和多任务方式训练的框分支的 box ap APbbM 的性能。

1. 重新实现的基线。我们重新实现的 Mask R-CNN 的性能与原始论文中描述的性能相当，我们的目标检测器表现更好。
2. 多尺度训练和多 GPU 同步批归一化。这两种技术有助于网络更好地收敛并提高泛化能力。
3. 自底向上路径增强。无论是否有自适应特征池化，自底向上路径增强始终将掩码 AP 和框 ap APbb 分别提高 0.6 和 0.9 以上。对大尺度实例的改进最为显着。这证实了来自较低特征层次的信息的有用性。
4. 自适应特征池化。无论是否有自底向上路径增强，自适应特征池化始终提高性能。所有尺度的性能通常都会提高，这与我们的观察结果一致，即其他层的特征在最终预测中也有用。
5. 全连接融合。全连接融合旨在预测质量更好的掩码。它在掩码 AP 方面产生了 0.7 的改进。它对所有尺度的实例都通用。
6. 更重的头部。更重的头部对多任务方式训练的边界框的 box ap APbbM 非常有效。而对于掩码 AP 和独立训练的目标检测器，改进很小。

![image-20240729161823917](/img/panet/image-20240729161823917.png)

Table3. 在val-2017上独立训练的目标检测器的mask AP、box AP $AP^{bb}$和多任务方式训练的box分支的box AP $AP^{bbM}$的性能指标。基于我们重新实现的基线(RBL)，我们逐渐增加了多尺度训练(MST)、多gpu同步批归一化(MBN)、自下而上路径增强(BPA)、自适应特征池化(AFP)、全连接融合(FF)和重头(HHD)的研究。MRB是原论文中报道的Mask R-CNN结果的简称。最后一行显示了与基线RBL相比的总体改善。

PANet 中包含所有这些组件，掩码 AP 比 baselines 提高了 4.4。独立训练的目标检测器的 box AP $AP^{bb}$增加了 4.2。它们都是显着的。小尺寸和中尺寸实例的贡献最大。一半的改进来自多尺度训练和多 GPU 同步批归一化，这些是帮助训练更好模型的策略。

**自适应特征池化消融研究**

我们对自适应特征池化进行了消融研究，以找出在哪里放置融合操作以及最合适的融合操作。我们将它放置在 ROIAlign 和 fc1 之间，用“fu.fc1fc2”表示，或者放置在 fc1 和 fc2 之间，用“fc1fu.fc2”表示，如表 4 所示。类似的设置也应用于掩码预测分支。对于特征融合，我们测试了最大值和求和操作。

![image-20240729162708703](/img/panet/image-20240729162708703.png)

`表4. 基于独立训练目标检测器掩模AP和Box AP的val-2017自适应特征池化消融研究`

如表 4 所示，自适应特征池化对融合操作不敏感。然而，允许一个参数层来适应来自不同级别的特征网格是非常重要的。我们在框架中使用最大值作为融合操作，并将其放在第一个参数层之后。

**全连接融合消融研究**

我们研究了以不同的方式实例化增强的 fc 分支的性能。我们考虑了两个方面，即开始新分支的层和融合来自新分支和 FCN 的预测的方法。我们分别从 conv2、conv3 和 conv4 创建新路径。“max”、“sum”和“product”操作用于融合。我们将我们重新实现的 Mask R-CNN，包括自底向上路径增强和自适应特征池化作为基线。相应的结果如表 5 所示。它们清楚地表明，从 conv3 开始并使用求和进行融合会产生最佳结果。

![image-20240729162733861](/img/panet/image-20240729162733861.png)

`表5所示。基于Mask AP的val-2017全连接融合消融研究。`

**COCO 2017 挑战赛**

使用 PANet，我们参加了 COCO 2017 实例分割和目标检测挑战赛。我们的框架在实例分割任务中取得了第一名，在目标检测任务中取得了第二名，而没有使用大批次训练。如图 1 和表 6 和表 7 所示，与去年的冠军相比，我们在实例分割方面取得了 9.1% 的绝对和 24% 的相对改进。而目标检测方面，则取得了 9.4% 的绝对和 23% 的相对改进。

![image-20240729162909706](/img/panet/image-20240729162909706.png)

`表6所示。测试开发中不同年份COCO实例分割挑战的Mask AP。`

![image-20240729162934233](/img/panet/image-20240729162934233.png)

`表7. COCO目标检测挑战赛不同年份测试开发的方框AP。`

最先进的性能来自于 PANet 中的几个细节。首先，我们使用了可变形卷积，其中采用了 DCN [11]。常规的测试技巧 [23, 33, 10, 15, 39, 62]，例如多尺度测试、水平翻转测试、掩码投票和框投票，都被采用。对于多尺度测试，我们将长边设置为 1,400，而另一个则从 600 到 1,200，步长为 200。仅使用 4 个尺度。其次，我们使用了更大型的初始模型，这些模型是公开可用的。我们使用 3 个 ResNeXt-101 (64x4d) [61]、2 个 SE-ResNeXt-101 (32x4d) [25]、1 个 ResNet-269 [64] 和 1 个 SENet [25] 作为边界框和掩码生成的集成。使用不同的大型初始模型的性能相似。一个 ResNeXt-101 (64x4d) 被用作生成候选框的基本模型。我们使用不同的随机种子和平衡采样 [55] 来增强模型之间的多样性来训练这些模型。我们提交的检测结果是通过收紧实例掩码获得的。我们在图 5 中展示了几个视觉结果——我们的大多数预测都具有很高的质量。

![image-20240729160459095](/img/panet/image-20240729160459095.png)

`图5. 每行图像分别是本文的模型在COCO test-dev, cityscape test和MVD test上的可视化结果。`

### 4.3. 在 Cityscapes 上的实验

**数据集和指标**

Cityscapes [7] 包含由车载摄像头拍摄的街景。有 2,975 张训练图像、500 张验证图像和 1,525 张测试图像，具有精细注释。另外 20k 张图像具有粗略注释，不包括在训练中。我们在 val 和 secret 测试子集上报告我们的结果。8 个语义类别带有实例掩码注释。每张图像的大小为 1024x2048。我们根据 AP 和 AP50 评估结果。

**超参数**

我们使用与 Mask R-CNN [21] 相同的超参数集进行公平比较。具体来说，我们在训练时随机从 {800, 1024} 中选择短边进行训练，并在推理时使用短边为 1024 的图像。没有使用测试技巧或 DCN。我们以学习率 0.01 训练模型 18k 次迭代，并以学习率 0.001 训练 6k 次迭代。每个图像批次中有 8 张图像(每个 GPU 1 张图像)。

ResNet-50 是此数据集上的初始模型。

**方法和结果**

我们在测试子集上与最先进的技术进行了比较，结果如表 8 所示。在“fine-only”数据上训练，我们的方法比使用“fine-only”数据的 Mask R-CNN 的性能提高了 5.6 个百分点。它甚至可以与在 COCO 上预训练的 Mask R-CNN 相媲美。通过在 COCO 上进行预训练，我们比使用相同设置的 Mask R-CNN 的性能提高了 4.4 个百分点。我们在图 5 中展示了视觉结果。

![image-20240729160910818](/img/panet/image-20240729160910818.png)

`表8. 结果在cityscape val子集上，记为AP [val]，在cityscape测试子集上，记为AP。`

我们对 val 子集上的改进进行的消融研究如表 9 所示。基于我们重新实现的基线，我们添加了多 GPU 同步批归一化，以帮助网络更好地收敛。它将精度提高了 1.5 个百分点。使用我们的完整 PANet，性能又提高了 1.9 个百分点。

![image-20240729160645261](/img/panet/image-20240729160645261.png)

`表9. 城市景观val子集消融研究结果。只有精细的注释用于训练。MBN是多gpu同步批处理规范化的缩写。`

### 4.4. 在 MVD 上的实验

MVD [45] 是一个相对较新且规模较大的实例分割数据集。它提供了 25,000 张街景图像，带有 37 个语义类别的精细实例级注释。它们是从几个国家使用不同的设备捕获的。内容和分辨率差异很大。我们在训练子集上使用 ResNet-50 作为初始模型训练我们的模型，并报告了在 val 和 secret 测试子集上根据 AP 和 AP50 的性能。

我们在表 10 中展示了我们的结果。与 LSUN 2017 实例分割挑战赛中的冠军 UCenter [40] 相比，我们的 PANet 使用一个在单尺度图像上测试的 ResNet-50 已经与在 COCO 上进行预训练的集成结果相当。通过使用 UCenter 也采用的多种尺度测试和水平翻转测试，我们的方法表现得甚至更好。定性的结果在图 5 中展示。

![image-20240729160743160](/img/panet/image-20240729160743160.png)

`表10. MVD值子集和测试子集的结果。`

## 5. 结论

我们提出了用于实例分割的 PANet。我们设计了一些简单而有效的组件，以增强代表性管道中的信息传播。我们从所有特征级别池化特征，并缩短了低层和顶层特征之间的距离，以进行可靠的信息传递。互补路径被增强以丰富每个候选框的特征。产生了令人印象深刻的结果。我们的未来工作是将我们的方法扩展到视频和 RGBD 数据。

## 附录

### A. Cityscapes 和 MVD 的训练细节和生成锚点的策略

在 Cityscapes [7] 上，我们采用了 Mask R-CNN [21] 中的训练超参数，并已在第 4.3 节中描述。RPN 锚点跨越 5 个尺度和 3 个宽高比，遵循 [21, 35]。而在 MVD [45] 上，我们采用了获胜者 [40] 中的训练超参数。我们以学习率 0.02 训练模型 60k 次迭代，并以学习率 0.002 训练 20k 次迭代。我们使用 16 张图像进行训练。我们设置输入图像的长边为 2400 像素，而另一个则从 600 到 2000 像素进行多尺度训练。我们采用了 {1600, 1800, 2000} 尺度进行多尺度测试。RPN 锚点跨越 7 个尺度，即 {82, 162, 322, 642, 1282, 2562, 5122}，和 5 个宽高比，即 {0.2, 0.5, 1, 2, 5}。RPN 使用与目标检测/实例分割网络训练相同的尺度进行训练。

### B. 实现 多 GPU 同步批归一化的细节

我们在 Caffe [29] 和 OpenMPI 上实现了多 GPU 批归一化。给定 n 个 GPU 和训练批次中的样本 B，我们首先将训练样本均匀地分割成 n 个子批次，每个子批次用 bi 表示，分配给一个 GPU。在每个 GPU 上，我们根据 bi 中的样本计算均值 µi。然后对所有 GPU 应用 AllReduce 操作来收集所有 µi，以获得整个批次 B 的均值 µB。µB 被广播到所有 GPU。然后我们独立地在每个 GPU 上计算临时统计数据，并应用 AllReduce 操作来生成整个批次 B 的方差 σ2 B。σ2 B 也被广播到所有 GPU。因此，每个 GPU 都具有在 B 上的所有训练样本上计算的统计数据。然后我们对每个训练样本执行归一化 $y_m = γ\frac{x_m−µ_B}{\sqrt{σ^2_B+ϵ}} + β$，如 [28] 中所示。在反向操作中，我们同样应用 AllReduce 操作来收集来自所有 GPU 的信息，以便进行梯度计算。