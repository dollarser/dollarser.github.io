---
title: Graph Attention Networks
date: 2024-08-21 17:21:00
toc: true
tags:
 - GAN
 - ICLR 2018
typora-root-url: ..
typora-copy-images-to: ..\img\gan
---

图形注意力网络
---------------

会议: ICLR 2018

论文地址：https://paperswithcode.com/paper/graph-attention-networks

github: https://github.com/PetarV-/GAT

开源库：[PyG](https://pytorch-geometric.readthedocs.io/en/latest/)



[TOC]

## 摘要

本文提出一种新的神经网络架构——图注意力网络（GAT），该网络可以处理具有图形结构的数据，并利用掩码自注意层来解决基于图卷积或其近似方法的先前方法的不足之处。通过将节点能够关注邻居特征的层堆叠起来，我们可以隐式地为邻居中的不同节点指定不同的权重，而无需进行任何昂贵的矩阵操作（如求逆）或依赖于事先知道图结构。这样，我们同时解决了谱基图神经网络模型的一些关键挑战，并使我们的模型适用于归纳和推断问题。实验结果表明，在四个已建立的归纳和推断图基准数据集上，GAT模型已经达到了或匹配了最先进的结果：Cora、Citeseer和Pubmed引用网络数据集以及一个蛋白质相互作用数据集（其中测试图在训练期间未被看到）。

<!--more-->

## 1 简介

卷积神经网络（CNN）已成功应用于诸如图像分类，语义分割或机器翻译等问题，在这些问题中，底层数据表示具有网格状结构。 这些架构通过将其应用于所有输入位置来有效地重复使用可学习参数的本地滤波器。 

然而，许多有趣的任务涉及的数据不能用网格结构来表示，而是在不规则域中。这种情况包括三维网格、社交网络、电信网络、生物网络或大脑连接组。这种数据通常可以以图形的形式呈现。 

在文献中已经有过一些尝试，将神经网络扩展到处理任意结构的图。早期的工作使用循环神经网络来处理以有向无环图表示的数据领域。图神经网络（GNN）由Gori等人（2005）、Scarselli等人（2009）引入，作为循环神经网络的一般化，可以直接处理更一般的图类型，例如循环、有向和无向图。图神经网络包括一个迭代过程，该过程传播节点状态直到达到平衡；然后通过神经网络基于每个节点的状态为每个节点产生输出 。李等人(2016)采纳并改进了这一想法，建议在传播步骤中使用门控循环单元(Cho等人，2014)。 

然而，人们越来越有兴趣将卷积推广到图领域。这方面的进展通常被归类为谱方法和非谱方法。 

一方面，谱方法使用图的谱表示，并已成功应用于节点分类。在 Bruna等人（2014）的工作中，卷积操作是在傅里叶域中定义的，通过计算图拉普拉斯矩阵的特征分解来实现，这可能导致大量的计算和非空间局部滤波器。后来的研究解决了这些问题。Henaff 等人。（2015）引入了具有平滑系数的谱滤波器参数化，以使它们在空间上局部化。随后，Defferrard 等人。（2016）提出通过图拉普拉斯矩阵的 Chebyshev 展开近似滤波器，从而避免了计算拉普拉斯特征向量的需求，并产生了空间局部化的滤波器。最后，Kipf 和 Welling（2017）通过限制滤波器仅在每个节点周围的一个步骤内运行来简化了此先前的方法。然而，在上述所有谱方法中，学习到的滤波器依赖于拉普拉斯本征基，而拉普拉斯本征基又取决于图结构。因此，基于特定结构训练的模型不能直接应用于具有不同结构的图。 

另一方面，我们有非频谱方法（Duvenaud et al.，2015；Atwood & Towsley，2016；Hamilton et al.，2017），它在图上直接定义卷积操作，在空间接近邻居组上进行操作。这些方法中的一些挑战包括如何为不同的大小邻域定义一个算子，并保持卷积神经网络的权重共享特性。在某些情况下，这需要为每个节点度数学习一个特定的权重矩阵（Duvenaud et al.，2015），使用过渡矩阵的幂来定义邻域，同时为每个输入通道和邻域度数学习权重（Atwood & Towsley，2016），或者提取并归一化包含固定数量节点的邻域（Niepert et al.，2016）。Monti等人（2016）提出了混合模型CNN（MoNet），这是一种为空间提供CNN架构统一泛化的空间方法。最近，Hamilton等人（2017）引入了GraphSAGE，这是一种用于递归采样的归纳近似的方法。该技术通过从每个节点采样一个固定大小的邻域，然后对其进行聚合（例如对所有采样邻居特征向量求平均值或将它们馈送到循环神经网络中）来实现。这种方法在几个大规模递归样本基准测试中表现出色。 

注意力机制已成为许多基于序列的任务的事实标准（Bahdanau等，2015年；Gehring等，2016年）。注意力机制的一个好处是它允许处理可变大小的输入，并关注输入中产生决策的相关部分。当使用注意力机制来计算单个序列的表示时，通常称为自注意或内注意。与递归神经网络（RNN）或卷积一起，自注意已被证明对于机器阅读理解（Cheng等，2016年）和学习句子嵌入（Lin等，2017年）等任务很有用。然而，Vaswani等人。 (2017) 表明，不仅自我注意可以改进基于循环神经网络或卷积的方法，而且自我注意足以构建一个强大的模型，在机器翻译任务上达到最先进的性能。 

受最近这项工作的启发，我们提出了一种基于注意力机制的架构来对图结构数据进行节点分类。其思想是在遵循自注意力策略的情况下，通过关注每个图中节点的邻居来计算它们的隐藏表示。这种注意力架构有几个有趣的性质：（1）操作效率高，因为可以在节点邻居对之间并行化；（2）可以通过为邻居指定任意权重来应用于具有不同度数的图节点；以及（3）该模型可以直接应用于归纳学习问题，包括模型必须推广到完全未见过的图的任务。我们在四个具有挑战性的基准测试集上验证了所提出的方案：Cora、Citeseer 和 Pubmed 引用网络，以及一个归纳蛋白质相互作用数据集，并取得了与最先进的结果相媲美的效果，这突显了当处理任意结构的图时，基于注意力的模型的潜力。 

值得注意的是，我们的工作也可以被重新解释为MoNet（Monti等人，2016）的一个特例。此外，我们共享神经网络边计算的方法类似于关系网络 (Santoro 等人，2017) 和 VAIN（Hoshen，2017） 的公式化方法，其中通过使用共享机制对对象或代理之间的关系进行逐对聚合。同样，我们提出的注意力模型可以与 Duan 等人的工作（2017）和 Denil 等人的工作（2017）相连接，这些工作使用邻域注意力操作来计算环境中不同对象之间的注意力系数。其他相关方法包括局部线性嵌入（LLE）(Roweis & Saul, 2000) 和记忆网络（Weston et al.，2014）。LLE 在每个数据点周围选择固定数量的邻居，并为每个邻居学习一个权重系数以将其重建为邻居的加权总和。第二个优化步骤提取该点的特征嵌入。记忆网络也与我们的工作有一些联系，特别是如果我们将节点的邻域解释为其内存，则可以使用它来计算节点功能，通过对它的值进行关注，然后通过将其新特性存储在同一位置来进行更新。 

## 2. GAT 架构

在这一部分，我们将介绍用于构建任意图注意力网络的基础层（通过堆叠该层），并直接概述它与神经图处理领域中现有工作的理论和实际好处和局限性。

### 2.1 图注意力层

我们将从描述单个图注意力层开始，该层在整个实验中用于所有GAT架构。我们使用的特定注意力设置与Bahdanau等人（2015）的工作非常相似——但框架对选择的注意力机制不敏感。 

我们的层输入是一组节点特征，即 $h = {\vec{h}_1, \vec{h}_2,..., \vec{h}_N}$，其中 $h_i \in R^F$，N是节点数，F是每个节点的功能数。该层产生一个新的节点特征（可能具有不同的基F’），$h'={\vec{h'}_1, \vec{h'}_2,..., \vec{h'}_N}$， $\vec{h'}_i∈R^{F'}$ 作为输出。 

为了获得足够的表达能力，以将输入特征转换为更高层次的特征，至少需要一个可学习的线性变换。为此，在初始步骤中，对每个节点应用共享的线性变换，由权重矩阵$W∈R^{F'×F}$参数化。然后在节点上执行自注意力——共享注意机制a：$R^{F'}×R^{F'}→R$ 计算注意力系数:

![image-20240827201542709](/img/gan/image-20240827201542709.png)

这表明节点 j 的特性对节点 i 有多么重要。在最一般的形式中，模型允许每个节点关注其他所有节点，放弃所有结构信息。我们通过执行遮蔽注意力来注入图结构到机制中——我们只计算 $N_i$ 中的节点 j 对应的 $e_{ij}$ ，其中 $N_i$ 是图中节点 i 的一些邻居。在我们的所有实验中，这些将是节点 i（包括 i）的 首先邻域。为了使系数在不同节点之间易于比较，我们使用 softmax 函数在所有选择 j 上对其进行归一化： 

![image-20240827201725207](/img/gan/image-20240827201725207.png)

在我们的实验中，注意机制 是一个单层前馈神经网络，由权重向量 $\vec{a}∈R^{2F'}$ ，并应用LeakyReLU （负输入斜率α=0.2 ）作为非线性激活函数。展开后，注意力机制计算出的系数（如图 1 (左) 所示）可以表示为： 

![image-20240827201739320](/img/gan/image-20240827201739320.png)

其中 $T$ 代表转置，$||$ 是拼接运算。 

一旦获得，归一化的注意力系数被用来计算它们对应的特征的线性组合，作为每个节点的最终输出特征（在潜空间应用非线性函数σ之后）：

![image-20240827201846655](/img/gan/image-20240827201846655.png)

为了稳定自注意力学习过程，我们发现将其扩展到多头注意力机制中是有益的，就像 Vaswani等人（2017）所做的那样。具体来说，K个独立的注意力机制执行方程4中的变换，然后它们的特征被连接起来，得到下面的输出特征表示：

![image-20240827201855208](/img/gan/image-20240827201855208.png)

其中，||表示连接操作，$α^k_{ij}$ 是由第 k 个注意力机制（$a^k$）计算得到的归一化注意力系数，$W^k$ 是对应的输入线性变换权重矩阵。请注意，在这种设置下，每个节点的最终返回输出 h' 将包含 $KF'$ 个特征（而不是 F'）。 特别地，如果我们在网络的最后一层（预测）上执行多头注意力，那么了连接操作就不再有意义了——相反，我们使用平均值，并且在那时才应用最终的非线性函数（通常为分类问题的softmax或logistic sigmoid）：

![image-20240827201908060](/img/gan/image-20240827201908060.png)

图 1 （右）显示了多头注意力层的聚合过程。

![image-20240827201832774](/img/gan/image-20240827201832774.png)

图1：左：我们的模型中采用的注意力机制a（Whhi，Whhj），由权重向量a∈R2F参数化，并应用LeakyReLU激活。右：节点1在其邻域上的多头注意力（K = 3个头）。不同的箭头样式和颜色表示独立的注意力计算。每个头的聚合特征通过连接或平均来获得h。1。 

### 2.2 与相关工作的比较

本节 2.1 中描述的图注意力层直接解决了神经网络处理图形结构数据时遇到的一些问题： 

+ 从计算的角度来看，它非常有效率：自注意力层的操作可以并行化到所有边缘上，输出特征的计算也可以并行化到 所有节点。不需要特征值分解或类似的昂贵矩阵操作。单个GAT注意力头计算'F'功能的时间复杂度可以表示为$O(|v|FF'+|E|F'）$，其中F是输入特征的数量，|V|和|E|分别是图中节点和边的数量。这种复杂性与基线方法相同，例如图形卷积网络（GCNs）(kipf & welling, 2017)。应用多头注意力会将存储和参数要求乘以一个因子K，而各个头部的计算完全独立，并且可以并行化。
+  与GCN不同，我们的模型允许（隐式）为同一邻域中的节点分配不同的重要性，从而提高了模型的能力。此外，分析学习到的注意力权重可能会带来可解释性的优势，就像在机器翻译领域一样（例如Bahdanau等人对2015年的定性分析）。 
+ 注意力机制以共享的方式应用于图中的所有边，因此它不依赖于提前访问全局图结构或其所有节点（许多先前技术的局限性）。这有几个可取之处：
  +  图不一定要是有向的（如果不存在从顶点 j 到顶点 i 的边，我们可以简单地忽略计算αij）。
  +  它使我们的技术可以直接应用于 *归纳* 学习，包括模型在训练过程中完全没有见过的图上进行评估的任务。 
+ 最近由汉密尔顿等人(2017)提出的方法对每个节点采样一个固定大小的邻域，以保持计算开销一致；这使得在推理过程中无法访问整个邻域。此外，当使用基于LSTM (Hochreiter & Schmidhuber, 1997) 的邻居聚合器时，该技术取得了最强的结果。这假设了不同邻域之间存在一致的时间顺序节点排列，并且作者通过始终向 LSTM 提供随机排序的序列来纠正这一点。我们的方法不会受到这两种问题的影响——它会处理所有邻域（代价是在可变计算开销的情况下），并且不假定其中任何一种顺序。 
+ 正如第 1 节所述，GAT 可以被重新表述为 MoNet 的一个特例（Monti 等人，2016）。具体来说，将伪坐标函数设置为 $u(x,y)=f(x)||f(y)$，其中 $f(x)$  表示节点 x 的特征 (潜在地通过 MLP 进行变换)，|| 是连接操作；权重函数设为$w_j(u) = softmax(MLP(u)) $（对整个节点邻域进行 softmax 操作），这会使 MoNet 的patch算子与我们的相似。然而，需要注意的是，与之前考虑过的 MoNet 实例相比，我们的模型使用了节点特征来进行相似性计算，而不是节点的结构属性（即假设提前知道了图结构）。 

我们能够构建一个版本的GAT层，它利用稀疏矩阵操作，将存储复杂性降低到与节点数和边数线性相关的，并使GAT模型能够在更大的图数据集上运行。然而，我们使用的张量操纵框架只支持对阶数为2的张量进行稀疏矩阵乘法，这限制了该层的批量处理能力（尤其是对于包含多个图的数据集）。适当地解决这一约束是未来工作的重要方向。根据图结构的规律性，GPU在这些稀疏情况下可能无法比CPU提供显著的性能提升。还应该注意的是，我们的模型的“感受野”的大小由网络深度所界定（与GCN和类似的模型相似）。可以很容易地应用诸如跳过连接（He等人，2016）等技术来适当扩展深度。最后，在所有图边缘上并行化，特别是在分布式方式下，可能会涉及大量冗余计算，因为感兴趣的图中邻域通常高度重叠。



## 3 实验评估

我们在四个基于图的标准基准任务（推断）上，对 GAT 模型进行了与多种强大的基线和先前方法的比较评估。 以及归纳法)，在所有这些方法中实现或匹配最先进的性能。 本节总结了我们的实验设置、结果，以及对 GAT 模型提取特征表示的大致定性分析。

### 3.1 数据集

**传导学习**  我们使用三个标准引用网络基准数据集——Cora、Citeseer 和 Pubmed（Sen 等，2008 年）——并密切遵循 Yang 等人（2016 年）的传导实验设置。在所有这些数据集中，节点对应于文档，边对应于（无向）引文。节点特征对应于文档的词袋表示中的元素。每个节点都有一个类别标签。我们只允许每类使用 20 个训练节点，但是为了遵守传导设置，训练算法可以访问所有节点的功能向量。对经过训练的模型的预测能力是在 1000 个测试节点上评估的，我们在验证目的时使用了另外 500 个节点（与 Kipf 和 Welling（2017）所使用的相同）。 Cora 数据集包含 2708 个节点、5429 条边、7 类和每个节点 1433 个特征。Citeseer 数据集包含 3327 个节点、4732 条边、6 类和每个节点 3703 个特征。Pubmed 数据集包含 19717 个节点、44338 条边、3 类和每个节点 500 个特征。 

**归纳学习**  我们使用了蛋白质相互作用 (PPI) 数据集，其中包含对应于不同人类组织的图（Zitnik 和 Leskovec，2017）。该数据集包含 20 个训练图、2 个验证图和 2 个测试图。在训练过程中，测试图保持完全不可见。为了构造这些图，我们使用了 Hamilton 等人（2017）提供的预处理数据。每个图中的平均节点数为 2,372。每个节点有 50 个特征，由基因组定位集合、模式基因集和免疫学标签组成。来自分子签名数据库(Molecular Signatures Database)的基因本体(Gene Ontology)中每个节点集有 121 个标签收集自Subramanian等人，2005)，一个节点可以同时具有多个标签。 表 1 提供了数据集有趣特征的概述。

表1：我们实验中使用的数据集摘要。

|                     | Cora           | Citeseer       | Pubmed                        | PPI               |
| ------------------- | -------------- | -------------- | ----------------------------- | ----------------- |
| Task                | Transductive   | Transductive   | Transductive  19717(1  graph) | Inductive         |
| #  Nodes            | 2708(1  graph) | 3327(1  graph) |                               | 56944(24  graphs) |
| #  Edges            | 5429           | 4732           | 44338                         | 818716            |
| #  Features/Node    | 1433           | 3703           | 500                           | 50                |
| #  Classes          | 7              | 6              | 3                             | 121(multilabel)   |
| #  Training Nodes   | 140            | 120            | 60                            | 44906(20  graphs) |
| #  Validation Nodes | 500            | 500            | 500                           | 6514(2  graphs)   |
| #  Test Nodes       | 1000           | 1000           | 1000                          | 5524(2  graphs)   |



### 3.2 目前最先进的方法

**传导学习**  在传导学习任务中，我们使用与Kipf＆Welling（2017）指定的一致的相同强大的基线和最先进的方法进行比较。 这包括标签传播 (LP) (Zhu 等人，2003)，半监督嵌入 (SemiEmb) (Weston 等人，2012)，流形正则化 (ManiReg) (Belkin 等人，2006)，基于skip-gram 的图嵌入 (DeepWalk) (Perozzi 等人，2014)，迭代分类算法 (ICA) (Lu 和Getoor，2003) 和Planetoid (Yang 等人，2016)。 我们还将我们的模型直接与GCN (Kipf 和Welling, 2017) 进行比较，以及使用高阶Chebyshev滤波器的图卷积模型(Deferrard等人，2016)，以及在Monti等人(2016)中介绍的MoNet模型。

**归纳学习**  在归纳学习任务中，我们比较了四种不同的监督式GraphSAGE归纳方法，这些方法在Hamilton等人，(2017)年提出。这些方法提供了多种方法来聚合采样邻居内的特征： GraphSAGE-GCN（将图卷积风格的操作扩展到归纳设置）, GraphSAGE-mean（取 GraphSAGE-GCN（通过GCN聚合邻居特征向量）、GraphSAGE-LSTM（通过将邻居功能馈入LSTM进行聚合）和GraphSAGE-pool（对共享非线性多层感知器转换后的特征向量执行元素最大操作）。其他归纳方法要么在归纳设置中完全不适用，要么假设节点按顺序添加到单个图中，使其无法用于测试图在训练期间完全不可见的情况（例如PPI数据集）。 

此外，对于这两个任务，我们提供了每个节点共享多层感知器（MLP）分类器的性能（根本没有包含图形结构）。

### 3.3 实验装置

**传导学习**  在传导学习任务中，我们使用了两层的GAT模型。其结构超参数已在 Cora 数据集上进行了优化，并在 CiteSeer 上重复使用。第一层由 K=8 个注意力头组成，每个头计算 F'=8 个特征（总计 64 个特征），后面跟着一个指数线性单元 (ELU) 非线性函数(Clevert 等人，2016)。第二层用于分类：一个单一的注意力头来计算 C 个特征（其中 C 是类的数量），后面跟着一个 softmax 激活。为了应对小规模训练集，我们在模型中广泛地应用正则化。在训练过程中，我们对 λ=0.0005 的 L2 正则化进行应用。此外，在两个层的输入以及归一化的注意力权重（这一点至关重要）处都应用了概率为 0.6 的dropout (Srivastava 等人，2014)。与 Monti 等人（2016 年）观察到的结果类似，我们发现在 PubMed 的训练数据大小（60 个示例）需要对 GAT 架构进行一些修改：我们应用了 K=8 个输出注意力头（而不是一个），并将 L2 正则化加强至 λ=0.001。否则，架构与用于 Cora 和 CiteSeer 的架构相同。 

**归纳学习**  对于归纳学习任务，我们使用了三层 GAT 模型。前两层每层包含 K=4 个注意力头来计算 F'=256维 特征（总共 1024 维特征），后面跟着一个 ELU 非线性函数。最后一层用于 (多标签) 分类：有 K=6 个注意力头，每个头计算 121维 特征，然后求平均值并接上逻辑sigmoid 激活。这个任务的数据集足够大，因此我们发 现没有必要应用 L2 正则化或丢弃法——然而，我们在中间注意层之间成功地应用了跳 跃连接（He等人，2016）。在训练过程中，我们使用了包含 2 个图的批次大小。为了严格评估在这种情况下应用注意力机制的好处（即与近似 GCN 相比），当使用恒 定注意力机制a(x,y)=1时，我们也提供了相同结构的结果——这将为每个邻居分配相同的权重。 

两个模型都使用 Glorot 初始化（Glorot 和 Bengio，2010 年）进行初始化，并使用 Adam SGD 优化器 (Kingma 和 Ba，2014 年) 在训练节点上最小化交叉熵。在PubMed中初始学习率为0.01，在所有其他数据集上的初始学习率为0.005。我们在验证节点上对交叉熵损失和准确度（推断）或微 F1 分数（归纳）采用早期停止策略，每个周期为100个时期。

### 3.4 结果

我们的比较评估实验的结果总结在表2和表3中。 
对于*传导任务*，我们在 100 次运行后报告了我们方法在测试节点上的平均分类精度（标准差），并为最先进的技术重用了已在Kipf＆Welling（2017）和Monti等人（2016）中报道的指标。具体而言，对于基于Chebyshev滤波器的方法（Deferrard等人，2016），我们提供了最大报告性能，用于滤波器阶数K = 2和K = 3。为了公平地评估注意力机制的好处，我们进一步评估了一个计算64个隐藏特征的GCN模型，并尝试使用ReLU和ELU激活，并在100次运行后报告结果（作为GCN-64* ），这是所有三种情况下ReLU的结果。 

表2:Cora、Citseeer和Pubmed的分类准确率结果总结。GCN-64*对应于计算64个隐藏特征的最佳GCN结果（使用ReLU或ELU）。

Transductive

| Method                              | Cora         | Citeseer     | Pubmed       |
| ----------------------------------- | ------------ | ------------ | ------------ |
| MLP                                 | 55.1%        | 46.5%        | 71.4%        |
| ManiReg(Belkin  et al., 2006)       | 59.5%        | 60.1%        | 70.7%        |
| SemiEmb(Weston  et al., 2012)       | 59.0%        | 59.6%        | 71.7%        |
| LP(Zhu  et al., 2003)               | 68.0%        | 45.3%        | 63.0%        |
| DeepWalk(Perozzi  et al., 2014)     | 67.2%        | 43.2%        | 65.3%        |
| ICA(Lu&  Getoor, 2003)              | 75.1%        | 69.1%        | 73.9%        |
| Planetoid(Yang  et al., 2016)       | 75.7%        | 64.7%        | 77.2%        |
| Chebyshev(Defferrard  et al., 2016) | 81.2%        | 69.8%        | 74.4%        |
| GCN(Kipf&  Welling, 2017)           | 81.5%        | 70.3%        | 79.0%        |
| MoNet(Monti  et al., 2016)          | 81.7  ± 0.5% | —            | 78.8  ± 0.3% |
| GCN-64∗                             | 81.4  ± 0.5% | 70.9  ± 0.5% | 79.0  ± 0.3% |
| GAT(ours)                           | 83.0  ± 0.7% | 72.5  ± 0.7% | 79.0  ± 0.3% |

对于*归纳任务*，我们在两个未见过的测试图中的节点上报告了微观平均F1分数， 平均10次运行后，复现Hamilton等人（2017）中已经报告的指标用于其他技术。具体来说，由于我们的设置是监督式的，我们与 监督式GraphSAGE方法 进行了比较。为了评估在所有邻居中聚合的好处，我们进一步提供了 (作为GraphSAGE*) 我们通过简单修改其体系结构(这是使用具有 [512, 512, 726] 特征的三层GraphSAGE-LSTM在每个层中计算，并且用于聚合邻居的128个特征)所能实现的最佳结果。最后，我们报告了我们恒定注意力GAT模型 (作为Const-GAT) 的10次运行的结果，以公平地评估注意机制相对于GCN类聚合方案的优势（具有相同架构）。 

表3：以微平均F1分数为指标，对PPI数据集的结果进行总结。GraphSAGE∗ 是通过修改其架构可以获得的最佳 GraphSAGE 结果。Const-GAT 是一个与 GAT 架构相同的模型，但具有恒定注意力机制（为每个邻居分配相同的重要性；类似于 GCN 的归纳算子）。

Inductive

| Method                                 | PPI                            |
| -------------------------------------- | ------------------------------ |
| Random                                 | 0.396                          |
| MLP                                    | 0.422                          |
| GraphSAGE-GCN(Hamilton  et al., 2017)  | 0.500                          |
| GraphSAGE-mean(Hamilton  et al., 2017) | 0.598                          |
| GraphSAGE-LSTM(Hamilton  et al., 2017) | 0.612                          |
| GraphSAGE-pool(Hamilton  et al., 2017) | 0.600                          |
| GraphSAGE∗                             | 0.768                          |
| Const-GAT(ours)                        | 0.934  ± 0.006                 |
| GAT(ours)                              | 0.973  ± 0.002                 |

我们的结果成功地在所有四个数据集上实现了最先进的性能——这与我们在第2.2节中的讨论一致。具体来说，我们能够以1.5%和1.6%的差距提高GCN在Cora和Citeseer上的表现，表明给同一邻域的不同节点分配不同的权重可能是有益的。值得注意的是，在PPI数据集上取得的进步：我们的GAT模型比我们能够获得的最佳GraphSAGE结果提高了20.5%，证明了我们的模型具有归纳能力，并且可以通过观察整个邻域来利用更大的预测能力。此外，它比恒定注意力机制（Const-GAT，具有相同架构但具有常数注意力机制）改进了3.9%，再次直接证明了能够为不同的邻居分配不同权重的重要性。 

我们还可以从定性的角度来研究学习到的特征表示的有效性——为此，我们在图 2 中展示了使用 t-SNE (Maaten & Hinton, 2008) 对 Cora 数据集预训练的 GAT 模型的第一层提取的特征表示进行变换后的结果。该表示在投影的二维空间中显示了可识别的聚类。请注意，这些聚类对应于数据集中的七个标签，验证了模型在 Cora 的七个主题类别上的判别能力。此外，我们还可视化了归一化注意力系数（所有八个注意力头平均）的相对强度。要正确解释这些系数（例如，Bahdanau 等人。2015年），需要对所研究的数据集有进一步的领域知识，并留给未来的工作。 

![image-20240827202313901](/img/gan/image-20240827202313901.png)

图2: Cora数据集上预训练GAT模型第一个隐藏层的计算特征表示的t-SNE图。节点颜色表示类。边缘厚度表示所有八个注意力头（$\sum^K_{k=1}α^k_{ij}+α^k_{ji}$）节点i和j之间的聚集归一化注意力系数。



## 4 结论

我们提出了图注意力网络（GAT），这是一种新颖的卷积神经网络架构，用于处理图形数据，它利用了掩码自我关注层。这些模型中使用的所有图注意力层都具有计算效率（不需要昂贵的矩阵操作，并且可以在图的所有节点上并行计算），可以为同一邻域内不同大小的邻域分配不同的重要性，并且无需提前知道整个图结构——从而解决了许多先前谱方法中的理论问题。我们的注意力模型在四个公认的节点分类基准测试中成功实现了最先进的性能或匹配性能，包括归纳和推断（特别是当完全未见过的图用于测试时）。 

有几个潜在的改进和扩展图注意力网络可以作为未来的工作，比如克服子节2.2中描述的实际问题，能够处理更大的批量大小。一个特别有趣的 研究方向 将是利用注意机制对模型进行深入分析，以提高其可解释性。此外，从应用的角度来看，将方法扩展到 图分类 而不是节点分类也是相关的。最后，将模型扩展到包含边特征（可能指示节点之间的关系）将使我们能够解决更广泛的问题。

