---
title: Semi-Supervised Classification with Graph Convolutional Networks
date: 2024-08-21 16:41:00
tags:
 - GCN
 - ICLR 2017
typora-root-url: ..
typora-copy-images-to: ..\img\gcn
---

基于图卷积网络的半监督分类（GCN）
---------------


会议: ICLR 2017

论文地址：https://arxiv.org/abs/1609.02907

github: https://github.com/tkipf/pygcn

[TOC]

## 摘要

本文提出了一种可扩展的方法来处理图结构数据上的半监督学习，该方法基于一种高效的卷积神经网络变体，它直接在图上操作。本文通过局部一阶近似谱图卷积，优化我们的卷积架构的选择。我们的模型与图中边的数量线性相关，并且可以学习编码了图的局部结构和节点特征的隐藏层表示。我们在引用网络和知识图数据库上的一系列实验中展示了我们的方法相比其他相关方法具有显著优势。

## 1 简介

我们考虑在图（如文献引用网络）中对节点（如文档）进行分类的问题，其中仅有一小部分节点有标签。这个问题可以被看作基于图的半监督学习，通过某种显式的基于图的正则化形式来平滑（迁移）标签信息到图中，例如，在损失函数中使用图拉普拉斯正则化项：

![image-20240828110529296](/img/gcn/image-20240828110529296.png)

式中，$L_0 $表示与图中带标签部分相关的监督损失。$f(·)$可以是类似于神经网络的可微函数，$λ$ 是一个权重因子，$X$ 是节点特征向量矩阵 $X_i$。$\Delta = D-A$ 是无向图 $G(V, E) $的未归一化拉普拉斯算子，其中 $G$包含 N 个节点 $v_i∈V$，边 $(v_i, v_j)∈E$，邻接矩阵 $A∈R^{N×N}$（二进制或加权），度数矩阵 $D_{i,i}=∑_jA_{ij}$。公式 (1) 的推导基于假设：图中的连接节点可能具有相同的标签。然而，这个假设可能会限制建模能力，因为图的边不一定编码了节点的相似性，而是可能包含额外的信息。

在本工作中，我们使用神经网络模型$f(X, A)$直接对图结构进行编码，并在有标签的所有节点上以监督方式训练目标$L_0$，从而避免在损失函数中显式引入基于图的正则化(归纳偏置)。 将$f(·)$ 概念化为图的邻接矩阵允许模型从监督损失$L_0$ 中传播梯度信息，并使其能够学习带有或不带有标签的节点表示。 

我们的贡献有两方面。首先，我们引入了一种简单且行为良好的 **层次传播规则** 用于直接在图上操作的神经网络模型，并展示了如何从谱图卷积（Hammond等人，2011）的一阶近似中得到该规则。其次，我们展示了这种基于图的神经网络模型可以快速、可扩展的用于 图节点的半监督分类 。在多个数据集上的实验表明，与最先进的半监督学习方法相比，我们的模型在分类准确率和效率（以时钟时间衡量）方面都具有优势。



## 2 快速图近似卷积

在这一部分，我们为本文其余部分所使用的基于图的神经网络模型$f(X，A)$提供理论驱动（基础）。 我们考虑多层图卷积网络（GCN）具有以下层传播规则：

![image-20240828110622809](/img/gcn/image-20240828110622809.png)

在这里，$\widetilde{A} = A + I_N $是无向图 G 包含了自连接的邻接矩阵，$I_N$ 是单位矩阵; $\tilde D_{i,i} = ∑_j~A_{i,j}$，$W(l)$ 是一个特定层的可训练权重矩阵。$σ(\cdot)$ 表示激活函数，如 $\operatorname{ReLU}(\cdot) = \max(0,\cdot)$。$H^{(l)} \in \mathbb{R}^{N\times D}$ 是第$l$ 层的激活值矩阵；$H(0)=X$。在下面，我们将展示这种传播规则的形式可以通过对图的局部谱滤波的一阶近似来解释（Hammond等人，2011年；Defferrard等人，2016年）。



### 2.1 谱图卷积

我们考虑在图上进行谱卷积，定义为信号$x∈RN$（每个节点都是标量）与滤波器$g_θ=diag(θ)$的乘法，其中$θ∈R^N$ 在傅立叶(Fourier)域中，即： 

![image-20240828114415901](/img/gcn/image-20240828114415901.png)

其中 $U$ 是归一化图拉普拉斯矩阵 $L=I_N−D^{−1/2}AD^{−1/2}=UΛU^T$，其中 Λ 是其特征值的对角线矩阵，且$ U^Tx$是 x 的图傅里叶变换。我们可以将 $g_θ$ 理解为关于 L 的特征值的函数，即 $g_θ(Λ)$。求解方程 (3) 非常昂贵，因为与特征向量矩阵 U 相乘需要 $O(N^2)$ 的计算量。此外，对于大型图来说，首先计算 L 的特征值分解可能非常昂贵。为了克服这个问题，Hammond等人(2011)建议通过截断Chebyshev多项式$T_k(x)$的级数来很好地近似$g_θ(Λ)$: 

![image-20240828114442499](/img/gcn/image-20240828114442499.png)

其中，$~{\boldsymbol \Lambda} = 2/\lambda_{\max} \times {\boldsymbol \Lambda}-\boldsymbol I_N$。其中，$\lambda_{\max}$ 是 L 的最大特征值。$\theta'$ 是 Chebyshev 样本系数向量。Chebyshev 多项式递归定义为：$T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x)$，其中 $T_0(x) = 1$,  $ T_1(x) = x$。有关此近似式的详细讨论，请参阅 Hammond 等人(2011)。

回到信号x与滤波器$g_{θ'}$的卷积定义，我们现在有： 

![image-20240828114629602](/img/gcn/image-20240828114629602.png)

其中，$\tilde L=2/λmax L−I_N$； 这可以通过注意到$(UΛU^T)^k=UΛ^kU^T$来轻松验证。 注意到这个表达式现在已经被K局部化了，因为它是一个关于拉普拉斯算子的K阶多项式，也就是说它只依赖于距离中心节点最多K步远的节点（K阶邻域）。 求解方程(5)的复杂度为$O(|E|)$，即图中边的数量线性增长。 Deferrard等人（2016）使用这种K局部卷积来定义图形上的卷积神经网络。

### 2.2 层级线性模型

因此，可以构建一个基于图卷积的神经网络模型，该模型由多个具有形式如等式(5)  的卷积层堆叠而成，每个层后跟一个点非线性。现在假设我们将 层之间的 卷积操作限制为$ K = 1$（参见等式 5），即一个关于 L 线性的函数，因此在图拉普拉斯谱上也是线性的函数。

通过堆叠多个这样的层，我们仍然可以恢复一个丰富的卷积滤波器函数类，但我们不局限于由Chebyshev多项式等给出的显式参数化。 我们直观地期望这种模型能够缓解非常宽的节点度分布图（如社交网络、引文网络、知识图和许多其他现实世界图形数据集）上的局部邻域结构过拟合问题。 此外，在给定计算预算的情况下，这种分层线性建模使我们能够构建更深的模型，而众所周知，在许多领域中这可以提高建模能力(何等人，2016年)。 

在 GCN 的线性公式中，我们进一步近似$λmax≈2$，因为我们可以预期神经网络参数将在训练过程中适应这种尺度变化。在这些近似下，方程（5）简化为： 

![image-20240828114755457](/img/gcn/image-20240828114755457.png)

其中有两个自由参数$θ'_0$ 和$θ'_1$。 这个滤波器的参数可以应用在整个图上。 然后，连续应用这种形式的滤波器实际上有效地卷积了节点的k阶邻域，其中k 是神经网络模型中连续滤波操作或卷积层的数量。 

实际上，为了应对过拟合问题并最小化每层的操作数量（如矩阵乘法），限制参数的数量可能更有益。 这给我们留下了以下表达式： 

![image-20240828114836943](/img/gcn/image-20240828114836943.png)

其中 $θ = θ'_0=-θ'_1$。注意，$I_N + D^{−1/2}AD^{−1/2}$ 现在具有特征值范围 [0, 2]。因此，反复应用此算子可能导致在深度神经网络模型中使用时出现数值不稳定性和爆炸/消失梯度的问题。为了解决这个问题，我们引入了以下重新缩放技巧：$I_N + D^{−1/2}AD^{−1/2} →  \tilde D^{−1/2} \tilde A \tilde D^{−1/2}$，其中 $\tilde A = A + I_N $ 和 $\tilde D_{ii} = \sum_j \tilde A_{ij}$。 

我们可以推广这个定义，将其应用于一个信号$X \in R^{N\times C}$包含 C 个输入通道（即每个节点都有一个 C 维特征向量）和 F 个滤波器或特征图：
$$
Z = \tilde D^{−1/2} \tilde A \tilde D^{−1/2} X Θ
$$
其中 $Θ \in R^{C×F}$ 是一个滤波器参数矩阵，$Z∈R^{N×F}$ 是卷积信号矩阵。由于可以高效地实现 $\tilde AX$ 的乘法运算，因此该滤波操作具有复杂度 $O(|E|FC)$。

## 3. 半监督节点分类

在引入了一个简单而灵活的模型 $f(X,A)$，用于高效地在图上传播信息后，我们可以回到半监督节点分类的问题。如引言中所述，我们可以通过使我们的模型 $f(X,A)$ 同时基于数据X和基本图形结构的邻接矩阵A来放松通常在基于图形的半监督学习中所做的某些假设。我们预计在这种设置下，在邻接矩阵包含数据X中不存在的信息的情况下，例如引用网络中的文档之间的引用链接或知识图谱中的关系时，该设置会特别强大。总体模型，多层GCN为半监督学习，用流程图1示意。

### 3.1 示例

在下面，我们考虑一个两层图卷积网络（GCN）用于具有对称邻接矩阵 A（二进制或加权）的图形半监督节点分类。首先我们在预处理步骤中计算估计的邻接矩阵：$\hat{A}=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$。我们的前向模型采用简单的形式： 

![image-20240828142945194](/img/gcn/image-20240828142945194.png)

式中，$W^{(0)} \in R^{C\times H}$ 是一个输入到隐藏层的权重矩阵，具有 H 个特征图。$W^{(1)} \in R^{C\times F}$ 是一个隐藏到输出的权重矩阵。softmax 激活函数定义为 $softmax(x_i ) = 1/Z \times exp(x_i)$ ，其中 $Z = ∑_i exp(x_i)$ 。它被应用于每一行。对于半监督多类分类，我们计算所有标记示例的交叉熵误差： 

其中$Y_L$ 是有标签的节点索引集合。 

神经网络权重 $W(0)$ 和 $W(1)$ 使用梯度下降进行训练。在本工作中，我们在每个训练迭代中使用整个数据集执行批量梯度下降，只要数据集适合内存，这是一个可行的选择。通过稀疏表示A，内存需求为$O(|E|)$，即与边数线性相关。通过dropout（Srivastava等人，2014）引入训练过程中的随机性。我们将在未来的工作中考虑具有 mini-batch 随机梯度下降的内存效率扩展。

![image-20240828143058162](/img/gcn/image-20240828143058162.png)

图 1：左：输入层通道数为C和输出层特征图为F 的多层图卷积网络 (GCN) 的示意图。图结构（用黑色线条表示）在各层共享，标签由 Y 表示。右：使用 5% 标签训练的两层 GCN 在 Cora 数据集（Sen 等人，2008 年）上的隐藏层激活的 t-SNE 可视化（Maaten 和 Hinton，2008）。颜色表示文档类别。 

### 3.2 实现

在实现时，我们使用 Abadi 等人 (2015) 的 TensorFlow 来实现一个基于 GPU 的高效的稀疏-稠密矩阵乘法来计算公式(9)的效率。公式(9) 的计算复杂度为 $O(|E|CHF)$ ，即图中边的数量呈线性增长。

## 4. 相关工作

我们的模型既受到基于图的半监督学习领域的启发，也受到最近关于在图上操作的神经网络的工作的启发。在接下来的内容中，我们将对这两个领域中的相关工作进行简要概述。

### 4.1 基于图的半监督学习

近年来，已经提出了许多使用图表示进行半监督学习的方法，其中大部分可以分为两大类：使用某种形式的显式图拉普拉斯正则化的方法和基于图嵌入的方法。图拉普拉斯正则化的著名例子包括标签传播（Zhu等人，2003年）、流形正则化（Belkin等人，2006年）和深度半监督嵌入（Weston等人，2012年）。 

最近，注意力转向了使用启发自 skip-gram 模型（Mikolov 等人2013）的方法来学习图嵌入的模型。DeepWalk (Perozzi 等人2014)通过预测从图中随机游走采样到的节点的局部邻域来学习嵌入。LINE (Tang 等人，2015 年) 和 node2vec (Grover & Leskovec, 2016) 使用更复杂的随机游走或广度优先搜索方案扩展了 DeepWalk。然而，对于所有这些方法，都需要一个多阶段管道，包括生成随机游走和半监督训练，在每个阶段都必须单独优化。Planetoid (Yang 等人2016 )通过在嵌入学习过程中注入标签信息来缓解这种情况。

### 4.2 图上的神经网络

之前在Gori等人(2005); Scarselli等人(2009)中已经介绍了基于图的神经网络，作为一种循环神经网络的形式。他们的框架要求应用收缩映射作为传播函数，直到节点表示达到稳定不动点。随后Li等人,2016通过将现代循环神经网络训练方法引入到原始图形神经网络框架中来缓解了这一限制。Duvenaud等人,2015 在图上引入了一个类似于卷积的传播规则以及用于图分类的方法。他们的方法需要学习与节点度相关的权重矩阵，这并不适用于具有宽泛的节点度分布的大图。相反，我们的模型为每层使用一个权重矩阵，并通过适当归一化邻接矩阵来处理变化的节点度（见第3.1节）。 

Atwood 和 Towles（2016 年）最近提出了一种使用基于图的神经网络对节点进行分类的近似方法。他们报告了 $O(N^2)$ 复杂度，限制了可能的应用范围。在不同的但相关的模型中，Niepert 等人（2016 年）将本地图形转换为序列，然后馈入常规 1D 卷积神经网络，这需要在预处理步骤中定义一个节点顺序。 

我们的方法基于谱域图卷积神经网络，最初由 Bruna等人(2014)，后来由Defferrard等人(2016)通过快速局部卷积扩展。与这些工作不同的是，在这里我们考虑了在网络中进行归纳节点分类的任务。我们证明，在这种情况下，可以对 Bruna 等人的原始框架和Defferrard等人引入一些简化（见第2.2节），这提高了大规模网络中的可伸缩性和分类性能。

## 5 实验

我们在多个实验中测试了我们的模型：基于引用网络的半监督文档分类、从知识图谱中提取的二分图中的半监督实体分类、对各种图传播模型的评估以及在随机图上的运行时分析。

### 5.1 数据集

我们密切遵循 杨等(2016) 中的实验设置。数据集统计信息总结在表1中。在 引文网络数据集——CiteSeer、Cora 和 PubMed(Sen 等，2008)中——节点是文档，边是引文链接。标签比例表示用于训练的带标签节点数与每个数据集中所有节点总数之比。NELL(Carlson 等，2010；Yang 等，2016)是从知识图谱中提取出的二分图数据集，其中包含 55,864 个关系节点和 9,891 个实体节点。

Table 1: Dataset statistics, as reported in Yang et al.(2016).

| Dataset  | Type             | Nodes  | Edges   | Classes | Features | Label rate |
| -------- | ---------------- | ------ | ------- | ------- | -------- | ---------- |
| Citeseer | Citation network | 3,327  | 4,732   | 6       | 3,703    | 0.036      |
| Cora     | Citation network | 2,708  | 5,429   | 7       | 1,433    | 0.052      |
| Pubmed   | Citation network | 19,717 | 44,338  | 3       | 500      | 0.003      |
| NELL     | Knowledge graph  | 65,755 | 266,144 | 210     | 5,414    | 0.001      |

**引文网络**  我们考虑三个引文网络数据集：Citeseer、Cora 和 Pubmed（Sen 等，2008）。这些数据集包含每个文档的稀疏词袋特征向量以及文档之间的引用链接列表。我们把引文链接视为无向边，并构建一个二进制对称邻接矩阵A。每篇论文都有一个类别标签。在训练中，我们仅使用每个类别的20个标签，但所有特征向量都包含在内。 

**NELL**  是一个从知识图中提取的数据集，该知识图由 (Carlson et al., 2010) 中介绍。知识图是由实体节点及其有向、带标签的边（关系）组成的集合。我们遵循了 Yang et al. (2016) 所描述的预处理方案。对于每个实体对 $(e_1, r, e_2)$，我们为它们分配单独的关系节点 $r_1 和r_2$，即 $(e_1, r_1)$ 和 $(e_2, r_2)$。实体节点由稀疏特征向量来表示。通过为每个关系节点分配唯一的 one-hot 表示，我们将 NELL 的特征数扩展到每个节点都有一个 61,278 维的稀疏特征向量。这里的半监督学习任务考虑的是训练集中每个类仅有一个标记示例的极端情况。我们根据图中的节点 i 和 j 之间是否存在一条或多条边来构建二进制且对称的邻接矩阵 $A_{ij}=1$。 

**随机图**  我们模拟各种大小的随机图数据集进行实验，其中我们测量每个时期的训练时间。对于一个有N个节点的数据集，我们创建一个随机图，均匀地随机分配2N条边。我们将单位矩阵$I_N$作为输入特征矩阵X，从而隐式地采用无特征的方法，模型只知道每个节点的身份，由唯一的one-hot向量指定。我们为每个节点添加虚拟标签$Y_i = 1$。

### 5.2 实验设置

除非另有说明，我们在第 3.1 节中描述的两层图卷积网络进行训练，并在包含 1,000 个标记示例的测试集上评估预测准确性。我们在附录 B 中提供了使用深度模型（最多有 10 层）的额外实验。我们选择与 Yang 等人(2016)相同的拆分数据集，其中包含一个包含 500 个标记示例的验证集，用于超参数优化（所有层的丢弃率、第一层图卷积网络的 L2 正则化因子和隐藏单元数）。我们不使用验证集标签进行训练。 

对于引用网络数据集，我们在 Cora 上优化超参数，并在 Citeseer 和 Pubmed 上使用相同的参数。我们使用 Adam (Kingma & Ba, 2015) 对所有模型进行训练，学习率为 0.01 ，最大训练轮数为 200 轮，并且使用包含 10 个元素的滑动窗口实现早停法，即如果验证损失连续 10 次没有下降，则停止训练。我们使用 Glorot & Bengio(2010) 中描述的初始化方法来初始化权重，并相应地对输入特征向量进行行归一化。对于随机图数据集，我们隐藏层大小设置为 32 个单元，省略了正则化（即不使用 dropout 或 L2 正则化）。

### 5.3 基线

我们与杨等人（2016）中使用的相同基线方法进行比较，即标签传播（LP）（Zhu等人，2003），半监督嵌入（SemiEmb）（Weston等人，2012），流形正则化（ManiReg）（Belkin等人，2006）和基于跳字词向量的图嵌入（DeepWalk）（Perozzi等人，2014）。 我们省略了TSVM（Joachims，1999），因为它无法扩展到我们的一个数据集中的大量类。 

我们进一步与卢格托尔 (Lu & Getoor, 2003) 提出的迭代分类算法进行比较，该算法结合了两个逻辑回归分类器：一个用于仅使用本地节点特征的局部分类，另一个用于使用本地特征和聚合算子的关联分类，如 Sen 等人所述。(2008)。我们首先使用所有带标签的训练集节点来训练本地分类器，并将其用于为关联分类器训练生成未标记节点的类标签。我们在所有未标记节点（由本地分类器进行 bootstrapping）上对随机排列的节点运行迭代分类（关联分类器），共进行了 10 次迭代。对于每个数据集，我们根据验证集性能选择正则化参数 L2 和聚合算子（计数vs.比例，参见Sen等人。, 2008）。 

最后，我们使用Planetoid（Yang等人，2016）进行比较，在此我们总是选择其最佳性能模型变体（推断与归纳之间）作为基线。

## 6 结果

### 6.1 半监督节点分类

结果总结在表2中。报告的数量表示百分比分类精度。对于ICA，我们报告了随机节点顺序下100次运行的平均准确率。所有其他基线方法的结果来自Planetoid论文（Yang等人，2016）。Planetoid* 表示他们在论文中介绍的不同变体中的相应数据集的最佳模型。

Table 2: Summary of results in terms of classiﬁcation accuracy(in percent).

| Method          | Citeseer  | Cora      | Pubmed    | NELL       |
| --------------- | --------- | --------- | --------- | ---------- |
| ManiReg[3]      | 60.1      | 59.5      | 70.7      | 21.8       |
| SemiEmb[28]     | 59.6      | 59.0      | 71.1      | 26.7       |
| LP[32]          | 45.3      | 68.0      | 63.0      | 26.5       |
| DeepWalk[22]    | 43.2      | 67.2      | 65.3      | 58.1       |
| ICA[18]         | 69.1      | 75.1      | 73.9      | 23.1       |
| Planetoid\*[29] | 64.7(26s) | 75.7(13s) | 77.2(25s) | 61.9(185s) |
| GCN(this paper) | 70.3(7s)  | 81.5(4s)  | 79.0(38s) | 66.0(48s)  |
|GCN(rand.splits) | 67.9 ± 0.5| 80.1 ± 0.5| 78.9 ± 0.7| 58.4 ± 1.7 |

我们进一步报告了我们的方法（包括验证误差评估）收敛所需的时间（括号内为秒数），以及行星仪。 对于后者，我们使用了作者提供的实现，并在与我们的GCN模型相同的硬件上进行了训练（带有GPU）。 我们在 Yang等人(2016年) 所使用的数据集拆分上训练和测试了我们的模型，并报告了 100 次随机权重初始化运行的平均准确率。 我们使用以下超参数组合对CiteSeer、Cora和Pubmed： 0.5 (丢弃率)，5 × 10 −4 （ L2正则化 ） 和 16 (隐藏单元数)； 对于 NELL ： 0.1 (丢弃率)，1 × 10 −5 （ L2 正则化）和 64 (隐藏单元数)。

此外，我们在 Yang等人, (2016)中报告了与相同大小的随机数据集拆分相匹配的模型性能，这些数据集被标记为GCN（随机拆分）。在这里，我们报告测试集拆分的预测准确度的平均值和标准误差。

### 6.2 模型评估

我们在引用网络数据集上比较了我们提出的每层传播模型的不同变体。 我们遵循了前一节中描述的实验设置。 结果总结在表3中。 我们的原始GCN模型的传播模型用归一化技巧（粗体）表示。 在所有其他情况下，神经网络层的传播模型都替换为传播模型下指定的模型。 报告的数字表示对于具有随机权重矩阵初始化的100次重复运行的平均分类精度。 对于每个层中的多个变量$\Theta_{i}$，我们对第一层的所有权重矩阵施加L2正则化。

Table 3: Comparison of propagation models.

| Description                  | Propagation model                                       | Citeseer | Cora | Pubmed |
| ---------------------------- | ------------------------------------------------------- | -------- | ---- | ------ |
| Chebyshev ﬁlter(Eq. 5)  K= 3 | $\sum^K_{k=0}T_k(\widetilde{L})XΘ_k$                    | 69.8     | 79.5 | 74.4   |
| Chebyshev ﬁlter(Eq. 5) K= 2  | $\sum^K_{k=0}T_k(\widetilde{L})XΘ_k$                    | 69.6     | 81.2 | 73.8   |
| 1st-order model(Eq. 6)       | $XΘ_0 + D^{−1/2 }AD^{−1/2}XΘ_1$                         | 68.3     | 80.0 | 77.5   |
| Single parameter(Eq. 7)      | $(I_N + D^{−1/2}AD^{−1/2})XΘ$                           | 69.3     | 79.2 | 77.4   |
| Renormalization trick(Eq. 8) | $\widetilde D^{−1/2}\widetilde A \widetilde D^{−1/2}XΘ$ | 70.3     | 81.5 | 79.0   |
| 1st-order term only          | $D^{−1/2}AD^{−1/2}XΘ $                                  | 68.7     | 80.5 | 77.8   |
| Multi-layer perceptron       | $XΘ$                                                    | 46.5     | 55.1 | 71.4   |



### 6.3 每个 epoch 的训练时间

在这里，我们报告了模拟随机图上的每个训练周期（前向传播、交叉熵计算、反向传播）平均用时的结果，以秒为单位。请参见第5节中的详细描述在这些实验中使用的随机图形数据集。我们在GPU上以及使用TensorFlow (Abadi等人，2015年) 的CPU仅实现中比较结果。图2总结了结果。

![image-20240828112130636](/img/gcn/image-20240828112130636.png)

Figure 2: Wall-clock time per epoch for random graphs.(*) indicates out-of-memory error.

## 7 讨论

### 7.1 半监督模型

在本文中，我们展示了半监督分类方法在实验中的优越性。基于图拉普拉斯算子的方法（Zhu等，2003；Belkin等，2006；Weston等，2012）很可能受到其假设边仅编码节点相似度的限制。另一方面，Skip-gram 基于的方法由于它们依赖于多步骤管道而难以优化。我们的模型克服了这两种局限性，在效率方面与相关方法相比仍然具有竞争力（以时钟时间衡量）。与仅聚合标签信息的方法（Lu 和 Getoor，2003）相比，传播来自邻居节点的功能信息可以提高分类性能。

 我们进一步证明了提出的归一化传播模型（方程8）比简单的1阶模型（方程6）或使用Chebyshev多项式的高阶图卷积模型具有更好的效率（更少的参数和操作，如乘法或加法），以及在许多数据集上提供了更好的预测性能。

### 7.2 局限性和未来工作

在这里，我们描述了当前模型的一些局限性，并概述了未来如何克服这些局限性。 

**内存需求** 在当前设置中，使用批量梯度下降法时，内存需求会线性增长。我们已经证明了对于无法装入 GPU 内存的大图来说，在 CPU 上训练仍然是一个可行的选择。小批量随机梯度下降可以缓解这个问题。然而，生成小批量的过程应该考虑 GCN 模型中的层数，因为具有 K 层的 GCN 的第 K 阶近邻必须存储在内存中才能进行精确计算。对于非常大且密集连接的图数据集，可能需要进一步的近似。

**有向边和边特征**  我们的框架目前不支持自然地表示边特征，仅限于无向图（带权或不带权）。然而，在 NELL 上的结果表明，通过将原始有向图表示为一个带有额外节点的二分图，可以处理有向边和边特征（见第 5.1 节以获取更多详细信息）。

**限制假设**  通过第 2 节中引入的近似，我们隐含地假设局部性（K 层 GNN 的依赖于 K 阶邻域）以及自我连接与邻居节点之间的边具有相等的重要性。然而，在某些数据集上，引入一个权重参数 λ 在定义 $\tilde A$ 中可能更有益： 
$$
\tilde A = A + λI_N
$$
这个参数现在在典型的半监督设置中扮演着与监督和无监督损失之间的权衡参数相似的角色（参见等式 1）。然而，这里可以使用梯度下降来学习。

### 8 结论

我们引入了一种新颖的方法来处理图形结构数据上的半监督分类。我们的 GCN 模型使用一种基于图上一阶近似谱卷积的 层间传播规则。在多个网络数据集上的实验表明，提出的 GCN 模型能够以对半监督分类 有用的方式编码图结构和节点特征。在这种情况下，与最近提出的一些方法相比，我们的模型具有显著的优势，同时计算效率更高。

