<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Large-Scale Image Retrieval with Attentive Deep Local Features | 遗世独立</title><meta name="author" content="神火不知灭"><meta name="copyright" content="神火不知灭"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="注意力深层局部特征的大规模图像检索 摘要 提出了一种适合于大规模图像检索的局部特征描述器，称为Deep-local-feature。新的特征是基于卷积神经网络，它只在地标图像数据集上使用图像级注释进行训练。为了识别在语义上有用的图像检索局部特征，我们还提出了一种用于关键点选择的注意机制，该机制与描述符共享大部分网络层。该框架可用于图像检索，作为其他关键点检测器和描述符的替代品，实现更精确的特征匹配">
<meta property="og:type" content="article">
<meta property="og:title" content="Large-Scale Image Retrieval with Attentive Deep Local Features">
<meta property="og:url" content="http://blog.sunlingzhang.com/2020/07/03/Paper/Large-Scale%20Image%20Retrieval%20with%20Attentive%20Deep%20Local%20Features/index.html">
<meta property="og:site_name" content="遗世独立">
<meta property="og:description" content="注意力深层局部特征的大规模图像检索 摘要 提出了一种适合于大规模图像检索的局部特征描述器，称为Deep-local-feature。新的特征是基于卷积神经网络，它只在地标图像数据集上使用图像级注释进行训练。为了识别在语义上有用的图像检索局部特征，我们还提出了一种用于关键点选择的注意机制，该机制与描述符共享大部分网络层。该框架可用于图像检索，作为其他关键点检测器和描述符的替代品，实现更精确的特征匹配">
<meta property="og:locale">
<meta property="og:image" content="http://blog.sunlingzhang.com/admin_head.jpg">
<meta property="article:published_time" content="2020-07-03T07:28:12.000Z">
<meta property="article:modified_time" content="2026-02-12T10:34:06.980Z">
<meta property="article:author" content="神火不知灭">
<meta property="article:tag" content="paper">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.sunlingzhang.com/admin_head.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Large-Scale Image Retrieval with Attentive Deep Local Features",
  "url": "http://blog.sunlingzhang.com/2020/07/03/Paper/Large-Scale%20Image%20Retrieval%20with%20Attentive%20Deep%20Local%20Features/",
  "image": "http://blog.sunlingzhang.com/admin_head.jpg",
  "datePublished": "2020-07-03T07:28:12.000Z",
  "dateModified": "2026-02-12T10:34:06.980Z",
  "author": [
    {
      "@type": "Person",
      "name": "神火不知灭",
      "url": "http://blog.sunlingzhang.com"
    }
  ]
}</script><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://blog.sunlingzhang.com/2020/07/03/Paper/Large-Scale%20Image%20Retrieval%20with%20Attentive%20Deep%20Local%20Features/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Large-Scale Image Retrieval with Attentive Deep Local Features',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/admin_head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">64</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">64</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/%E5%85%A5%E9%97%A8%E5%AE%9E%E8%B7%B5/"><i class="fa-fw fas fa-keyboard"></i><span> 入门实践</span></a></div><div class="menus_item"><a class="site-page" href="/tags/%E9%9A%8F%E7%AC%94/"><i class="fa-fw fas fa-edit"></i><span> 随笔</span></a></div><div class="menus_item"><a class="site-page" href="/tags/paper/"><i class="fa-fw fal fa-paperclip"></i><span> 论文解读</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 计算机</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/algorithm/"><i class="fa-fw fas fa-code"></i><span> 算法</span></a></li><li><a class="site-page child" href="/data-structure/"><i class="fa-fw fas fa-terminal"></i><span> 数据结构</span></a></li><li><a class="site-page child" href="/operation-system/"><i class="fa-fw fas fa-desktop"></i><span> 操作系统</span></a></li><li><a class="site-page child" href="/computer-composition/"><i class="fa-fw fas fa-microchip"></i><span> 计算机组成原理</span></a></li><li><a class="site-page child" href="/network/"><i class="fa-fw fas fa-network-wired"></i><span> 计算机网络</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 页面</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 编程语言</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tags/Java/"><i class="fa-fw fas fa-music"></i><span> Java</span></a></li><li><a class="site-page child" href="/tags/Python/"><i class="fa-fw fas fa-video"></i><span> Python</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">遗世独立</span></a><a class="nav-page-title" href="/"><span class="site-name">Large-Scale Image Retrieval with Attentive Deep Local Features</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/%E5%85%A5%E9%97%A8%E5%AE%9E%E8%B7%B5/"><i class="fa-fw fas fa-keyboard"></i><span> 入门实践</span></a></div><div class="menus_item"><a class="site-page" href="/tags/%E9%9A%8F%E7%AC%94/"><i class="fa-fw fas fa-edit"></i><span> 随笔</span></a></div><div class="menus_item"><a class="site-page" href="/tags/paper/"><i class="fa-fw fal fa-paperclip"></i><span> 论文解读</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 计算机</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/algorithm/"><i class="fa-fw fas fa-code"></i><span> 算法</span></a></li><li><a class="site-page child" href="/data-structure/"><i class="fa-fw fas fa-terminal"></i><span> 数据结构</span></a></li><li><a class="site-page child" href="/operation-system/"><i class="fa-fw fas fa-desktop"></i><span> 操作系统</span></a></li><li><a class="site-page child" href="/computer-composition/"><i class="fa-fw fas fa-microchip"></i><span> 计算机组成原理</span></a></li><li><a class="site-page child" href="/network/"><i class="fa-fw fas fa-network-wired"></i><span> 计算机网络</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 页面</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 编程语言</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tags/Java/"><i class="fa-fw fas fa-music"></i><span> Java</span></a></li><li><a class="site-page child" href="/tags/Python/"><i class="fa-fw fas fa-video"></i><span> Python</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Large-Scale Image Retrieval with Attentive Deep Local Features</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-07-03T07:28:12.000Z" title="Created 2020-07-03 15:28:12">2020-07-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-12T10:34:06.980Z" title="Updated 2026-02-12 18:34:06">2026-02-12</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h2 id="注意力深层局部特征的大规模图像检索">注意力深层局部特征的大规模图像检索</h2>
<h3 id="摘要">摘要</h3>
<p>提出了一种适合于大规模图像检索的局部特征描述器，称为Deep-local-feature。新的特征是基于卷积神经网络，它只在地标图像数据集上使用图像级注释进行训练。为了识别在<strong>语义上有用的图像检索局部特征</strong>，我们还提出了一种用于关键点选择的<strong>注意机制</strong>，该机制<strong>与描述符共享大部分网络层</strong>。该框架可用于图像检索，作为其他<strong>关键点检测器和描述符</strong>的替代品，实现更精确的特征匹配和几何匹配验证。我们的系统产生可信的分数拒绝误报(FP)，尤其是它的健壮性针对数据库中没有正确匹配的查询。为了评估所提出的描述符，我们引入了一个新的大规模数据集，被称为谷歌地标(GLD)数据集，包括数据库和 查询搜索作为背景杂波，部分遮挡，多个地标、可变尺度的物体等DELF的成绩超过了全球和当地最先进的水平(SOTA)在大范围数据集中的描述符。可在以下网页找到项目代码：<a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/tree/master/research/delf%E3%80%82">https://github.com/tensorflow/models/tree/master/research/delf。</a></p>
<span id="more"></span>
<h3 id="1-介绍">1. 介绍</h3>
<p>大规模图像检索是计算机视觉中的一项基本任务，它直接关系到目标检测、视觉位置识别、产品识别等各种实际应用。在过去的几十年里，图像检索系统取得了巨大的进步，从手工制作的特征和索引算法[22,33,27,16]到最近的基于卷积神经网络（CNNs）的全局描述符学习方法[2,29,11]。</p>
<p>尽管基于CNN的全局描述符在中小型数据集中的图像检索方面取得了最新进展[27,28]，但在大规模数据集中观察到的各种具有挑战性的条件（如杂波&lt;背景杂波&gt;、遮挡和视点和照明的变化）可能会阻碍其性能。全局描述符缺乏在图像之间查找补丁级别匹配的能力。因此，在存在遮挡和背景杂波的情况下，基于部分匹配的图像检索非常困难。在最近的一个趋势中，基于CNN的局部特征被提出用于斑块级匹配[12,42,40]。然而，这些技术并没有特别针对图像检索进行优化，因为它们缺乏检测语义上有意义的特征的能力，并且在实际应用中显示出有限的准确性。</p>
<p>大多数现有的图像检索算法都是在<strong>查询图像</strong>较少的中小型数据集中进行评估的，即[27,28]中只有55张和[16]中只有500张，并且数据集中的图像在地标位置和类型方面的多样性有限。因此，我们认为，通过大规模的数据集来提高检索结果的综合性和有效性，可以使我们从中得到更具挑战性的大规模图像检索方法论。</p>
<p>本文的主要目标是开发一个基于CNN的特征描述子的大规模图像检索系统。为此，我们首先引入一个新的大规模数据集Google Landmarks(GLD)，它包含了来自近13K个独特地标的超过100万个地标图像。这个数据集覆盖了世界范围，因此比现有的数据集更加多样化和全面。查询集由额外的100K个具有各种特性的图像组成；特别是，我们在数据库中包含了不匹配(可能指数据库中不存在查询结果)的图像，这使得我们的数据集更具挑战性。这允许评估检索系统的健壮性通过查询不必要的地标描述。</p>
<p>然后，我们提出了一种基于CNN的有注意力机制的局部特征，它只使用图像级的类标签进行弱监督训练，而不需要对象级和补丁级的标注。这种新的特征描述符被称为DELF（Deep Local feature），图1说明了特征提取和图像检索的总体过程。在我们的方法中，注意力模型与所提取的描述符紧密耦合；它采用相同的CNN架构，并且只需很少的额外计算就可以生成特征分数（符合对象检测的最新进展[30]）。这使得本地描述符和关键点的提取都可以通过一个前向通道网络。结果表明，与基于全局和局部描述子的方法相比，基于DELF的图像检索系统具有更高的检索效率。</p>
<h3 id="2-相关工作">2. 相关工作</h3>
<p>有标准的数据集通常用于评价图像检索技术。<strong>Oxford5K</strong>[27]有5062个在牛津拍摄的建筑图像，其中55个查询图像。<strong>Paris6k</strong>[28]由6412幅巴黎地标图片组成，也有55幅查询图片。这两个数据集通常使用来自Flickr100k数据集[27]的<strong>Flickr100k</strong>图像进行扩充，后者分别构建<strong>Oxford105k</strong>和<strong>Paris106k</strong>数据集。另一方面，<strong>Holidays dataset</strong>数据集[16]提供了1491张图片，包括500张查询图片，这些图片来自个人假日照片。这三个数据集都非常小，尤其是查询图像的数量非常少，这使得在这些数据集中测试的性能很难<strong>通用化</strong>。虽然<strong>Pitts250k</strong>[35]比较大，但它专门用于具有重复图案的视觉区域，可能不适合一般的图像检索任务。</p>
<p>实例检索是近十年来研究的热点问题。最近的调查见[43]。早期的系统依赖于手工制作的局部特征[22,5,8]，再加上使用<strong>KD树</strong>或<strong>词汇树</strong>的<strong>近似最近邻搜索方法</strong>[6,25]。时至今日，这种基于特征的技术与几何重排序相结合，在检索系统需要高精度操作时提供了强大的性能。</p>
<p>最近，许多研究集中在局部特征的聚集方法上(应该指使用局部描述符聚合成全局描述符)，其中包括一些流行的技术，如<strong>VLAD</strong>[18]和<strong>Fisher Vector</strong>（FV）[19]。这种全局描述符的主要优点是能够以紧凑的索引提供高性能的图像检索。</p>
<p>在过去的几年中，一些基于cnn的全局描述符被提出使用预先训练的[4,34]或学习网络[2,29,11]。为了保持相关图像和无关图像之间的排序，这些全局描述符最常用三元组损失进行训练。一些使用这些基于CNN的全局描述符的检索算法利用深度局部特征作为传统聚集技术（如VLAD或FV）中手工构建的特征的替代品[24,36]。其他的工作已经重新评估和提出了不同的特征聚合方法使用这些深的局部特征[3，21]。</p>
<p>CNN也被用来检测、表示和比较局部图像特征。Verdie等人[37]学习了<strong>可重复关键点检测</strong>的回归函数。Yi等人[41]提出了一种基于CNN的通用技术来估计局部特征的典型方向，并成功地将其应用到多个不同的描述符上。<strong>MatchNet</strong>[12]和<strong>Deep Compare</strong>[42]提出联合学习块表达和相关的指标。最近，LIFT[40]提出了一个端到端的框架来检测关键点、估计方向和计算描述符。与我们的工作不同的是，这些技术不是为图像检索应用而设计的，因为它们没有学习<strong>选择语义上有意义的特征</strong>。</p>
<p>许多视觉识别问题都采用了基于深层神经网络的视觉注意力，包括目标检测[45]、语义分割[14]、图像捕获[38]、视觉问题回答[39]等。然而，视觉注意力在图像检索应用中的学习视觉特征还没有被积极探索。</p>
<h3 id="3-谷歌地标数据集">3. 谷歌地标数据集</h3>
<p>我们的数据集是基于[44]中描述的算法构造的。与现有的用于图像检索的数据集[27,28,16]相比，新的数据集要大得多，包含多个地标，并且涉及大量挑战。它包含来自12894个地标的1 060 709个图像，以及111 036个其他查询图像。数据集中的图像被捕捉到世界上不同的位置，每个图像都与一个GPS坐标相关联。图2和图3分别示出了示例图像及其地理分布。虽然现有数据集中的大多数图像都是以地标为中心的，这使得全局特征描述子工作得很好，但是我们的数据集包含了更真实的图像，包括前景/背景杂波、遮挡、部分视野外的对象等。由于我们的查询图像是从个人照片库中收集的，其中一些可能不包含任何地标，因此不应该从数据库中检索任何图像。我们称这些查询图像为<em>distractors</em>分心器，它在评估算法对无关和噪声查询的鲁棒性方面起着至关重要的作用。</p>
<p>我们使用视觉特征和GPS坐标来构建 地面真相 。数据库中的所有图像都使用这两种信息进行聚类，并为每个簇分配一个地标标识符。如果查询图像的位置与与检索到的图像相关联的簇中心之间的物理距离小于阈值，我们假设这两个图像属于同一个地标。请注意，地面真实性注释非常具有挑战性，特别是考虑到很难预先定义什么是地标，地标有时不明显，并且在一个图像中可能有多个实例。显然，由于GPS误差的影响，这种地面真相构建方法存在噪声。另外，一些地标（如埃菲尔铁塔、金门大桥）的照片可以从很远的地方拍摄到，因此照片位置可能与实际地标位置相对较远。然而，在手工检查数据子集时，我们发现很少出现阈值为25km的错误注释。即使有很少的小错误，它也不成问题，特别是在相对评估中，因为算法不太可能在地标之间混淆，如果它们的视觉外观足够歧视的话。</p>
<h3 id="4-使用DELF图像检索">4. 使用DELF图像检索</h3>
<p>我们的大规模检索系统可以分解为四个主要模块：</p>
<p>（i）密集的局部特征提取；</p>
<p>（ii）关键点选择；</p>
<p>（iii）降维；</p>
<p>（iv）索引和检索。</p>
<p>这一部分详细介绍了DELF特征提取和学习算法以及我们的索引和检索过程。</p>
<h4 id="4-1-密集局部特征提取">4.1 密集局部特征提取</h4>
<p>我们采用一个完全卷积网络（FCN）从图像中提取密集特征，该网络是利用训练后的CNN的特征提取层构造的。我们使用一个取自ResNet50[13]模型的FCN，使用conv4x卷积块的输出。为了处理尺度的变化，我们显式地构造了一个图像金字塔，并对每个层次独立地应用FCN。将得到的特征映射视为局部描述子的密集网格。基于接收场对特征进行局部定位，可通过考虑FCN卷积层和池层的结构来计算特征。我们使用感受野中心的像素坐标作为特征定位。图像在原始尺度下的感受野大小为291×291。利用图像金字塔，我们得到了描述不同大小图像区域的特征。</p>
<p>我们使用在ImageNet[31]上训练的原始ResNet50模型作为基线，并对其进行微调，以增强我们的局部描述符的辨别力。由于我们考虑了一个地标识别应用，我们使用地标图像的注释数据集[4]，并使用<strong>标准交叉熵损失</strong>对网络进行训练，以便进行图像分类，如图4（a）所示。输入图像最初被中心裁剪以生成方形图像，然后重新缩放到250 x 250。然后随机使用224 x 224部分进行训练。作为训练的结果，局部描述符隐式学习与地标检索问题更相关的表示。以这种方式，对象级和补丁级的标签都不需要即可获得改进的局部描述符。</p>
<h4 id="4-2-基于注意力的关键点选择">4.2 基于注意力的关键点选择</h4>
<p>与直接使用密集提取的特征进行图像检索不同，我们设计了一种有效地选择特征子集的技术。由于密集提取的特征中有相当一部分与我们的识别任务无关，并且可能会增加杂波(背景杂波)，分散检索过程的注意力，因此关键点的选择对于检索系统的准确性和计算效率都非常重要。</p>
<h5 id="4-2-1-弱监督学习">4.2.1 弱监督学习</h5>
<p>我们建议训练一个地标分类器来显式地测量局部特征描述子的相关性分数。为了训练函数，特征一个加权和池化，其中权重由注意力网络预测。培训程序与第4.1描述的损失函数和数据集相似，如图4（b）所示，其中注意力网络以黄色突出显示。这将生成整个输入图像的嵌入，然后用于训练基于softmax的地标分类器。</p>
<p>更确切地说，我们制定如下训练计划。用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>n</mi></msub><mo>∈</mo><msup><mi>R</mi><mi>d</mi></msup><mo separator="true">,</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">f_n\in R^d, n=1,...,N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0435em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>，这d维特征与注意模型联合学习。我们的目标是学习每个特征的得分函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo stretchy="false">(</mo><msub><mi>f</mi><mi>n</mi></msub><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\alpha(f_n ;\theta )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>表示函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\alpha(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span>的参数。网络的输出逻辑y由特征向量的加权和生成，该加权和由</p>
<p>(1)$$y=W(\sum_n\alpha(f_n;\theta)\cdot f_n)$$</p>
<p>式中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>M</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W\in R^{M\times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span>表示训练用于预测M类的CNN最终完全连接层的权重。</p>
<p>对于训练，我们使用交叉熵损失，它由</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mo>=</mo><mo>−</mo><msup><mi>y</mi><mo>∗</mo></msup><mo>⋅</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mrow><msup><mn>1</mn><mi>T</mi></msup><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L=-y^* \cdot log(\frac{exp(y)}{1^T exp(y)})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9331em;vertical-align:-0.1944em;"></span><span class="mord">−</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7387em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord">1</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7673em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>式中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">y^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8831em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> 是one-hot之后的ground-truth向量，1是一向量[N维1向量]。分数函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\alpha(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span>中的参数通过反向传播进行训练，其中梯度由</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>θ</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow></mfrac><munder><mo>∑</mo><mi>n</mi></munder><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>α</mi><mi>n</mi></msub></mrow></mfrac><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>α</mi><mi>n</mi></msub></mrow><mrow><mi mathvariant="normal">∂</mi><mi>θ</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>y</mi></mrow></mfrac><munder><mo>∑</mo><mi>n</mi></munder><mi>W</mi><msub><mi>f</mi><mi>n</mi></msub><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>α</mi><mi>n</mi></msub></mrow><mrow><mi mathvariant="normal">∂</mi><mi>θ</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial L}{\partial\theta}=\frac{\partial L}{\partial y}\sum_n \frac{\partial y}{\partial\alpha_n}\frac{\partial \alpha_n}{\partial\theta}=\frac{\partial L}{\partial y}\sum_n Wf_n \frac{\partial \alpha_n}{\partial\theta}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.0574em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.6214em;vertical-align:-1.25em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.9em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.25em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.6214em;vertical-align:-1.25em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.9em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.25em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>式中反向传播的输出分数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>n</mi></msub><mo>=</mo><mo>=</mo><mi>α</mi><mo stretchy="false">(</mo><msub><mi>f</mi><mi>n</mi></msub><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\alpha_n==\alpha(f_n;\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">==</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span>相对于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>与标准多层感知器相同。</p>
<p>我们将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\alpha(\cdot )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span>限制为非负，以防止它学习负权重。score函数使用2层CNN设计，顶部使用softplus[9]激活（限制为非负）。为了简单起见，我们采用了尺寸为1 x 1的卷积滤波器，这在实践中效果良好。一旦注意力模型被训练出来，就可以用来评估模型所提取特征的相关性。</p>
<h5 id="4-2-2-训练注意力">4.2.2 训练注意力</h5>
<p>在该框架中，描述子和注意模型都是通过图像级标签进行隐式学习的。不幸的是，这给学习过程带来了一些挑战。当特征表示和分数函数可以通过反向传播联合训练时，我们发现这种方法在实际应用中产生了弱模型。因此，我们采用两步训练策略。首先，我们通过微调学习描述符，如第4.1节所述。在给定固定的描述子的情况下，学习得分函数。</p>
<p>另一个改进是在注意力训练过程中通过随机图像重缩放来实现的。这是直观的，因为注意力模型应该能够为不同尺度的特征生成有效的分数。在这种情况下，输入图像最初被中心裁剪以产生方形图像，然后重新缩放到900 x 900。然后随机抽取720 x 720个输出，最后用系数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>&lt;</mo><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma&lt;=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>随机缩放。</p>
<h5 id="4-2-3-特点">4.2.3 特点</h5>
<p>我们系统的一个非传统的方面是，关键点选择是在描述符提取之后进行的，这与现有的技术（例如SIFT[22]和LIFT[40]）不同，后者首先检测到关键点，然后再进行描述。传统的关键点检测器只根据关键点的低电平特性，在不同成像条件下对关键点进行重复检测。然而，对于像图像检索这样的高级识别任务，选择能够区分不同对象实例的关键点也是至关重要的。该流程通过训练一个在特征映射中编码高级语义的模型，以及学习如何为分类任务选择有区别的特征来达到这两个目的。这与最近提出的学习关键点检测器的技术（即LIFT[40]）相反，后者根据SIFT匹配收集训练数据。虽然我们的模型不受约束地学习姿势和视点的不变性，但它隐含地学习这样做，类似于基于CNN的图像分类技术。</p>
<h4 id="4-3-降维">4.3 降维</h4>
<p>我们降低所选特征的维数以提高检索精度，这是常见的做法[15]。首先，对选取的特征进行L2标准化，通过PCA将其维数降到40，在紧凑性和区分性之间取得了很好的折衷。最后，这些特征再次经过L2标准化。</p>
<h4 id="4-4-图片检索系统">4.4 图片检索系统</h4>
<p>我们从查询图像和数据库图像中提取特征描述子，从中选择每个图像中具有<strong>最高关注分数</strong>的预定义数量的局部特征。我们的图像检索系统是基于<strong>最近邻搜索</strong>的，它是由<strong>KD树</strong>[7]和<strong>乘积量化</strong>（PQ）[17]相结合来实现的。我们使用PQ将每个描述子编码成50位编码，每个40D特征描述子被分成10个子向量，每个子向量用k均值聚类法识别25个聚类中心，实现50位编码。我们执行非对称距离计算，其中查询描述符不进行编码，以提高最近邻检索的准确性。为了加快最近邻搜索的速度，我们使用8K码本构造了一个描述符的倒排索引，为了减少编码错误，我们使用KD树对每个Voronoi(类似VLAD的聚类中心范围)单元进行划分，并对每个特征小于30K的子树使用局部优化的乘积量化器[20]。</p>
<p>当给定一个查询时，我们对从查询图像中提取的每个局部描述符执行近似近邻搜索。然后，对于从索引中检索到的前K个最近的局部描述符，我们将每个数据库图片的所有匹配项集合起来。最后，我们使用<strong>RANSAC</strong>[10]进行几何验证，并使用inliner(样本点)的数量作为检索图像的分数。这个几何验证步骤拒绝了许多分心器查询，因为分心器的特征可能与地标图像的特征不一致。</p>
<p>这个流程索引10亿个描述符需要的内存少于8GB，这足以处理我们的大型地标数据集。在我们的实验设置下，使用单个CPU，最近邻搜索的延迟小于2秒，我们在每个查询中软分配5个聚类中心，并在每个倒排索引树中搜索多达10K个叶节点。</p>
<h3 id="5-实验">5 实验</h3>
<p>本节主要讨论与我们数据集中现有的全局和局部特征描述符相比，DELF的性能。此外，我们还展示了如何使用DELF在现有数据集中获得良好的精度。</p>
<h4 id="5-1-实施细节">5.1 实施细节</h4>
<p><strong>多尺度描述子提取</strong>  我们使用相距<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mn>2</mn></msqrt></mrow><annotation encoding="application/x-tex">\sqrt2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.1328em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9072em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;">2</span></span><span style="top:-2.8672em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1328em;"><span></span></span></span></span></span></span></span></span>倍的尺度来构造图像金字塔。对于范围从0.25到2.0的一组比例尺，使用7种不同的比例尺。感受野的大小与尺度成反比；例如，对于2.0尺度，网络的感受野覆盖146 x 146像素。</p>
<p><strong>训练</strong>  我们使用landmarks数据集[4]来微调描述符和训练关键点选择。在数据集中，有“完整”版本，称为LF（在删除了Oxf5k / Par6k的重叠类之后，通过[11]），包含586个地标的140372个图像，以及通过基于SIFT的匹配过程[11]获得的“干净”版本(LC)，包含586个地标的35382个图像。我们使用LF训练我们的注意模型，并使用LC对图像检索的网络进行微调。</p>
<p><strong>参数</strong>  我们为一个查询中的每个特征确定最接近的K（=60）个近邻，并从每个图像中提取多达1000个局部特征，每个特征是40维的。</p>
<h4 id="5-2-算法比较">5.2 算法比较</h4>
<p>DELF与最近的几个全局和局部描述符进行了比较。虽然有各种与图像检索相关的研究成果，但我们相信以下方法要么与我们的算法相关，要么由于其良好的性能而对评估至关重要。</p>
<p><strong>深度图像检索(DIR)</strong>  这是一个最新的全局描述符，它在多个现有数据集中达到了最先进的性能。DIR特征描述符为2048维，所有情况下都使用多分辨率描述符。我们还使用查询扩展（QE）进行评估，这通常可以提高标准数据集的准确性。我们使用发布的源代码来实现ResNet101[13]版本。在检索方面，采用了暴力搜索的并行实现，避免了近似近邻搜索的错误造成的惩罚。</p>
<p><strong>siaMAC</strong>  这是一个最新的全局描述符，可以在现有数据集中获得高性能。我们使用发布的源代码与暴力搜索的并行实现。基于VGG16[32]的CNN提取512维全局描述子。我们还对DIR中的查询扩展（QE）进行了实验。</p>
<p><strong>CONGAS</strong>  CONGAS是一个40D的手工构建的局部特征，已被广泛应用于实例级图像匹配和检索[1,44]。该特征描述子是通过在检测到的关键点的尺度和方向上采集Gabor小波响应来提取的，并且与SIFT等基于梯度的局部描述子具有非常相似的性能和特性。采用拉普拉斯高斯关键点检测器</p>
<p><strong>LIFT</strong>  LIFT[40]是最近提出的一种特征匹配流程，它将关键点检测、方向估计和关键点描述结合起来学习。特征是128维的。我们使用公开的源代码。</p>
<h4 id="5-3-评估">5.3 评估</h4>
<p>图像检索系统通常是基于平均平均精度（mAP）来评估的，平均平均精度是通过按每个查询的相关性降序对图像进行排序并平均每个查询的AP来计算的。然而，对于带有干扰查询的数据集，这种评估方法并不具有代表性，因为确定每个图像是否与查询相关很重要。在我们的例子中，使用绝对检索分数来估计每个图像的相关性。对于性能评估，我们使用了一个改进版本的精度（PRE）和召回（REC），方法是同时考虑所有查询图像，由</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>P</mi><mrow><mi>R</mi><mi>E</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∣</mi><msubsup><mi>R</mi><mi>q</mi><mrow><mi>T</mi><mi>P</mi></mrow></msubsup><mi mathvariant="normal">∣</mi></mrow><mrow><mi mathvariant="normal">∣</mi><msub><mi>R</mi><mi>q</mi></msub><mi mathvariant="normal">∣</mi></mrow></mfrac><mtext>和</mtext><msub><mi>R</mi><mrow><mi>E</mi><mi>C</mi></mrow></msub><mo>=</mo><munder><mo>∑</mo><mi>q</mi></munder><mi mathvariant="normal">∣</mi><msubsup><mi>R</mi><mi>q</mi><mrow><mi>T</mi><mi>P</mi></mrow></msubsup><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">P_{RE}=\frac{|R_q^{TP}|}{|R_q|} 和 R_{EC}=\sum_q |R_q^{TP}|
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">RE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.5865em;vertical-align:-0.9721em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.7731em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9721em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord cjk_fallback">和</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">EC</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4361em;vertical-align:-1.3861em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.9em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3861em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span></span></p>
<p>式中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">R_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>表示给定阈值的查询q的一组检索图像，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>R</mi><mi>q</mi><mrow><mi>T</mi><mi>P</mi></mrow></msubsup><mo stretchy="false">(</mo><mo>⊆</mo><msub><mi>R</mi><mi>q</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R_q^{TP}(\subseteq R_q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2244em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">TP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mrel">⊆</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>是一组真正类。这与[26]中引入的micro-AP指标类似。请注意，在我们的例子中，在最终评分中只考虑每个地标的最高得分图像。我们更喜欢非标准化的回调值，它表示检索到的真阳性数。</p>
<h4 id="5-4-定量结果">5.4 定量结果</h4>
<p>图5显示了与其他方法相比，DELF（用DELF+FT+ATT表示）的精确召回曲线。由于特征提取速度非常慢，无法进行大规模实验，因此无法显示LIFT的结果。DELF明显优于所有其他技术。全局特征描述符，比如DIR，在我们富有挑战性的数据集中受到了影响。特别是，由于查询集中存在大量干扰因素，使用QE的DIR会显著降低准确性。CONGAS做得相当不错，但仍然比DELF差很多。</p>
<p>为了分析精细调整和注意力对图像检索的好处，我们比较了我们的完整模型（DELF+FT+ATT）及其变体：DELF-noFT、DELF+FT和DELFnoFT+ATT。DELF-noFT是指提取的特征基于ImageNet上预训练的CNN，而不需要精细调整和注意力学习。DELF+FT表示有微调但没有注意建模的模型，DELFnoFT+ATT对应于未经微调但使用注意力的模型。如图5所示，微调和注意力建模都对性能改进做出了重大贡献。特别要注意的是，注意力的使用比微调更重要。这表明，所提出的注意层可以有效地学习为检索任务选择最有区别的特征，即使这些特征只是在ImageNet上预先训练过的。</p>
<p>在内存需求方面，DELF、CONGAS和DIR几乎同样复杂。DELF和CONGAS采用相同的特征维数和每个图像的最大特征数；它们需要大约8GB的内存。DIR描述符需要每个图像8KB，加起来大约8GB来索引整个数据集。</p>
<h4 id="5-5-定量结果">5.5 定量结果</h4>
<p>我们给出定性的结果来说明DELF与两种基于全局和局部特征的竞争算法DIR和CONGAS的性能比较。同时，通过可视化分析了基于注意力的关键点检测算法。</p>
<p><strong>DELF vs. DIR</strong> 图6显示了检索结果，其中DELF的性能优于DIR。DELF得到图像中特定局部区域之间的匹配，这对于在不同成像条件下找到同一目标具有重要意义。DIR的常见故障案例发生在数据库包含类似的对象或场景时，例如方尖碑、山脉、港口，如图6所示。在许多情况下，DIR无法区分这些特定的对象或场景；尽管它发现语义上相似的图像，但它们通常与感兴趣的实例不对应。DIR和其他全局描述符的另一个缺点是它们不善于识别感兴趣的小对象。图7显示了DIR优于DELF的情况。虽然DELF能够在不同的图像上匹配局部模式，但当不同地标的地板砖或植被相似时，这会导致错误。</p>
<p><strong>DELF vs. CONGAS</strong>  与CONGAS相比，DELF的主要优势在于它的召回率；它比CONGAS检索到更多相关的地标，这表明DELF描述符更具辨别力。我们没有观察到CONGAS优于DELF的显著例子。图8显示了来自查询和数据库的成对图像，这些图像通过DELF成功匹配，但被CONGAS忽略，其中特征对应通过连接用于匹配特征的接收字段的中心来呈现。由于感受野可能相当大，一些特征似乎局限于无差别的区域，例如海洋或天空。然而，在这些情况下，这些特征会考虑到邻近区域中更具歧视性的区域。</p>
<p><strong>关键点检测方法分析</strong>  图9显示了关键点检测的三种变化，其中我们的注意模型的好处被清楚地定性地说明，而微调特征的L2范数与未经微调的L2范数略有不同。</p>
<h4 id="5-6-现有数据集中的结果">5.6 现有数据集中的结果</h4>
<p>为了完整性，我们展示了DELF在现有数据集中的性能，比如Oxf5k、Par6k及其扩展Oxf105k和Par106k。对于这个实验，我们简单地使用所提出的方法来获得每幅图像的分数，并通过计算两个标准化分数的加权平均值来与DIR的分数进行后期融合，其中DELF的权重设置为0.25。结果显示在表1中，我们提出了现有方法的准确性在他们的原始论文和我们的复制使用公共源代码，这是非常接近。当与DIR结合使用时，DELF显著地提高了数据集中的准确性，尽管它本身并没有显示出最好的性能。这一事实表明，DELF能够对全局特征描述符中不可用的补充信息进行编码。</p>
<h3 id="6-结论">6 结论</h3>
<p>本文提出了一种新的局部特征描述子DELF，它是专门为大规模图像检索应用而设计的。DELF是在弱监督下学习的，只使用图像级别的标签，并与我们的新的注意机制的语义特征选择相结合。在所提出的基于CNN的模型中，一次前向传递就足以获得关键点和描述符。为了正确评估大规模图像检索算法的性能，我们引入了Google Landmarks数据集，该数据集由超过1M个数据库图像、13K个唯一路标和100K个查询图像组成。在这样一个大规模的环境下的评估表明，DELF的性能远远超过现有的全局和局部描述符。在已有的数据集上，我们也给出了结果，并表明当与全局描述符相结合时，DELF具有良好的性能。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://blog.sunlingzhang.com">神火不知灭</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://blog.sunlingzhang.com/2020/07/03/Paper/Large-Scale%20Image%20Retrieval%20with%20Attentive%20Deep%20Local%20Features/">http://blog.sunlingzhang.com/2020/07/03/Paper/Large-Scale%20Image%20Retrieval%20with%20Attentive%20Deep%20Local%20Features/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/paper/">paper</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a></div><div class="post-share"><div class="social-share" data-image="/admin_head.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2020/07/27/default/Designing%20Network%20Design%20Spaces/" title="Designing Network Design Spaces"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">Designing Network Design Spaces</div></div><div class="info-2"><div class="info-item-1">网络设计空间的设计 摘要 本文提出了一种新的网络设计范式。我们的目标是帮助提高对网络设计的理解，并发现在不同环境下普遍适用的设计原则。我们不再专注于设计单个的网络实例，而是设计了参数化网络总体的网络设计空间。整个过程类似于经典的人工网络设计，但提升到了设计空间层面。使用我们的方法，我们探索网络设计的结构方面，并得出一个低维的设计空间，由简单的，规则的网络组成，我们称之为RegNet。RegNet参数化的核心观点非常简单：良好网络的宽度和深度可以用量化的线性函数来解释。我们分析了RegNet的设计空间，得出了与当前网络设计实践不符的有趣发现。RegNet的设计空间提供了简单而快速的网络，可以很好地在各种不同的FLOP状态下工作。在类似的训练环境和FLOPs下，RegNet模型的性能优于流行的efficientnet模型，而在gpu上的速度高达5倍。  介绍 深卷积神经网络是视觉识别的引擎。在过去的几年里，更好的体系结构已经在广泛的视觉识别任务中取得了长足的进步。示例包括LeNet、AlexNet、VGG和ResNet。这一工作既提高了神经网络的有效性，也促进了我们对网络设计的理解。...</div></div></div></a><a class="pagination-related" href="/2020/07/02/Paper/Unifying%20Deep%20Local%20and%20Global%20Features%20for%20Image%20Search/" title="Unifying Deep Local and Global Features for Image Search"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Unifying Deep Local and Global Features for Image Search</div></div><div class="info-2"><div class="info-item-1">统一局部和全局特征进行图像搜索的深层(网络) ​    本文使用机翻，稍加润色，主要用于个人理解，不恰当之处请看客见谅。 摘要 图像检索是在图像数据库中搜索与查询图像相似的项的问题。为了解决这一问题，研究了两种主要的图像表示方法：全局图像特征和局部图像特征。在这项工作中，我们的主要贡献是将全局和局部特征统一到一个单一的深度模型中，从而实现精确的检索和高效的特征提取。我们将新模型称为DELG，代表了深层网络的本地和全局特性。我们利用最近特征学习工作的经验教训，提出了一个将全局特征的广义均值池和局部特征的注意选择相结合的模型。通过仔细平衡两部分之间的梯度流，整个网络可以端到端地学习——只需要图像级别的标签。我们还引入了一种基于自动编码器的局部特征降维技术，并将其集成到模型中，提高了训练效率和匹配性能。在重新修改的牛津和巴黎数据集上的实验表明，我们共同学习的基于ResNet-50的特征优于使用深层全局特征（大多数具有更重量级的主干）和那些进一步使用局部特征重新排序的结果。代码和模型将被发布。 关键词：deep features，image retrieval，unified model...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/05/Paper/Mask2Former_Masked-attention%20Mask%20Transformer%20for%20Universal%20Image%20Segmentation/" title="Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-05</div><div class="info-item-2">Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation</div></div><div class="info-2"><div class="info-item-1">摘要 图像分割将具有不同语义（如类别或实例成员关系）的像素分组，每种语义选择定义了一项任务。虽然各项任务仅在语义上有所不同，但当前研究主要集中于为每个任务设计专门的架构。我们提出了掩码注意力掩码变换器（Mask2Former），这是一种能够处理任何图像分割任务（全景、实例或语义）的新架构。其关键组件包括掩码注意力，它通过将交叉注意力约束在预测掩码区域内来提取局部特征。除了将研究工作量至少减少三倍外，它在四个流行数据集上显著优于最佳专用架构。最值得注意的是，Mask2Former 在全景分割（COCO 上的 57.8 PQ）、实例分割（COCO 上的 50.1 AP）和语义分割（ADE20K 上的 57.7 mIoU）方面设定了新的最先进水平。  1. 引言 图像分割研究像素分组问题。像素分组的不同语义，例如类别或实例成员关系，导致了不同类型的分割任务，如全景、实例或语义分割。虽然这些任务仅在语义上有所不同，但当前方法为每个任务开发专门的架构。基于全卷积网络（FCN）的逐像素分类架构用于语义分割，而预测一组与单个类别相关联的二进制掩码的掩码分类架构在实例级分割中占主导地位。尽管这些...</div></div></div></a><a class="pagination-related" href="/2020/07/02/Paper/Unifying%20Deep%20Local%20and%20Global%20Features%20for%20Image%20Search/" title="Unifying Deep Local and Global Features for Image Search"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-02</div><div class="info-item-2">Unifying Deep Local and Global Features for Image Search</div></div><div class="info-2"><div class="info-item-1">统一局部和全局特征进行图像搜索的深层(网络) ​    本文使用机翻，稍加润色，主要用于个人理解，不恰当之处请看客见谅。 摘要 图像检索是在图像数据库中搜索与查询图像相似的项的问题。为了解决这一问题，研究了两种主要的图像表示方法：全局图像特征和局部图像特征。在这项工作中，我们的主要贡献是将全局和局部特征统一到一个单一的深度模型中，从而实现精确的检索和高效的特征提取。我们将新模型称为DELG，代表了深层网络的本地和全局特性。我们利用最近特征学习工作的经验教训，提出了一个将全局特征的广义均值池和局部特征的注意选择相结合的模型。通过仔细平衡两部分之间的梯度流，整个网络可以端到端地学习——只需要图像级别的标签。我们还引入了一种基于自动编码器的局部特征降维技术，并将其集成到模型中，提高了训练效率和匹配性能。在重新修改的牛津和巴黎数据集上的实验表明，我们共同学习的基于ResNet-50的特征优于使用深层全局特征（大多数具有更重量级的主干）和那些进一步使用局部特征重新排序的结果。代码和模型将被发布。 关键词：deep features，image retrieval，unified model...</div></div></div></a><a class="pagination-related" href="/2024/08/05/Paper/Hierarchical%20Graph%20Pooling%20with%20Structure%20Learning/" title="Hierarchical Graph Pooling with Structure Learning"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-05</div><div class="info-item-2">Hierarchical Graph Pooling with Structure Learning</div></div><div class="info-2"><div class="info-item-1">Hierarchical Graph Pooling with Structure Learning 会议: AAAI 2020(疑似撤稿) 论文地址：https://arxiv.org/abs/1911.05954 github: https://github.com/cszhangzhen/HGP-SL DGL开源库：https://github.com/dmlc/dgl/tree/master/examples/pytorch/hgp_sl [TOC] 摘要 图神经网络 (GNN) 将深度神经网络扩展到图结构数据，在许多图相关任务中取得了最先进的性能。然而，现有的 GNN 模型主要关注设计图卷积操作。图池化 (或下采样) 操作在分层表示学习中发挥着重要作用，通常被忽视。在这篇论文中，我们提出了一种新的图池化操作符，称为具有结构学习的分层图池化 (HGP-SL)，它可以集成到各种图神经网络架构中。HGP-SL 将图池化和结构学习集成到一个统一的模块中，以生成图的分层表示。具体来说，图池化操作根据我们定义的节点信息分数自适应地选择一组节点来形成一个诱导子图，用于后续层。为了保留...</div></div></div></a><a class="pagination-related" href="/2024/11/05/Paper/GraphSAGE_Inductive%20Representation%20Learning%20on%20Large%20Graphs/" title="Graph SAGE: Inductive Representation Learning on Large Graphs"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-05</div><div class="info-item-2">Graph SAGE: Inductive Representation Learning on Large Graphs</div></div><div class="info-2"><div class="info-item-1">Inductive Representation Learning on Large Graphs </div></div></div></a><a class="pagination-related" href="/2026/02/05/Paper/YOLOv11/" title="YOLOv11技术文档"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-05</div><div class="info-item-2">YOLOv11技术文档</div></div><div class="info-2"><div class="info-item-1">[TOC] 1.概述 YOLOv11 是 Ultralytics 公司于 2024年9月30日正式发布的新一代目标检测框架。作为 YOLO 系列的第11代迭代，YOLOv11 在保持与 YOLOv8/v9/v10 高度兼容的 API 设计基础上，通过三大核心技术创新实现了精度与效率的双重突破：在 COCO 数据集上 mAP 指标平均提升 2-3 个百分点，同时推理速度提升约 15%，参数量减少 22%。 YOLOv11 的核心定位是**“面向复杂场景的轻量化实时检测”**，特别针对小目标检测、遮挡目标识别和密集场景分析进行了深度优化。其创新设计包括：  C3k2结构：替代 YOLOv8 的 C2f 模块，通过参数 c3k 控制浅层网络特性，优化计算效率。 新增C2PSA模块：在传统 C2 结构中嵌入位置敏感注意力机制（PSA），增强全局上下文建模能力。 深度可分离卷积应用：在分类分支中替换标准卷积为 DWConv，减少参数量 40%，降低显存占用 30%。 模型结构调整：通过调整 depth、width、max_channels 的比例参数，实现不同规模模型的性能平衡。  YOL...</div></div></div></a><a class="pagination-related" href="/2025/04/22/Paper/ABINet_Autonomous,%20Bidirectional%20and%20Iterative%20Language%20Modeling%20for%20Scene%20Text%20Recognition/" title="ABINet: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-22</div><div class="info-item-2">ABINet: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition</div></div><div class="info-2"><div class="info-item-1">[TOC]  名称：Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition 论文：https://arxiv.org/abs/2103.06495 会议：AAAI2020 Github: https://github.com/FangShancheng/ABINet  ABINet（Attention-based Bidirectional Network）是一种用于场景文本识别（Scene Text Recognition, STR）的深度学习模型。它在处理复杂背景、噪声干扰以及弯曲或倾斜文本时表现出色。ABINet 的核心创新点是引入了 双向注意力机制 和 迭代优化策略 ，从而显著提升了文本识别的准确性和鲁棒性。 以下是 ABINet 的详细解析，包括其架构设计、工作原理、优势和实现细节。  1. ABINet 的背景 问题  自然场景中的文本通常具有复杂的形状（如弯曲、倾斜等），并且背景可能包含大量噪声。 传统的基于分类的方法...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/admin_head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">神火不知灭</div><div class="author-info-description">日常记录学习用博客，仅用来练习使用</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">64</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">64</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/dollarser"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">技术笔记，日常记录</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B7%B1%E5%B1%82%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81%E7%9A%84%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2"><span class="toc-number">1.</span> <span class="toc-text">注意力深层局部特征的大规模图像检索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.2.</span> <span class="toc-text">1. 介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">1.3.</span> <span class="toc-text">2. 相关工作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%B0%B7%E6%AD%8C%E5%9C%B0%E6%A0%87%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.4.</span> <span class="toc-text">3. 谷歌地标数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%BD%BF%E7%94%A8DELF%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2"><span class="toc-number">1.5.</span> <span class="toc-text">4. 使用DELF图像检索</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-%E5%AF%86%E9%9B%86%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-number">1.5.1.</span> <span class="toc-text">4.1 密集局部特征提取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E9%80%89%E6%8B%A9"><span class="toc-number">1.5.2.</span> <span class="toc-text">4.2 基于注意力的关键点选择</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-1-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">4.2.1 弱监督学习</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-2-%E8%AE%AD%E7%BB%83%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">1.5.2.2.</span> <span class="toc-text">4.2.2 训练注意力</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-3-%E7%89%B9%E7%82%B9"><span class="toc-number">1.5.2.3.</span> <span class="toc-text">4.2.3 特点</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-%E9%99%8D%E7%BB%B4"><span class="toc-number">1.5.3.</span> <span class="toc-text">4.3 降维</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-%E5%9B%BE%E7%89%87%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F"><span class="toc-number">1.5.4.</span> <span class="toc-text">4.4 图片检索系统</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%AE%9E%E9%AA%8C"><span class="toc-number">1.6.</span> <span class="toc-text">5 实验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-%E5%AE%9E%E6%96%BD%E7%BB%86%E8%8A%82"><span class="toc-number">1.6.1.</span> <span class="toc-text">5.1 实施细节</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-%E7%AE%97%E6%B3%95%E6%AF%94%E8%BE%83"><span class="toc-number">1.6.2.</span> <span class="toc-text">5.2 算法比较</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-%E8%AF%84%E4%BC%B0"><span class="toc-number">1.6.3.</span> <span class="toc-text">5.3 评估</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-%E5%AE%9A%E9%87%8F%E7%BB%93%E6%9E%9C"><span class="toc-number">1.6.4.</span> <span class="toc-text">5.4 定量结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-%E5%AE%9A%E9%87%8F%E7%BB%93%E6%9E%9C"><span class="toc-number">1.6.5.</span> <span class="toc-text">5.5 定量结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-6-%E7%8E%B0%E6%9C%89%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84%E7%BB%93%E6%9E%9C"><span class="toc-number">1.6.6.</span> <span class="toc-text">5.6 现有数据集中的结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E7%BB%93%E8%AE%BA"><span class="toc-number">1.7.</span> <span class="toc-text">6 结论</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/12/Python/Claude_Code/" title="Claude Code应用指南">Claude Code应用指南</a><time datetime="2026-02-12T09:30:00.000Z" title="Created 2026-02-12 17:30:00">2026-02-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/12/Python/MuSGD/" title="MuSGD优化器">MuSGD优化器</a><time datetime="2026-02-12T09:00:00.000Z" title="Created 2026-02-12 17:00:00">2026-02-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/11/Python/pytorch-%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" title="深度学习常用损失函数">深度学习常用损失函数</a><time datetime="2026-02-11T10:00:00.000Z" title="Created 2026-02-11 18:00:00">2026-02-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/10/Python/%E4%BD%8E%E7%A7%A9%E8%BF%91%E4%BC%BC/" title="矩阵相关">矩阵相关</a><time datetime="2026-02-10T10:00:00.000Z" title="Created 2026-02-10 18:00:00">2026-02-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/10/Python/Muon%E4%BC%98%E5%8C%96%E5%99%A8/" title="Muon优化器">Muon优化器</a><time datetime="2026-02-10T09:00:00.000Z" title="Created 2026-02-10 17:00:00">2026-02-10</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By 神火不知灭</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex@0.16.28/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex@0.16.28/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>