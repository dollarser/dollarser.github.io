<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation | 遗世独立</title><meta name="author" content="神火不知灭"><meta name="copyright" content="神火不知灭"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="摘要 图像分割将具有不同语义（如类别或实例成员关系）的像素分组，每种语义选择定义了一项任务。虽然各项任务仅在语义上有所不同，但当前研究主要集中于为每个任务设计专门的架构。我们提出了掩码注意力掩码变换器（Mask2Former），这是一种能够处理任何图像分割任务（全景、实例或语义）的新架构。其关键组件包括掩码注意力，它通过将交叉注意力约束在预测掩码区域内来提取局部特征。除了将研究工作量至少减少三倍外">
<meta property="og:type" content="article">
<meta property="og:title" content="Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation">
<meta property="og:url" content="http://blog.sunlingzhang.com/2024/12/05/Paper/Mask2Former_Masked-attention%20Mask%20Transformer%20for%20Universal%20Image%20Segmentation/index.html">
<meta property="og:site_name" content="遗世独立">
<meta property="og:description" content="摘要 图像分割将具有不同语义（如类别或实例成员关系）的像素分组，每种语义选择定义了一项任务。虽然各项任务仅在语义上有所不同，但当前研究主要集中于为每个任务设计专门的架构。我们提出了掩码注意力掩码变换器（Mask2Former），这是一种能够处理任何图像分割任务（全景、实例或语义）的新架构。其关键组件包括掩码注意力，它通过将交叉注意力约束在预测掩码区域内来提取局部特征。除了将研究工作量至少减少三倍外">
<meta property="og:locale">
<meta property="og:image" content="http://blog.sunlingzhang.com/admin_head.jpg">
<meta property="article:published_time" content="2024-12-05T03:00:00.000Z">
<meta property="article:modified_time" content="2026-02-12T10:34:06.980Z">
<meta property="article:author" content="神火不知灭">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Mask2Former">
<meta property="article:tag" content="Segmentation">
<meta property="article:tag" content="paper">
<meta property="article:tag" content="论文翻译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.sunlingzhang.com/admin_head.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation",
  "url": "http://blog.sunlingzhang.com/2024/12/05/Paper/Mask2Former_Masked-attention%20Mask%20Transformer%20for%20Universal%20Image%20Segmentation/",
  "image": "http://blog.sunlingzhang.com/admin_head.jpg",
  "datePublished": "2024-12-05T03:00:00.000Z",
  "dateModified": "2026-02-12T10:34:06.980Z",
  "author": [
    {
      "@type": "Person",
      "name": "神火不知灭",
      "url": "http://blog.sunlingzhang.com"
    }
  ]
}</script><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://blog.sunlingzhang.com/2024/12/05/Paper/Mask2Former_Masked-attention%20Mask%20Transformer%20for%20Universal%20Image%20Segmentation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/admin_head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">64</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">64</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/%E5%85%A5%E9%97%A8%E5%AE%9E%E8%B7%B5/"><i class="fa-fw fas fa-keyboard"></i><span> 入门实践</span></a></div><div class="menus_item"><a class="site-page" href="/tags/%E9%9A%8F%E7%AC%94/"><i class="fa-fw fas fa-edit"></i><span> 随笔</span></a></div><div class="menus_item"><a class="site-page" href="/tags/paper/"><i class="fa-fw fal fa-paperclip"></i><span> 论文解读</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 计算机</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/algorithm/"><i class="fa-fw fas fa-code"></i><span> 算法</span></a></li><li><a class="site-page child" href="/data-structure/"><i class="fa-fw fas fa-terminal"></i><span> 数据结构</span></a></li><li><a class="site-page child" href="/operation-system/"><i class="fa-fw fas fa-desktop"></i><span> 操作系统</span></a></li><li><a class="site-page child" href="/computer-composition/"><i class="fa-fw fas fa-microchip"></i><span> 计算机组成原理</span></a></li><li><a class="site-page child" href="/network/"><i class="fa-fw fas fa-network-wired"></i><span> 计算机网络</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 页面</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 编程语言</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tags/Java/"><i class="fa-fw fas fa-music"></i><span> Java</span></a></li><li><a class="site-page child" href="/tags/Python/"><i class="fa-fw fas fa-video"></i><span> Python</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">遗世独立</span></a><a class="nav-page-title" href="/"><span class="site-name">Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/%E5%85%A5%E9%97%A8%E5%AE%9E%E8%B7%B5/"><i class="fa-fw fas fa-keyboard"></i><span> 入门实践</span></a></div><div class="menus_item"><a class="site-page" href="/tags/%E9%9A%8F%E7%AC%94/"><i class="fa-fw fas fa-edit"></i><span> 随笔</span></a></div><div class="menus_item"><a class="site-page" href="/tags/paper/"><i class="fa-fw fal fa-paperclip"></i><span> 论文解读</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 计算机</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/algorithm/"><i class="fa-fw fas fa-code"></i><span> 算法</span></a></li><li><a class="site-page child" href="/data-structure/"><i class="fa-fw fas fa-terminal"></i><span> 数据结构</span></a></li><li><a class="site-page child" href="/operation-system/"><i class="fa-fw fas fa-desktop"></i><span> 操作系统</span></a></li><li><a class="site-page child" href="/computer-composition/"><i class="fa-fw fas fa-microchip"></i><span> 计算机组成原理</span></a></li><li><a class="site-page child" href="/network/"><i class="fa-fw fas fa-network-wired"></i><span> 计算机网络</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 页面</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 编程语言</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tags/Java/"><i class="fa-fw fas fa-music"></i><span> Java</span></a></li><li><a class="site-page child" href="/tags/Python/"><i class="fa-fw fas fa-video"></i><span> Python</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-12-05T03:00:00.000Z" title="Created 2024-12-05 11:00:00">2024-12-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-12T10:34:06.980Z" title="Updated 2026-02-12 18:34:06">2026-02-12</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h2 id="摘要">摘要</h2>
<p>图像分割将具有不同语义（如类别或实例成员关系）的像素分组，每种语义选择定义了一项任务。虽然各项任务仅在语义上有所不同，但当前研究主要集中于为每个任务设计专门的架构。我们提出了掩码注意力掩码变换器（Mask2Former），这是一种能够处理任何图像分割任务（全景、实例或语义）的新架构。其关键组件包括掩码注意力，它通过将交叉注意力约束在预测掩码区域内来提取局部特征。除了将研究工作量至少减少三倍外，它在四个流行数据集上显著优于最佳专用架构。最值得注意的是，Mask2Former 在全景分割（COCO 上的 57.8 PQ）、实例分割（COCO 上的 50.1 AP）和语义分割（ADE20K 上的 57.7 mIoU）方面设定了新的最先进水平。</p>
<span id="more"></span>
<h2 id="1-引言">1. 引言</h2>
<p>图像分割研究像素分组问题。像素分组的不同语义，例如类别或实例成员关系，导致了不同类型的分割任务，如全景、实例或语义分割。虽然这些任务仅在语义上有所不同，但当前方法为每个任务开发专门的架构。基于全卷积网络（FCN）的逐像素分类架构用于语义分割，而预测一组与单个类别相关联的二进制掩码的掩码分类架构在实例级分割中占主导地位。尽管这些专门架构推动了各个任务的发展，但它们缺乏推广到其他任务的灵活性。例如，基于 FCN 的架构在实例分割方面表现不佳，导致与语义分割相比，实例分割发展出了不同的架构。因此，在每个任务的每个专门架构上都花费了重复的研究和（硬件）优化工作。</p>
<p>为了解决这种碎片化问题，近期的工作尝试设计通用架构，能够使用相同架构处理所有分割任务（即通用图像分割）。这些架构通常基于端到端的集合预测目标（如 DETR），并且在不修改架构、损失或训练过程的情况下成功处理多个任务。请注意，通用架构仍然针对不同任务和数据集分别进行训练，尽管它们具有相同的架构。除了灵活性之外，通用架构最近在语义和全景分割方面也取得了最先进的成果。然而，近期的工作仍然侧重于推进专用架构，这就提出了一个问题：为什么通用架构没有取代专用架构呢？</p>
<p>尽管现有通用架构足够灵活，可以处理任何分割任务，但如图 1 所示，在实践中它们的性能落后于最佳专用架构。例如，通用架构的最佳报告性能目前比实例分割的 SOTA 专用架构低（&gt; 9 AP）。除了性能较差之外，通用架构也更难训练。它们通常需要更先进的硬件和更长的训练计划。例如，训练 MaskFormer 需要 300 个 epoch 才能达到 40.1 AP，并且在具有 32G 内存的 GPU 中只能容纳单张图像。相比之下，专用的 Swin - HTC++ 仅在 72 个 epoch 内就能获得更好的性能。性能和训练效率问题都阻碍了通用架构的部署。</p>
<p>在这项工作中，我们提出了一种名为掩码注意力掩码变换器（Mask2Former）的通用图像分割架构，它在不同分割任务上优于专用架构，同时在每个任务上仍然易于训练。我们基于一个简单的元架构构建，该架构由骨干特征提取器、像素解码器和 Transformer 解码器组成。我们提出了关键改进，以实现更好的结果和高效训练。首先，我们在 Transformer 解码器中使用掩码注意力，它将注意力限制在以预测片段为中心的局部特征上，这些片段可以根据分组的特定语义是对象或区域。与标准 Transformer 解码器中关注图像中所有位置的交叉注意力相比，我们的掩码注意力导致更快的收敛和改进的性能。其次，我们使用多尺度高分辨率特征，这有助于模型分割小对象 / 区域。第三，我们提出优化改进，例如切换自注意力和交叉注意力的顺序，使查询特征可学习，并去除丢弃法；所有这些都在不增加计算量的情况下提高了性能。最后，我们通过在少量随机采样点上计算掩码损失，在不影响性能的情况下节省了 3 倍的训练内存。这些改进不仅提高了模型性能，还使训练变得更加容易，使计算资源有限的用户更容易使用通用架构。</p>
<p>我们在三个图像分割任务（全景、实例和语义分割）上使用四个流行数据集（COCO、Cityscapes、ADE20K 和 Mapillary Vistas）评估 Mask2Former。首次在所有这些基准测试中，我们的单一架构在性能上与专用架构相当或更好。Mask2Former 在 COCO 全景分割上使用完全相同的架构达到了 57.8 PQ 的新最先进水平，在 COCO 实例分割上达到了 50.1 AP，在 ADE20K 语义分割上达到了 57.7 mIoU。</p>
<p>通用架构随着 DETR 的出现而兴起，并表明具有端到端集合预测目标的掩码分类架构对于任何图像分割任务都足够通用。MaskFormer 表明基于 DETR 的掩码分类不仅在全景分割上表现良好，而且在语义分割上也达到了最先进水平。K - Net 进一步将集合预测扩展到实例分割。不幸的是，这些架构未能取代专用模型，因为它们在特定任务或数据集上的性能仍然比最佳专用架构差（例如，MaskFormer 不能很好地分割实例）。据我们所知，Mask2Former 是第一个在所有考虑的任务和数据集上优于最先进专用架构的架构。</p>
<h3 id="图-1-最先进的分割架构通常专门针对每个图像分割任务。尽管最近的工作提出了尝试所有任务并在语义和全景分割上具有竞争力的通用架构，但它们在分割实例方面存在困难。我们提出了-Mask2Former，它首次在多个数据集的三个研究分割任务上优于最佳专用架构。">图 1. 最先进的分割架构通常专门针对每个图像分割任务。尽管最近的工作提出了尝试所有任务并在语义和全景分割上具有竞争力的通用架构，但它们在分割实例方面存在困难。我们提出了 Mask2Former，它首次在多个数据集的三个研究分割任务上优于最佳专用架构。</h3>
<h3 id="1-1-研究现状">1.1 研究现状</h3>
<p>专门的语义分割架构通常将任务视为逐像素分类问题。基于 FCN 的架构独立地为每个像素预测类别标签。后续方法发现上下文对于精确的逐像素分类起着重要作用，并专注于设计定制的上下文模块或自注意力变体。专门的实例分割架构通常基于 “掩码分类”。它们预测一组与单个类别标签相关联的二进制掩码。开创性的工作 Mask R - CNN 从检测到的边界框生成掩码。后续方法要么专注于检测更精确的边界框，要么寻找生成动态数量掩码的新方法，例如使用动态核或聚类算法。尽管在每个任务中的性能都有所提高，但这些专门的创新缺乏从一个任务推广到另一个任务的灵活性，导致重复的研究工作。例如，尽管已经提出了多种构建特征金字塔表示的方法，但正如我们在实验中所示，BiFPN 在实例分割方面表现更好，而 FaPN 在语义分割方面表现更好。全景分割被提出以统一语义和实例分割任务。全景分割架构要么将专门的语义和实例分割架构的最佳部分组合成一个单一框架，要么设计平等对待语义区域和实例对象的新目标。尽管有这些新架构，研究人员仍在继续为不同的图像分割任务开发专门的架构。我们发现全景架构通常只报告在单个全景分割任务上的性能，这不能保证在其他任务上的良好性能（图 1）。例如，全景分割不测量架构对实例分割预测进行排名的能力。因此，我们避免将仅针对全景分割进行评估的架构称为通用架构。相反，在这里，我们在所有研究任务上评估我们的 Mask2Former，以保证其通用性。</p>
<h2 id="2-掩码注意力掩码变换器">2. 掩码注意力掩码变换器</h2>
<p>我们现在介绍 Mask2Former。我们首先回顾 Mask2Former 所基于的掩码分类元架构。然后，我们介绍我们带有掩码注意力的新 Transformer 解码器，这是实现更好收敛和结果的关键。最后，我们提出训练改进措施，使 Mask2Former 高效且易于使用。</p>
<h3 id="2-1-掩码分类基础">2.1 掩码分类基础</h3>
<p>掩码分类架构通过预测 N 个二进制掩码以及 N 个相应的类别标签将像素分组为 N 个片段。掩码分类通过为不同片段分配不同语义（例如类别或实例），足以处理任何分割任务。然而，挑战在于为每个片段找到良好的表示。例如，Mask RCNN 使用边界框作为表示，这限制了其在语义分割中的应用。受 DETR 启发，图像中的每个片段可以表示为一个 c 维特征向量（“对象查询”），并可以由 Transformer 解码器处理，通过集合预测目标进行训练。一个简单的元架构将由三个组件组成。一个骨干网络，从图像中提取低分辨率特征。一个像素解码器，将骨干网络输出的低分辨率特征逐步上采样，以生成高分辨率的逐像素嵌入。最后一个 Transformer 解码器，对图像特征进行操作以处理对象查询。最终的二进制掩码预测从带有对象查询的逐像素嵌入中解码得到。这种元架构的一个成功实例是 MaskFormer，我们建议读者参考 [14] 以获取更多详细信息。</p>
<h3 id="2-2-带掩码注意力的-Transformer-解码器">2.2 带掩码注意力的 Transformer 解码器</h3>
<p>Mask2Former 采用上述元架构，用我们提出的 Transformer 解码器（图 2 右侧）取代标准的 Transformer 解码器。我们的 Transformer 解码器的关键组件包括一个掩码注意力算子，它通过将交叉注意力约束在每个查询的预测掩码的前景区域内来提取局部特征，而不是关注整个特征图。为了处理小对象，我们提出一种有效的多尺度策略来利用高分辨率特征。它以循环方式将像素解码器的特征金字塔中的连续特征图输入到连续的 Transformer 解码器层中。最后，我们纳入优化改进措施，在不引入额外计算的情况下提高模型性能。我们现在详细讨论这些改进。</p>
<h4 id="图-2-Mask2Former-概述。Mask2Former-采用与-MaskFormer-相同的元架构，包括骨干网络、像素解码器和-Transformer-解码器。我们提出一种带有掩码注意力的新-Transformer-解码器，而不是标准的交叉注意力（3-2-1-节）。为了处理小对象，我们提出一种有效的方法，通过每次将多尺度特征的一个尺度输入到一个-Transformer-解码器层来利用像素解码器的高分辨率特征（3-2-2-节）。此外，我们切换自注意力和交叉注意力的顺序（即我们的掩码注意力），使查询特征可学习，并去除丢弃法以使计算更有效（3-2-3-节）。请注意，为了清晰起见，此图中省略了位置嵌入和来自中间-Transformer-解码器层的预测。">图 2. Mask2Former 概述。Mask2Former 采用与 MaskFormer 相同的元架构，包括骨干网络、像素解码器和 Transformer 解码器。我们提出一种带有掩码注意力的新 Transformer 解码器，而不是标准的交叉注意力（3.2.1 节）。为了处理小对象，我们提出一种有效的方法，通过每次将多尺度特征的一个尺度输入到一个 Transformer 解码器层来利用像素解码器的高分辨率特征（3.2.2 节）。此外，我们切换自注意力和交叉注意力的顺序（即我们的掩码注意力），使查询特征可学习，并去除丢弃法以使计算更有效（3.2.3 节）。请注意，为了清晰起见，此图中省略了位置嵌入和来自中间 Transformer 解码器层的预测。</h4>
<h4 id="2-2-1-掩码注意力">2.2.1 掩码注意力</h4>
<p>上下文特征已被证明对图像分割很重要。然而，最近的研究表明，基于 Transformer 的模型收敛缓慢是由于交叉注意力层中的全局上下文，因为交叉注意力需要许多训练 epoch 才能学会关注局部对象区域。我们假设局部特征足以更新查询特征，并且上下文信息可以通过自注意力收集。为此，我们提出掩码注意力，这是交叉注意力的一种变体，它仅关注每个查询的预测掩码的前景区域内。</p>
<p>标准交叉注意力（带有残差路径）计算：。这里， 是层索引， 指的是第 层的 个 维查询特征，。 表示输入到 Transformer 解码器的查询特征。， 是分别在变换 和 下的图像特征， 和 是我们将在 3.2.2 节中介绍的图像特征的空间分辨率。， 和 是线性变换。</p>
<p>我们的掩码注意力通过以下方式调制注意力矩阵：。此外，特征位置 处的注意力掩码 为：。这里， 是前一个（）Transformer 解码器层的调整大小后的掩码预测的二值化输出（阈值为 0.5）。它被调整为与 相同的分辨率。 是从 获得的二进制掩码预测，即在将查询特征输入到 Transformer 解码器之前。</p>
<h4 id="2-2-2-高分辨率特征">2.2.2 高分辨率特征</h4>
<p>高分辨率特征可提高模型性能，特别是对于小对象。然而，这在计算上要求很高。因此，我们提出一种有效的多尺度策略，在控制计算量增加的同时引入高分辨率特征。我们不是始终使用高分辨率特征图，而是利用由低分辨率和高分辨率特征组成的特征金字塔，并一次将多尺度特征的一个分辨率输入到一个 Transformer 解码器层。</p>
<p>具体来说，我们使用像素解码器生成的分辨率为原始图像的 1/32、1/16 和 1/8 的特征金字塔。对于每个分辨率，我们添加一个正弦位置嵌入 （遵循 [5]）和一个可学习的尺度级别嵌入 （遵循 [66]）。我们按照从最低分辨率到最高分辨率的顺序将它们用于相应的 Transformer 解码器层，如图 2 左侧所示。我们重复这个 3 层 Transformer 解码器 次。因此，我们最终的 Transformer 解码器有 层。更具体地说，前三层接收分辨率为 ，， 和 ，， 的特征图，其中 和 是原始图像分辨率。这个模式以循环方式对所有后续层重复。</p>
<h4 id="2-2-3-优化改进">2.2.3 优化改进</h4>
<p>标准 Transformer 解码器层由三个模块组成，按以下顺序处理查询特征：自注意力模块、交叉注意力模块和前馈网络（FFN）。此外，查询特征 在输入到 Transformer 解码器之前被零初始化，并与可学习的位置嵌入相关联。此外，在残差连接和注意力图上都应用了丢弃法。</p>
<p>为了优化 Transformer 解码器设计，我们进行了以下三项改进。首先，我们切换自注意力和交叉注意力（我们新的 “掩码注意力”）的顺序，以使计算更有效：输入到第一个自注意力层的查询特征与图像无关，并且没有来自图像的信号，因此应用自注意力不太可能丰富信息。其次，我们也使查询特征 可学习（我们仍然保留可学习的查询位置嵌入），并且可学习的查询特征在用于 Transformer 解码器预测掩码 之前直接受到监督。我们发现这些可学习的查询特征的功能类似于区域提议网络，并且具有生成掩码提议的能力。最后，我们发现丢弃法不是必需的，并且通常会降低性能。因此，我们在解码器中完全去除了丢弃法。</p>
<h3 id="2-3-提高训练效率">2.3 提高训练效率</h3>
<p>训练通用架构的一个限制是由于高分辨率掩码预测导致的大内存消耗，这使得它们比更节省内存的专用架构更难使用。例如，MaskFormer 在具有 32G 内存的 GPU 中只能容纳单张图像。受 PointRend 和 Implicit PointRend 的启发，它们表明可以通过在 个随机采样点上计算掩码损失而不是在整个掩码上计算来训练分割模型，我们在匹配和最终损失计算中都使用采样点计算掩码损失。更具体地说，在构建二分匹配成本矩阵的匹配损失中，我们为所有预测掩码和真实掩码统一采样相同的 个点集。在预测与匹配的真实掩码之间的最终损失中，我们使用重要性采样为不同的预测掩码和真实掩码对采样不同的 个点集。我们设置 ，即 112×112 个点。这种新的训练策略有效地将训练内存减少了 3 倍，从每张图像 18GB 减少到 6GB，使 Mask2Former 对于计算资源有限的用户更容易使用。</p>
<h2 id="3-实验">3. 实验</h2>
<p>我们通过在标准基准上与专用的最先进架构进行比较，证明 Mask2Former 是通用图像分割的有效架构。我们通过在所有三个任务上进行消融实验来评估我们提出的设计决策。最后，我们表明 Mask2Former 可以推广到标准基准之外，在四个数据集上获得最先进的结果。</p>
<h3 id="3-1-数据集">3.1 数据集</h3>
<p>我们使用四个广泛使用的支持语义、实例和全景分割的图像分割数据集来研究 Mask2Former：COCO（80 个 “事物” 和 53 个 “物品” 类别）、ADE20K（100 个 “事物” 和 50 个 “物品” 类别）、Cityscapes（8 个 “事物” 和 11 个 “物品” 类别）和 Mapillary Vistas（37 个 “事物” 和 28 个 “物品” 类别）。全景和语义分割任务在 “事物” 和 “物品” 类别的并集上进行评估，而实例分割仅在 “事物” 类别上进行评估。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://blog.sunlingzhang.com">神火不知灭</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://blog.sunlingzhang.com/2024/12/05/Paper/Mask2Former_Masked-attention%20Mask%20Transformer%20for%20Universal%20Image%20Segmentation/">http://blog.sunlingzhang.com/2024/12/05/Paper/Mask2Former_Masked-attention%20Mask%20Transformer%20for%20Universal%20Image%20Segmentation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/paper/">paper</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">论文翻译</a><a class="post-meta__tags" href="/tags/Mask2Former/">Mask2Former</a><a class="post-meta__tags" href="/tags/Segmentation/">Segmentation</a></div><div class="post-share"><div class="social-share" data-image="/admin_head.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/04/16/Python/pytorch-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-CTCLoss/" title="损失函数-CTCLoss"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">损失函数-CTCLoss</div></div><div class="info-2"><div class="info-item-1">什么是CTCLoss？ CTC (Connectionist Temporal Classification) 是一种用于序列到序列学习的损失函数，特别适用于输入和输出长度不固定的场景。它在语音识别、手写体识别等任务中应用广泛。CTC 的核心思想是通过引入一个“空白”符号（blank token），允许模型对不定长的输入序列生成不定长的输出序列，同时避免了对输入和输出进行显式的对齐操作。 传统的序列标注方法通常需要将输入和输出进行严格的对齐（例如，逐帧标注），而 CTC 允许模型自动学习输入和输出之间的对齐关系，从而大大简化了训练过程。   CTCLoss 的工作原理 1. 输入与输出的关系  输入是一个不定长的序列，比如语音信号或手写笔迹的时间序列。 输出是一个较短的目标序列，比如文本转录结果。 输入和输出的长度可能不同，且没有明确的对齐关系。  2. 引入空白符号 CTC 引入了一个特殊的“空白”符号（通常记作 - 或 blank），表示某个时间步没有对应的输出。空白符号在最终的输出中会被移除。 3. 路径的概念 CTC 将输入序列到输出序列的所有可能对齐方式称为“路径”。例...</div></div></div></a><a class="pagination-related" href="/2024/11/05/Paper/GraphSAGE_Inductive%20Representation%20Learning%20on%20Large%20Graphs/" title="Graph SAGE: Inductive Representation Learning on Large Graphs"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Graph SAGE: Inductive Representation Learning on Large Graphs</div></div><div class="info-2"><div class="info-item-1">Inductive Representation Learning on Large Graphs </div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/11/05/Paper/GraphSAGE_Inductive%20Representation%20Learning%20on%20Large%20Graphs/" title="Graph SAGE: Inductive Representation Learning on Large Graphs"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-05</div><div class="info-item-2">Graph SAGE: Inductive Representation Learning on Large Graphs</div></div><div class="info-2"><div class="info-item-1">Inductive Representation Learning on Large Graphs </div></div></div></a><a class="pagination-related" href="/2024/08/05/Paper/Hierarchical%20Graph%20Pooling%20with%20Structure%20Learning/" title="Hierarchical Graph Pooling with Structure Learning"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-05</div><div class="info-item-2">Hierarchical Graph Pooling with Structure Learning</div></div><div class="info-2"><div class="info-item-1">Hierarchical Graph Pooling with Structure Learning 会议: AAAI 2020(疑似撤稿) 论文地址：https://arxiv.org/abs/1911.05954 github: https://github.com/cszhangzhen/HGP-SL DGL开源库：https://github.com/dmlc/dgl/tree/master/examples/pytorch/hgp_sl [TOC] 摘要 图神经网络 (GNN) 将深度神经网络扩展到图结构数据，在许多图相关任务中取得了最先进的性能。然而，现有的 GNN 模型主要关注设计图卷积操作。图池化 (或下采样) 操作在分层表示学习中发挥着重要作用，通常被忽视。在这篇论文中，我们提出了一种新的图池化操作符，称为具有结构学习的分层图池化 (HGP-SL)，它可以集成到各种图神经网络架构中。HGP-SL 将图池化和结构学习集成到一个统一的模块中，以生成图的分层表示。具体来说，图池化操作根据我们定义的节点信息分数自适应地选择一组节点来形成一个诱导子图，用于后续层。为了保留...</div></div></div></a><a class="pagination-related" href="/2020/07/02/Paper/Unifying%20Deep%20Local%20and%20Global%20Features%20for%20Image%20Search/" title="Unifying Deep Local and Global Features for Image Search"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-02</div><div class="info-item-2">Unifying Deep Local and Global Features for Image Search</div></div><div class="info-2"><div class="info-item-1">统一局部和全局特征进行图像搜索的深层(网络) ​    本文使用机翻，稍加润色，主要用于个人理解，不恰当之处请看客见谅。 摘要 图像检索是在图像数据库中搜索与查询图像相似的项的问题。为了解决这一问题，研究了两种主要的图像表示方法：全局图像特征和局部图像特征。在这项工作中，我们的主要贡献是将全局和局部特征统一到一个单一的深度模型中，从而实现精确的检索和高效的特征提取。我们将新模型称为DELG，代表了深层网络的本地和全局特性。我们利用最近特征学习工作的经验教训，提出了一个将全局特征的广义均值池和局部特征的注意选择相结合的模型。通过仔细平衡两部分之间的梯度流，整个网络可以端到端地学习——只需要图像级别的标签。我们还引入了一种基于自动编码器的局部特征降维技术，并将其集成到模型中，提高了训练效率和匹配性能。在重新修改的牛津和巴黎数据集上的实验表明，我们共同学习的基于ResNet-50的特征优于使用深层全局特征（大多数具有更重量级的主干）和那些进一步使用局部特征重新排序的结果。代码和模型将被发布。 关键词：deep features，image retrieval，unified model...</div></div></div></a><a class="pagination-related" href="/2020/07/03/Paper/Large-Scale%20Image%20Retrieval%20with%20Attentive%20Deep%20Local%20Features/" title="Large-Scale Image Retrieval with Attentive Deep Local Features"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2020-07-03</div><div class="info-item-2">Large-Scale Image Retrieval with Attentive Deep Local Features</div></div><div class="info-2"><div class="info-item-1">注意力深层局部特征的大规模图像检索 摘要 提出了一种适合于大规模图像检索的局部特征描述器，称为Deep-local-feature。新的特征是基于卷积神经网络，它只在地标图像数据集上使用图像级注释进行训练。为了识别在语义上有用的图像检索局部特征，我们还提出了一种用于关键点选择的注意机制，该机制与描述符共享大部分网络层。该框架可用于图像检索，作为其他关键点检测器和描述符的替代品，实现更精确的特征匹配和几何匹配验证。我们的系统产生可信的分数拒绝误报(FP)，尤其是它的健壮性针对数据库中没有正确匹配的查询。为了评估所提出的描述符，我们引入了一个新的大规模数据集，被称为谷歌地标(GLD)数据集，包括数据库和 查询搜索作为背景杂波，部分遮挡，多个地标、可变尺度的物体等DELF的成绩超过了全球和当地最先进的水平(SOTA)在大范围数据集中的描述符。可在以下网页找到项目代码：https://github.com/tensorflow/models/tree/master/research/delf。  1. 介绍 大规模图像检索是计算机视觉中的一项基本任务，它直接关系到目标检测、视觉位置识别、...</div></div></div></a><a class="pagination-related" href="/2025/04/21/Paper/DBNet_Real-time%20Scene%20Text%20Detection%20with%20Differentiable%20Binarization/" title="DBNet: Real-time Scene Text Detection with Differentiable Binarization"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-21</div><div class="info-item-2">DBNet: Real-time Scene Text Detection with Differentiable Binarization</div></div><div class="info-2"><div class="info-item-1">[TOC] 名称：DBNet: Real-time Scene Text Detection with Differentiable Binarization 论文：https://arxiv.org/abs/1911.08947 会议：AAAI2020 V2：Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion V2：https://arxiv.org/abs/2202.10304 顶刊：TPAMI 2022  DBNet（Differentiable Binarization Network）是一种用于文本检测的深度学习模型，特别适用于自然场景中的文本检测任务。它在处理弯曲、倾斜或复杂背景中的文本时表现出色。DBNet 的核心创新点是引入了 可微分二值化（Differentiable Binarization, DB） 模块，使得模型能够在训练过程中直接优化分割掩码的二值化效果。 以下是 DBNet 的详细解析，包括其架构设计、工作原理、优势和实现细节...</div></div></div></a><a class="pagination-related" href="/2025/04/22/Paper/ABINet_Autonomous,%20Bidirectional%20and%20Iterative%20Language%20Modeling%20for%20Scene%20Text%20Recognition/" title="ABINet: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-22</div><div class="info-item-2">ABINet: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition</div></div><div class="info-2"><div class="info-item-1">[TOC]  名称：Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition 论文：https://arxiv.org/abs/2103.06495 会议：AAAI2020 Github: https://github.com/FangShancheng/ABINet  ABINet（Attention-based Bidirectional Network）是一种用于场景文本识别（Scene Text Recognition, STR）的深度学习模型。它在处理复杂背景、噪声干扰以及弯曲或倾斜文本时表现出色。ABINet 的核心创新点是引入了 双向注意力机制 和 迭代优化策略 ，从而显著提升了文本识别的准确性和鲁棒性。 以下是 ABINet 的详细解析，包括其架构设计、工作原理、优势和实现细节。  1. ABINet 的背景 问题  自然场景中的文本通常具有复杂的形状（如弯曲、倾斜等），并且背景可能包含大量噪声。 传统的基于分类的方法...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/admin_head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">神火不知灭</div><div class="author-info-description">日常记录学习用博客，仅用来练习使用</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">64</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">64</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/dollarser"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">技术笔记，日常记录</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%BC%95%E8%A8%80"><span class="toc-number">2.</span> <span class="toc-text">1. 引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE-1-%E6%9C%80%E5%85%88%E8%BF%9B%E7%9A%84%E5%88%86%E5%89%B2%E6%9E%B6%E6%9E%84%E9%80%9A%E5%B8%B8%E4%B8%93%E9%97%A8%E9%92%88%E5%AF%B9%E6%AF%8F%E4%B8%AA%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E3%80%82%E5%B0%BD%E7%AE%A1%E6%9C%80%E8%BF%91%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%8F%90%E5%87%BA%E4%BA%86%E5%B0%9D%E8%AF%95%E6%89%80%E6%9C%89%E4%BB%BB%E5%8A%A1%E5%B9%B6%E5%9C%A8%E8%AF%AD%E4%B9%89%E5%92%8C%E5%85%A8%E6%99%AF%E5%88%86%E5%89%B2%E4%B8%8A%E5%85%B7%E6%9C%89%E7%AB%9E%E4%BA%89%E5%8A%9B%E7%9A%84%E9%80%9A%E7%94%A8%E6%9E%B6%E6%9E%84%EF%BC%8C%E4%BD%86%E5%AE%83%E4%BB%AC%E5%9C%A8%E5%88%86%E5%89%B2%E5%AE%9E%E4%BE%8B%E6%96%B9%E9%9D%A2%E5%AD%98%E5%9C%A8%E5%9B%B0%E9%9A%BE%E3%80%82%E6%88%91%E4%BB%AC%E6%8F%90%E5%87%BA%E4%BA%86-Mask2Former%EF%BC%8C%E5%AE%83%E9%A6%96%E6%AC%A1%E5%9C%A8%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E4%B8%89%E4%B8%AA%E7%A0%94%E7%A9%B6%E5%88%86%E5%89%B2%E4%BB%BB%E5%8A%A1%E4%B8%8A%E4%BC%98%E4%BA%8E%E6%9C%80%E4%BD%B3%E4%B8%93%E7%94%A8%E6%9E%B6%E6%9E%84%E3%80%82"><span class="toc-number">2.1.</span> <span class="toc-text">图 1. 最先进的分割架构通常专门针对每个图像分割任务。尽管最近的工作提出了尝试所有任务并在语义和全景分割上具有竞争力的通用架构，但它们在分割实例方面存在困难。我们提出了 Mask2Former，它首次在多个数据集的三个研究分割任务上优于最佳专用架构。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E7%A0%94%E7%A9%B6%E7%8E%B0%E7%8A%B6"><span class="toc-number">2.2.</span> <span class="toc-text">1.1 研究现状</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%8E%A9%E7%A0%81%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8E%A9%E7%A0%81%E5%8F%98%E6%8D%A2%E5%99%A8"><span class="toc-number">3.</span> <span class="toc-text">2. 掩码注意力掩码变换器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%8E%A9%E7%A0%81%E5%88%86%E7%B1%BB%E5%9F%BA%E7%A1%80"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 掩码分类基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%B8%A6%E6%8E%A9%E7%A0%81%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84-Transformer-%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 带掩码注意力的 Transformer 解码器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE-2-Mask2Former-%E6%A6%82%E8%BF%B0%E3%80%82Mask2Former-%E9%87%87%E7%94%A8%E4%B8%8E-MaskFormer-%E7%9B%B8%E5%90%8C%E7%9A%84%E5%85%83%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%8C%85%E6%8B%AC%E9%AA%A8%E5%B9%B2%E7%BD%91%E7%BB%9C%E3%80%81%E5%83%8F%E7%B4%A0%E8%A7%A3%E7%A0%81%E5%99%A8%E5%92%8C-Transformer-%E8%A7%A3%E7%A0%81%E5%99%A8%E3%80%82%E6%88%91%E4%BB%AC%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E5%B8%A6%E6%9C%89%E6%8E%A9%E7%A0%81%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E6%96%B0-Transformer-%E8%A7%A3%E7%A0%81%E5%99%A8%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF%E6%A0%87%E5%87%86%E7%9A%84%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%883-2-1-%E8%8A%82%EF%BC%89%E3%80%82%E4%B8%BA%E4%BA%86%E5%A4%84%E7%90%86%E5%B0%8F%E5%AF%B9%E8%B1%A1%EF%BC%8C%E6%88%91%E4%BB%AC%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E6%9C%89%E6%95%88%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E9%80%9A%E8%BF%87%E6%AF%8F%E6%AC%A1%E5%B0%86%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B0%BA%E5%BA%A6%E8%BE%93%E5%85%A5%E5%88%B0%E4%B8%80%E4%B8%AA-Transformer-%E8%A7%A3%E7%A0%81%E5%99%A8%E5%B1%82%E6%9D%A5%E5%88%A9%E7%94%A8%E5%83%8F%E7%B4%A0%E8%A7%A3%E7%A0%81%E5%99%A8%E7%9A%84%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E7%89%B9%E5%BE%81%EF%BC%883-2-2-%E8%8A%82%EF%BC%89%E3%80%82%E6%AD%A4%E5%A4%96%EF%BC%8C%E6%88%91%E4%BB%AC%E5%88%87%E6%8D%A2%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8C%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E9%A1%BA%E5%BA%8F%EF%BC%88%E5%8D%B3%E6%88%91%E4%BB%AC%E7%9A%84%E6%8E%A9%E7%A0%81%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%89%EF%BC%8C%E4%BD%BF%E6%9F%A5%E8%AF%A2%E7%89%B9%E5%BE%81%E5%8F%AF%E5%AD%A6%E4%B9%A0%EF%BC%8C%E5%B9%B6%E5%8E%BB%E9%99%A4%E4%B8%A2%E5%BC%83%E6%B3%95%E4%BB%A5%E4%BD%BF%E8%AE%A1%E7%AE%97%E6%9B%B4%E6%9C%89%E6%95%88%EF%BC%883-2-3-%E8%8A%82%EF%BC%89%E3%80%82%E8%AF%B7%E6%B3%A8%E6%84%8F%EF%BC%8C%E4%B8%BA%E4%BA%86%E6%B8%85%E6%99%B0%E8%B5%B7%E8%A7%81%EF%BC%8C%E6%AD%A4%E5%9B%BE%E4%B8%AD%E7%9C%81%E7%95%A5%E4%BA%86%E4%BD%8D%E7%BD%AE%E5%B5%8C%E5%85%A5%E5%92%8C%E6%9D%A5%E8%87%AA%E4%B8%AD%E9%97%B4-Transformer-%E8%A7%A3%E7%A0%81%E5%99%A8%E5%B1%82%E7%9A%84%E9%A2%84%E6%B5%8B%E3%80%82"><span class="toc-number">3.2.1.</span> <span class="toc-text">图 2. Mask2Former 概述。Mask2Former 采用与 MaskFormer 相同的元架构，包括骨干网络、像素解码器和 Transformer 解码器。我们提出一种带有掩码注意力的新 Transformer 解码器，而不是标准的交叉注意力（3.2.1 节）。为了处理小对象，我们提出一种有效的方法，通过每次将多尺度特征的一个尺度输入到一个 Transformer 解码器层来利用像素解码器的高分辨率特征（3.2.2 节）。此外，我们切换自注意力和交叉注意力的顺序（即我们的掩码注意力），使查询特征可学习，并去除丢弃法以使计算更有效（3.2.3 节）。请注意，为了清晰起见，此图中省略了位置嵌入和来自中间 Transformer 解码器层的预测。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-%E6%8E%A9%E7%A0%81%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">3.2.2.</span> <span class="toc-text">2.2.1 掩码注意力</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E7%89%B9%E5%BE%81"><span class="toc-number">3.2.3.</span> <span class="toc-text">2.2.2 高分辨率特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-%E4%BC%98%E5%8C%96%E6%94%B9%E8%BF%9B"><span class="toc-number">3.2.4.</span> <span class="toc-text">2.2.3 优化改进</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%8F%90%E9%AB%98%E8%AE%AD%E7%BB%83%E6%95%88%E7%8E%87"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 提高训练效率</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%AE%9E%E9%AA%8C"><span class="toc-number">4.</span> <span class="toc-text">3. 实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 数据集</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/12/Python/Claude_Code/" title="Claude Code应用指南">Claude Code应用指南</a><time datetime="2026-02-12T09:30:00.000Z" title="Created 2026-02-12 17:30:00">2026-02-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/12/Python/MuSGD/" title="MuSGD优化器">MuSGD优化器</a><time datetime="2026-02-12T09:00:00.000Z" title="Created 2026-02-12 17:00:00">2026-02-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/11/Python/pytorch-%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" title="深度学习常用损失函数">深度学习常用损失函数</a><time datetime="2026-02-11T10:00:00.000Z" title="Created 2026-02-11 18:00:00">2026-02-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/10/Python/%E4%BD%8E%E7%A7%A9%E8%BF%91%E4%BC%BC/" title="矩阵相关">矩阵相关</a><time datetime="2026-02-10T10:00:00.000Z" title="Created 2026-02-10 18:00:00">2026-02-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/10/Python/Muon%E4%BC%98%E5%8C%96%E5%99%A8/" title="Muon优化器">Muon优化器</a><time datetime="2026-02-10T09:00:00.000Z" title="Created 2026-02-10 17:00:00">2026-02-10</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By 神火不知灭</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex@0.16.28/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex@0.16.28/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>