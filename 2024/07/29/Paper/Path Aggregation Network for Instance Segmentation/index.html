<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Path Aggregation Network for Instance Segmentation | 遗世独立</title><meta name="author" content="神火不知灭"><meta name="copyright" content="神火不知灭"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="用于实例分割的路径聚合网络 会议: CVPR 2018 论文地址：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1803.01534 github: https:&#x2F;&#x2F;github.com&#x2F;ShuLiu1993&#x2F;PANet [TOC] 摘要 信息在神经网络中的传播方式至关重要。本文提出了路径聚合网络 (PANet)，旨在提升基于候选框的实例分割框架中的信息流。具体来说，我们通过自底向上的路径增强，将低">
<meta property="og:type" content="article">
<meta property="og:title" content="Path Aggregation Network for Instance Segmentation">
<meta property="og:url" content="http://blog.sunlingzhang.com/2024/07/29/Paper/Path%20Aggregation%20Network%20for%20Instance%20Segmentation/index.html">
<meta property="og:site_name" content="遗世独立">
<meta property="og:description" content="用于实例分割的路径聚合网络 会议: CVPR 2018 论文地址：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1803.01534 github: https:&#x2F;&#x2F;github.com&#x2F;ShuLiu1993&#x2F;PANet [TOC] 摘要 信息在神经网络中的传播方式至关重要。本文提出了路径聚合网络 (PANet)，旨在提升基于候选框的实例分割框架中的信息流。具体来说，我们通过自底向上的路径增强，将低">
<meta property="og:locale">
<meta property="og:image" content="http://blog.sunlingzhang.com/admin_head.jpg">
<meta property="article:published_time" content="2024-07-29T03:49:00.000Z">
<meta property="article:modified_time" content="2026-02-12T10:34:06.980Z">
<meta property="article:author" content="神火不知灭">
<meta property="article:tag" content="CS">
<meta property="article:tag" content="CVPR 2018">
<meta property="article:tag" content="paper">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.sunlingzhang.com/admin_head.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Path Aggregation Network for Instance Segmentation",
  "url": "http://blog.sunlingzhang.com/2024/07/29/Paper/Path%20Aggregation%20Network%20for%20Instance%20Segmentation/",
  "image": "http://blog.sunlingzhang.com/admin_head.jpg",
  "datePublished": "2024-07-29T03:49:00.000Z",
  "dateModified": "2026-02-12T10:34:06.980Z",
  "author": [
    {
      "@type": "Person",
      "name": "神火不知灭",
      "url": "http://blog.sunlingzhang.com"
    }
  ]
}</script><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://blog.sunlingzhang.com/2024/07/29/Paper/Path%20Aggregation%20Network%20for%20Instance%20Segmentation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Path Aggregation Network for Instance Segmentation',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/admin_head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">64</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">64</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/%E5%85%A5%E9%97%A8%E5%AE%9E%E8%B7%B5/"><i class="fa-fw fas fa-keyboard"></i><span> 入门实践</span></a></div><div class="menus_item"><a class="site-page" href="/tags/%E9%9A%8F%E7%AC%94/"><i class="fa-fw fas fa-edit"></i><span> 随笔</span></a></div><div class="menus_item"><a class="site-page" href="/tags/paper/"><i class="fa-fw fal fa-paperclip"></i><span> 论文解读</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 计算机</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/algorithm/"><i class="fa-fw fas fa-code"></i><span> 算法</span></a></li><li><a class="site-page child" href="/data-structure/"><i class="fa-fw fas fa-terminal"></i><span> 数据结构</span></a></li><li><a class="site-page child" href="/operation-system/"><i class="fa-fw fas fa-desktop"></i><span> 操作系统</span></a></li><li><a class="site-page child" href="/computer-composition/"><i class="fa-fw fas fa-microchip"></i><span> 计算机组成原理</span></a></li><li><a class="site-page child" href="/network/"><i class="fa-fw fas fa-network-wired"></i><span> 计算机网络</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 页面</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 编程语言</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tags/Java/"><i class="fa-fw fas fa-music"></i><span> Java</span></a></li><li><a class="site-page child" href="/tags/Python/"><i class="fa-fw fas fa-video"></i><span> Python</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">遗世独立</span></a><a class="nav-page-title" href="/"><span class="site-name">Path Aggregation Network for Instance Segmentation</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/%E5%85%A5%E9%97%A8%E5%AE%9E%E8%B7%B5/"><i class="fa-fw fas fa-keyboard"></i><span> 入门实践</span></a></div><div class="menus_item"><a class="site-page" href="/tags/%E9%9A%8F%E7%AC%94/"><i class="fa-fw fas fa-edit"></i><span> 随笔</span></a></div><div class="menus_item"><a class="site-page" href="/tags/paper/"><i class="fa-fw fal fa-paperclip"></i><span> 论文解读</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 计算机</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/algorithm/"><i class="fa-fw fas fa-code"></i><span> 算法</span></a></li><li><a class="site-page child" href="/data-structure/"><i class="fa-fw fas fa-terminal"></i><span> 数据结构</span></a></li><li><a class="site-page child" href="/operation-system/"><i class="fa-fw fas fa-desktop"></i><span> 操作系统</span></a></li><li><a class="site-page child" href="/computer-composition/"><i class="fa-fw fas fa-microchip"></i><span> 计算机组成原理</span></a></li><li><a class="site-page child" href="/network/"><i class="fa-fw fas fa-network-wired"></i><span> 计算机网络</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-book"></i><span> 页面</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 编程语言</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tags/Java/"><i class="fa-fw fas fa-music"></i><span> Java</span></a></li><li><a class="site-page child" href="/tags/Python/"><i class="fa-fw fas fa-video"></i><span> Python</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Path Aggregation Network for Instance Segmentation</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-07-29T03:49:00.000Z" title="Created 2024-07-29 11:49:00">2024-07-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-12T10:34:06.980Z" title="Updated 2026-02-12 18:34:06">2026-02-12</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h2 id="用于实例分割的路径聚合网络">用于实例分割的路径聚合网络</h2>
<p>会议: CVPR 2018</p>
<p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.01534">https://arxiv.org/abs/1803.01534</a></p>
<p>github: <a target="_blank" rel="noopener" href="https://github.com/ShuLiu1993/PANet">https://github.com/ShuLiu1993/PANet</a></p>
<p>[TOC]</p>
<h2 id="摘要">摘要</h2>
<p>信息在神经网络中的传播方式至关重要。本文提出了路径聚合网络 (PANet)，旨在提升基于候选框的实例分割框架中的信息流。具体来说，我们通过自底向上的路径增强，将低层中的精确定位信号引入整个特征层次，从而缩短了低层和顶层特征之间的信息路径。我们提出了自适应特征池化，将特征网格和所有特征级别连接起来，使每个特征级别中的有用信息可以直接传播到后续的候选框子网络中。我们创建了一个互补分支，用于捕获每个候选框的不同视角，从而进一步提高掩码预测的精度。这些改进易于实现，且计算开销微小。我们的 PANet 在 COCO 2017 挑战赛的实例分割任务中取得了第一名，并在没有使用大批次训练的情况下，在目标检测任务中取得了第二名。它也是 MVD 和 Cityscapes 上的最先进技术。代码地址: <a target="_blank" rel="noopener" href="https://github.com/ShuLiu1993/PANet">https://github.com/ShuLiu1993/PANet</a></p>
<span id="more"></span>
<h2 id="1-引言">1. 引言</h2>
<p>实例分割是最重要和最具挑战性的任务之一。它旨在预测类别标签和像素级的实例掩码，以定位图像中出现的不同数量的实例。这项任务广泛应用于自动驾驶、机器人、视频监控等领域。</p>
<p>在深度卷积神经网络的帮助下，已经提出了几种实例分割框架，例如 [21, 33, 3, 38]，其性能迅速提升 [12]。Mask R-CNN [21] 是一种简单而有效的实例分割系统。基于 Fast/Faster R-CNN [16, 51]，使用全卷积网络 (FCN) 进行掩码预测，并辅以框回归和分类。为了获得高性能，FPN [35] 被用于提取网络内部的层次特征，其中添加了一个自顶向下的路径和横向连接，以传播语义信息强的特征。</p>
<p>最近发布的几个数据集 [37, 7, 45] 为算法改进提供了很大的空间。COCO [37] 包含 20 万张图像。每张图像中都有大量具有复杂空间布局的实例。Cityscapes [7] 和 MVD [45] 提供了包含大量交通参与者的街景，每张图像中都有模糊、遮挡严重和极其微小的实例。</p>
<p>在图像分类中设计网络的一些原则也被证明对目标识别有效。例如，通过直接的残差连接 [23, 24] 和密集连接 [26] 缩短信息路径和简化信息传播，以及通过创建遵循“分割-变换-合并”策略的并行路径来增加信息路径的灵活性和多样性 [61, 6]，都是有益的。</p>
<p><strong>本文发现</strong></p>
<p>我们的研究表明，最先进的 Mask R-CNN 中的信息传播可以进一步改进。具体来说，低层特征对于大型实例识别很有帮助。但从低层结构到顶层特征的路径很长，增加了获取精确定位信息的难度。此外，每个候选框都是基于从单个特征级别池化的特征网格进行预测的，这个级别是启发式地分配的。由于在其他级别中丢弃的信息可能对最终预测有帮助，因此这个过程可以进行优化。最后，掩码预测是在单个视角上进行的，失去了收集更多多样化信息的机会。</p>
<p><strong>本文贡献</strong></p>
<p>受这些原则和观察结果的启发，我们提出了 PANet，如图 1 所示，用于实例分割。</p>
<p><img src="/img/panet/image-20240729151551834.png" alt="image-20240729151551834"></p>
<p><code>图1. 展示了网络框架插图。(a) FPN骨干网。(b)自下而上的路径聚合。(c)自适应特征池化。(d)检测框分支。(e)全连接融合。注意图中为了简洁起见，省略了(a)和(b)中特征映射的通道维度。</code></p>
<p>首先，为了缩短信息路径并使用低层中存在的精确定位信号增强特征金字塔，我们创建了自底向上的路径增强。实际上，低层特征已经在 [44, 42, 13, 46, 35, 5, 31, 14] 系统中使用。但将低层特征传播以增强整个特征层次以进行实例识别尚未被探索。</p>
<p>其次，为了恢复每个候选框与所有特征级别之间中断的信息路径，我们开发了自适应特征池化。这是一个简单的组件，用于为每个候选框聚合来自所有特征级别的特征，避免任意分配的结果。通过此操作，与 [4, 62] 中的路径相比，创建了更直接的路径。</p>
<p>最后，为了捕获每个候选框的不同视角，我们通过添加小型全连接 (fc) 层来增强掩码预测，这些层具有与 Mask R-CNN 原本使用的 FCN 互补的特性。通过融合来自这两个视角的预测，增加了信息多样性，并产生了质量更好的掩码。</p>
<p>前两个组件由对象检测和实例分割共享，从而显著提高了这两个任务的表现。</p>
<p><strong>实验结果</strong></p>
<p>使用 PANet，我们在几个数据集上取得了最先进的性能。以 ResNet-50 [23] 作为初始网络，我们的 PANet 在单个尺度上测试的性能已经超过了 COCO 2016 挑战赛中对象检测 [27] 和实例分割 [33] 任务的冠军。请注意，这些先前结果是通过更大型的模型 [23, 58] 以及多尺度测试和水平翻转测试获得的。</p>
<p>我们在没有使用大批次训练的情况下，在 COCO 2017 挑战赛的实例分割任务中取得了第一名，并在目标检测任务中取得了第二名。我们还将在 Cityscapes 和 MVD 上对系统进行基准测试，这同样会产生排名靠前的结果，表明我们的 PANet 是一个非常实用且性能优异的框架。</p>
<h2 id="2-相关工作">2. 相关工作</h2>
<p><strong>实例分割</strong></p>
<p>实例分割主要有两种方法。最流行的是基于候选框的方法。这种方法与目标检测有很强的联系。在 R-CNN [17] 中，来自 [60, 68] 的对象候选框被输入到网络中，以提取用于分类的特征。而 Fast/Faster R-CNN [16, 51] 和 SPPNet [22] 通过从全局特征图中池化特征来加速过程。早期的工作 [18, 19] 从 MCG [1] 中提取掩码候选框作为输入，以提取特征，而 CFM [9]、MNC [10] 和 Hayder 等人 [20] 将特征池化与网络合并，以提高速度。更新的设计是在网络中生成掩码作为候选框 [48, 49, 8] 或最终结果 [10, 34, 41]。Mask R-CNN [21] 是一个属于该类别的有效框架。我们的工作基于 Mask R-CNN，并从不同的方面对其进行了改进。</p>
<p>另一类方法主要是基于分割的。它们学习了专门设计的变换 [3, 33, 38, 59] 或实例边界 [30]。然后从预测的变换中解码实例掩码。</p>
<p><strong>多级特征</strong></p>
<p>在图像识别中使用了来自不同层的特征。SharpMask [49]、Peng 等人 [47] 和 LRR [14] 通过融合特征图来进行分割，以获得更精细的细节。FCN [44]、U-Net [54] 和 Noh 等人 [46] 通过跳跃连接将来自较低层的信息融合在一起。TDM [56] 和 FPN [35] 通过横向连接增强自顶向下的路径，用于目标检测。与 TDM 不同，它将具有最高分辨率的融合特征图用于池化特征，SSD [42]、DSSD [13]、MS-CNN [5] 和 FPN [35] 将候选框分配到适当特征级别进行推理。我们将 FPN 作为基线，并对其进行了大幅改进。</p>
<p>ION [4]、Zagoruyko 等人 [62]、Hypernet [31] 和 Hypercolumn [19] 通过连接来自不同层的特征网格来进行更好的预测。但需要进行一系列操作，即归一化、连接和降维，以获得可行的新特征。相比之下，我们的设计要简单得多。</p>
<p><strong>更大的上下文区域</strong></p>
<p>[15, 64, 62] 中的方法使用类似于中间凹的结构为每个候选框池化特征，以利用具有不同分辨率的区域中的上下文信息。来自更大区域的特征提供了周围的环境信息。PSPNet [67] 和 ParseNet [43] 中使用了全局池化，极大地提高了语义分割的质量。Peng 等人 [47] 观察到类似的趋势，其中使用了全局卷积。我们的掩码预测分支也支持访问全局信息，但技术完全不同。</p>
<h2 id="3-模型框架">3 模型框架</h2>
<p>我们的框架如图 1 所示。路径增强和聚合是为了提高性能。创建了一个自底向上的路径，以使低层信息更容易传播。我们设计了自适应特征池化，以允许每个候选框访问来自所有级别的信息进行预测。一个互补路径被添加到掩码预测分支中。这个新的结构导致了良好的性能。与 FPN 类似，这种改进与 CNN 结构无关，例如 [57, 32, 23]。</p>
<h3 id="3-1-自底向上的路径增强">3.1. 自底向上的路径增强</h3>
<p><strong>动机</strong></p>
<p>论文[63] 中的深刻见解表明，高层神经元对整个目标有强烈的响应，而底层神经元更可能被局部纹理和图案激活，这表明了增强自顶向下路径以传播语义信息强的特征并增强所有特征以具有合理的分类能力的必要性。我们的框架通过基于高边缘或实例部分响应是对准确定位实例的强指标的事实，进一步增强了整个特征层次的空间定位能力。为此，我们构建了一条从低层到顶层的干净（指不经过变换直接连接）横向连接路径。因此，在这些层之间有一个“捷径”(图 1 中的虚线绿色线)，它由不到 10 层组成，横跨这些层。相比之下，FPN 中的 CNN 主干提供了一个从低层到最顶层的长路径(图 1 中的虚线红线)，它甚至经过 100 多层。</p>
<p><strong>增强的自底向上结构</strong></p>
<p>我们的框架首先完成自底向上的路径增强。我们遵循 FPN 来定义具有相同空间大小的特征图的层位于相同的网络阶段。每个特征级别对应一个阶段。我们还采用 ResNet [23] 作为基本结构，并使用 {P2, P3, P4, P5} 来表示 FPN 生成的特征级别。我们的增强路径从最低级别 P2 开始，并逐渐接近 P5，如图 1(b) 所示。</p>
<p>从 P2 到 P5，空间大小逐渐以 2 为因子下采样。我们使用 {N2, N3, N4, N5} 来表示新产生的特征图，对应于 {P2, P3, P4, P5}。请注意，N2 就是 P2，没有经过任何处理。</p>
<p>如图 2 所示，每个构建块从更高分辨率的特征图 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">N_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和更粗的特征图 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">P_{i+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 通过横向连接接收，并生成新的特征图 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">N_{i+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>。每个特征图 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">N_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 首先通过一个步长为 2 的 3x3 卷积层来减少空间大小。然后，特征图 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">P_{i+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span> 的每个元素和下采样的图通过横向连接相加。然后，融合后的特征图经过另一个 3x3 卷积层来生成 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">N_{i+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>，以供后续子网络使用。这是一个迭代过程，并在接近 P5 时终止。</p>
<p><img src="/img/panet/image-20240729155815876.png" alt="image-20240729155815876"></p>
<p><code>图2. 自底向上路径扩展的构建块示意图。</code></p>
<p>在这些构建块中，我们始终使用特征图的通道 256。所有卷积层后面都跟着一个 ReLU [32]。然后，每个候选框的特征网格从新的特征图中池化，即 {N2, N3, N4, N5}。</p>
<h3 id="3-2-自适应特征池化">3.2. 自适应特征池化</h3>
<p><strong>动机</strong></p>
<p>在 FPN [35] 中，根据候选框的大小将候选框分配到不同的特征级别。这使得小候选框被分配到低特征级别，而大候选框被分配到高特征级别。虽然简单有效，但它仍然可能产生非最优结果。例如，两个具有 10 像素差异的候选框可以被分配到不同的级别。实际上，这两个候选框相当相似。此外，特征的重要性可能与其所属的级别没有很强的相关性。高层特征是由具有大感受野生成的，并捕获更丰富的上下文信息。允许小候选框访问这些特征可以更好地利用有用的上下文信息进行预测。同样，低层特征包含许多精细的细节和高定位精度。让大候选框访问它们显然是有益的。基于这些想法，我们提出了为每个候选框从所有级别池化特征并进行融合，以供后续预测。我们将这个过程称为自适应特征池化。</p>
<p>我们现在分析自适应特征池化中来自不同级别的特征池化的比例。我们使用最大操作来融合来自不同级别的特征，这允许网络选择逐元素的有用信息。我们根据候选框在 FPN 中被分配到的级别将候选框聚类为四类。对于每组候选框，我们计算从不同级别选择的特征的比例。在符号表示中，1-4 级别代表从低到高的级别。如图 3 所示，蓝色线代表最初在 FPN 中被分配到第 1 级别的小候选框。令人惊讶的是，近 70% 的特征来自其他更高的级别。我们还使用黄色线来表示最初在 FPN 中被分配到第 4 级别的大候选框。同样，50%+ 的特征是从其他较低的级别池化的。这个观察清楚地表明，多个级别的特征共同有助于准确的预测。这也是设计自底向上路径增强的强有力支持。</p>
<p><img src="/img/panet/image-20240729160144008.png" alt="image-20240729160144008"></p>
<p><code>图3. 自适应特征池化从不同特征层池化的特征比例。每条线代表一组应分配到FPN中相同特征级别的提案，即具有相似规模的提案。横轴表示汇集特征的来源。结果表明，不同规模的提案都利用了几个不同层次的特征。</code></p>
<p><strong>自适应特征池化结构</strong></p>
<p>自适应特征池化实际上在实现上很简单，如图 1© 所示。首先，对于每个候选框，我们将它们映射到不同的特征级别，如图 1(b) 中的深灰色区域所示。遵循 Mask R-CNN [21]，ROIAlign 被用于从每个级别池化特征网格。然后，使用元素级最大值或求和的融合操作被用于融合来自不同级别的特征网格。</p>
<p>在后续子网络中，池化的特征网格独立地通过一个参数层，该层后面跟着融合操作，以允许网络适应特征。例如，FPN 的框分支中有两个 fc 层。我们在第一层之后应用融合操作。由于 Mask R-CNN 中的掩码预测分支使用了四个连续的卷积层，我们将融合操作放置在第一个和第二个卷积层之间。关于自适应特征池化在框分支上的详细说明，请参见附录中的图 6。</p>
<p>我们的设计侧重于融合网络内部特征层次的信息，而不是来自输入图像金字塔中不同特征图的 [52]。与 [4, 62, 31] 中的过程相比，它更简单，在这些过程中需要 L-2 归一化、连接和降维。</p>
<h3 id="3-3-全连接融合">3.3. 全连接融合</h3>
<p><strong>动机</strong></p>
<p>全连接层(MLP) ，在实例分割 [10, 41, 34] 中的掩码预测和掩码候选框生成 [48, 49] 中被广泛使用。结果 [8, 33] 表明 FCN 也能够预测实例的像素级掩码。最近，Mask R-CNN [21] 在池化的特征网格上应用了一个小型 FCN 来预测相应的掩码，从而避免了类别之间的竞争。我们注意到 fc 层与 FCN 相比会产生不同的特性，其中后者根据局部感受野和不同空间位置的共享参数对每个像素进行预测。相反，fc 层是位置敏感的，因为不同空间位置的预测是通过不同的参数集实现的。因此，它们具有适应不同空间位置的能力。此外，每个空间位置的预测都是基于整个候选框的全局信息实现的。这有助于区分实例 [48] 并识别属于同一对象的独立部分。鉴于 fc 和卷积层之间不同的特性，我们将来自这两种类型层的预测进行融合，以获得更好的掩码预测。</p>
<p><strong>掩码预测结构</strong></p>
<p>我们的掩码预测组件轻量级且易于实现。掩码分支对每个候选框的池化特征网格进行操作。如图 4 所示，主路径是一个小型 FCN，由 4 个连续的卷积层和 1 个反转卷积层组成。每个卷积层由 256 个 3x3 滤波器组成，反转卷积层将特征上采样因子为 2。</p>
<p><img src="/img/panet/image-20240729160326105.png" alt="image-20240729160326105"></p>
<p><code>图4. 掩码预测分支与全连接融合。</code></p>
<p>它为每个类别独立地预测二值像素级掩码，以解耦分割和分类，类似于 Mask R-CNN。我们进一步创建了一条从 conv3 层到 fc 层的短路径。有两个 3x3 卷积层，其中第二个卷积层将通道数量减半，以减少计算开销。</p>
<p>使用 fc 层预测类别无关的前景/背景掩码。它不仅效率高，而且允许 fc 层中的参数用更多样本来训练，从而获得更好的泛化能力。我们使用的掩码大小是 28x28，因此 fc 层产生一个 784x1x1 的向量。这个向量被重塑为与 FCN 预测的掩码相同的空间大小。为了获得最终的掩码预测，将 FCN 的每个类别的掩码和 fc 的前景/背景预测相加。使用一个 fc 层而不是多个 fc 层来进行最终预测，可以防止将隐藏的空间特征图坍缩成一个短的特征向量，从而丢失空间信息。</p>
<h2 id="4-实验">4. 实验</h2>
<p>我们在具有挑战性的 COCO [37]、Cityscapes [7] 和 MVD [45] 数据集上将我们的方法与最先进的技术进行了比较。我们在所有这些数据集上都取得了排名靠前的结果。我们在 COCO 数据集上进行了全面的消融研究。我们还展示了我们在 COCO 2017 实例分割和目标检测挑战赛中的结果。</p>
<h3 id="4-1-实现细节">4.1. 实现细节</h3>
<p>我们基于 Caffe [29] 重新实现了 Mask R-CNN 和 FPN。我们在实验中使用的所有预训练模型都是公开可用的。我们采用以图像为中心的训练 [16]。对于每张图像，我们采样 512 个感兴趣区域 (ROIs)，正负比例为 1:3。权重衰减为 0.0001，动量为 0.9。其他超参数略有不同，具体取决于数据集，我们在各自的实验中详细说明了它们。遵循 Mask R-CNN，候选框来自一个独立训练的 RPN [35, 51]，以便于消融和公平比较，即主干不与目标检测/实例分割共享。</p>
<h3 id="4-2-在-COCO-上的实验">4.2. 在 COCO 上的实验</h3>
<p><strong>数据集和指标</strong></p>
<p>COCO [37] 数据集是实例分割和目标检测最具挑战性的数据集之一，因为数据复杂。它包含 11.5 万张训练图像和 5 千张验证图像(2017 年的新分割)。2 万张图像用于测试开发，2 万张图像用作测试挑战。测试挑战和测试开发的真实标签没有公开。有 80 个类别，带有像素级实例掩码注释。我们在 train-2017 子集上训练我们的模型，并在 val-2017 子集上报告消融研究的结果。我们还报告了测试开发集上的结果以进行比较。</p>
<p>我们遵循标准的评估指标，即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>P</mi><mtext>、</mtext><mi>A</mi><msub><mi>P</mi><mn>50</mn></msub><mtext>、</mtext><mi>A</mi><msub><mi>P</mi><mn>75</mn></msub><mtext>、</mtext><mi>A</mi><msub><mi>P</mi><mi>S</mi></msub><mtext>、</mtext><mi>A</mi><msub><mi>P</mi><mi>M</mi></msub><mtext>和</mtext><mi>A</mi><msub><mi>P</mi><mi>L</mi></msub></mrow><annotation encoding="application/x-tex">AP、AP_{50}、AP_{75}、AP_S、AP_M和 AP_L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord cjk_fallback">、</span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">50</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">75</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">和</span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 。最后三个指标衡量不同尺度对象的表现。由于我们的框架对实例分割和目标检测都适用，我们还训练了独立的目标检测器。我们报告了独立训练的目标检测器的掩码 AP 和框 ap <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><msup><mi>P</mi><mrow><mi>b</mi><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">AP^{bb}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">bb</span></span></span></span></span></span></span></span></span></span></span></span>，以及多任务方式训练的框分支的 box ap <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><msup><mi>P</mi><mrow><mi>b</mi><mi>b</mi><mi>M</mi></mrow></msup></mrow><annotation encoding="application/x-tex">AP^{bbM}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">bb</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span></span></span></span></span></span></span></span></span>。</p>
<p><strong>超参数</strong></p>
<p>我们在一个图像批次中使用 16 张图像进行训练。如果未特别说明，则图像的短边和长边分别为 800 和 1000。对于实例分割，我们使用学习率 0.02 训练模型 120k 次迭代，并使用学习率 0.002 训练 40k 次迭代。对于目标检测，我们训练了一个没有掩码预测分支的目标检测器。目标检测器以学习率 0.02 训练 60k 次迭代，并以学习率 0.002 训练 20k 次迭代。</p>
<p>这些参数是从 Mask R-CNN 和 FPN 中采用的，没有进行任何微调。</p>
<p><strong>实例分割结果</strong></p>
<p>我们报告了我们的 PANet 在测试开发集上的性能以进行比较，包括多尺度训练和不进行多尺度训练。如表 1 所示，我们的 PANet 使用在多尺度图像上训练并在单尺度图像上测试的 ResNet-50，已经超过了 Mask R-CNN 和 2016 年的冠军，其中后者使用了更大的模型集成和测试技巧 [23, 33, 10, 15, 39, 62]。在 800 的图像尺度上训练和测试，我们的方法在相同的初始模型下比单模型最先进的 Mask R-CNN 的性能提高了近 3 个百分点。</p>
<p><img src="/img/panet/image-20240729161430557.png" alt="image-20240729161430557"></p>
<p><code>表1：在COCO测试-验证子集中，基于PANet（COCO 2016实例分割挑战赛的获胜者）和Mask R-CNN（基准模型）的Mask AP的比较。</code></p>
<p><strong>目标检测结果</strong></p>
<p>与 Mask R-CNN 采用的方式类似，我们还报告了从框分支推断的边界框结果。表 2 显示，我们的方法使用 ResNet-50，在单尺度图像上进行训练和测试，以大幅优势超过了所有其他单模型，即使使用了更大的 ResNeXt-101 [61] 作为初始模型。使用多尺度训练和单尺度测试，我们的 PANet 使用 ResNet-50 超过了 2016 年的冠军，后者使用了更大的模型集成和测试技巧。</p>
<p><img src="/img/panet/image-20240729161536835.png" alt="image-20240729161536835"></p>
<p><code>表2. COCO 2016目标检测挑战赛的获胜者PANet、RetinaNet和Mask R-CNN在COCO测试开发子集上的box AP比较，其中后三个为基线。</code></p>
<p><strong>组件消融研究</strong></p>
<p>首先，我们分析了我们提出的每个组件的重要性。除了自底向上路径增强、自适应特征池化和全连接融合之外，我们还分析了多尺度训练、多 GPU 同步批归一化和更重的头部。对于多尺度训练，我们将长边设置为 1,400，而另一个则从 400 到 1,400。我们在一个批次中跨所有 GPU 的所有样本上计算均值和方差，在训练过程中不固定任何参数，并且当使用多 GPU 同步批归一化时，使所有新层后面跟着一个批归一化层。更重的头部使用 4 个连续的 3x3 卷积层，由框分类和框回归共享，而不是两个 fc 层。这与 [36] 中使用的头部类似，但在他们的情况下，框分类和框回归分支的卷积层不共享。</p>
<p>我们在 val-2017 子集上从基线逐渐添加所有组件的消融研究，结果如表 3 所示。ResNet-50 [23] 是我们的初始模型。我们报告了掩码 AP、独立训练的目标检测器的框 ap APbb 和多任务方式训练的框分支的 box ap APbbM 的性能。</p>
<ol>
<li>重新实现的基线。我们重新实现的 Mask R-CNN 的性能与原始论文中描述的性能相当，我们的目标检测器表现更好。</li>
<li>多尺度训练和多 GPU 同步批归一化。这两种技术有助于网络更好地收敛并提高泛化能力。</li>
<li>自底向上路径增强。无论是否有自适应特征池化，自底向上路径增强始终将掩码 AP 和框 ap APbb 分别提高 0.6 和 0.9 以上。对大尺度实例的改进最为显着。这证实了来自较低特征层次的信息的有用性。</li>
<li>自适应特征池化。无论是否有自底向上路径增强，自适应特征池化始终提高性能。所有尺度的性能通常都会提高，这与我们的观察结果一致，即其他层的特征在最终预测中也有用。</li>
<li>全连接融合。全连接融合旨在预测质量更好的掩码。它在掩码 AP 方面产生了 0.7 的改进。它对所有尺度的实例都通用。</li>
<li>更重的头部。更重的头部对多任务方式训练的边界框的 box ap APbbM 非常有效。而对于掩码 AP 和独立训练的目标检测器，改进很小。</li>
</ol>
<p><img src="/img/panet/image-20240729161823917.png" alt="image-20240729161823917"></p>
<p>Table3. 在val-2017上独立训练的目标检测器的mask AP、box AP <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><msup><mi>P</mi><mrow><mi>b</mi><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">AP^{bb}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">bb</span></span></span></span></span></span></span></span></span></span></span></span>和多任务方式训练的box分支的box AP <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><msup><mi>P</mi><mrow><mi>b</mi><mi>b</mi><mi>M</mi></mrow></msup></mrow><annotation encoding="application/x-tex">AP^{bbM}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">bb</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span></span></span></span></span></span></span></span></span>的性能指标。基于我们重新实现的基线(RBL)，我们逐渐增加了多尺度训练(MST)、多gpu同步批归一化(MBN)、自下而上路径增强(BPA)、自适应特征池化(AFP)、全连接融合(FF)和重头(HHD)的研究。MRB是原论文中报道的Mask R-CNN结果的简称。最后一行显示了与基线RBL相比的总体改善。</p>
<p>PANet 中包含所有这些组件，掩码 AP 比 baselines 提高了 4.4。独立训练的目标检测器的 box AP <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><msup><mi>P</mi><mrow><mi>b</mi><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">AP^{bb}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">bb</span></span></span></span></span></span></span></span></span></span></span></span>增加了 4.2。它们都是显着的。小尺寸和中尺寸实例的贡献最大。一半的改进来自多尺度训练和多 GPU 同步批归一化，这些是帮助训练更好模型的策略。</p>
<p><strong>自适应特征池化消融研究</strong></p>
<p>我们对自适应特征池化进行了消融研究，以找出在哪里放置融合操作以及最合适的融合操作。我们将它放置在 ROIAlign 和 fc1 之间，用“fu.fc1fc2”表示，或者放置在 fc1 和 fc2 之间，用“fc1fu.fc2”表示，如表 4 所示。类似的设置也应用于掩码预测分支。对于特征融合，我们测试了最大值和求和操作。</p>
<p><img src="/img/panet/image-20240729162708703.png" alt="image-20240729162708703"></p>
<p><code>表4. 基于独立训练目标检测器掩模AP和Box AP的val-2017自适应特征池化消融研究</code></p>
<p>如表 4 所示，自适应特征池化对融合操作不敏感。然而，允许一个参数层来适应来自不同级别的特征网格是非常重要的。我们在框架中使用最大值作为融合操作，并将其放在第一个参数层之后。</p>
<p><strong>全连接融合消融研究</strong></p>
<p>我们研究了以不同的方式实例化增强的 fc 分支的性能。我们考虑了两个方面，即开始新分支的层和融合来自新分支和 FCN 的预测的方法。我们分别从 conv2、conv3 和 conv4 创建新路径。“max”、“sum”和“product”操作用于融合。我们将我们重新实现的 Mask R-CNN，包括自底向上路径增强和自适应特征池化作为基线。相应的结果如表 5 所示。它们清楚地表明，从 conv3 开始并使用求和进行融合会产生最佳结果。</p>
<p><img src="/img/panet/image-20240729162733861.png" alt="image-20240729162733861"></p>
<p><code>表5所示。基于Mask AP的val-2017全连接融合消融研究。</code></p>
<p><strong>COCO 2017 挑战赛</strong></p>
<p>使用 PANet，我们参加了 COCO 2017 实例分割和目标检测挑战赛。我们的框架在实例分割任务中取得了第一名，在目标检测任务中取得了第二名，而没有使用大批次训练。如图 1 和表 6 和表 7 所示，与去年的冠军相比，我们在实例分割方面取得了 9.1% 的绝对和 24% 的相对改进。而目标检测方面，则取得了 9.4% 的绝对和 23% 的相对改进。</p>
<p><img src="/img/panet/image-20240729162909706.png" alt="image-20240729162909706"></p>
<p><code>表6所示。测试开发中不同年份COCO实例分割挑战的Mask AP。</code></p>
<p><img src="/img/panet/image-20240729162934233.png" alt="image-20240729162934233"></p>
<p><code>表7. COCO目标检测挑战赛不同年份测试开发的方框AP。</code></p>
<p>最先进的性能来自于 PANet 中的几个细节。首先，我们使用了可变形卷积，其中采用了 DCN [11]。常规的测试技巧 [23, 33, 10, 15, 39, 62]，例如多尺度测试、水平翻转测试、掩码投票和框投票，都被采用。对于多尺度测试，我们将长边设置为 1,400，而另一个则从 600 到 1,200，步长为 200。仅使用 4 个尺度。其次，我们使用了更大型的初始模型，这些模型是公开可用的。我们使用 3 个 ResNeXt-101 (64x4d) [61]、2 个 SE-ResNeXt-101 (32x4d) [25]、1 个 ResNet-269 [64] 和 1 个 SENet [25] 作为边界框和掩码生成的集成。使用不同的大型初始模型的性能相似。一个 ResNeXt-101 (64x4d) 被用作生成候选框的基本模型。我们使用不同的随机种子和平衡采样 [55] 来增强模型之间的多样性来训练这些模型。我们提交的检测结果是通过收紧实例掩码获得的。我们在图 5 中展示了几个视觉结果——我们的大多数预测都具有很高的质量。</p>
<p><img src="/img/panet/image-20240729160459095.png" alt="image-20240729160459095"></p>
<p><code>图5. 每行图像分别是本文的模型在COCO test-dev, cityscape test和MVD test上的可视化结果。</code></p>
<h3 id="4-3-在-Cityscapes-上的实验">4.3. 在 Cityscapes 上的实验</h3>
<p><strong>数据集和指标</strong></p>
<p>Cityscapes [7] 包含由车载摄像头拍摄的街景。有 2,975 张训练图像、500 张验证图像和 1,525 张测试图像，具有精细注释。另外 20k 张图像具有粗略注释，不包括在训练中。我们在 val 和 secret 测试子集上报告我们的结果。8 个语义类别带有实例掩码注释。每张图像的大小为 1024x2048。我们根据 AP 和 AP50 评估结果。</p>
<p><strong>超参数</strong></p>
<p>我们使用与 Mask R-CNN [21] 相同的超参数集进行公平比较。具体来说，我们在训练时随机从 {800, 1024} 中选择短边进行训练，并在推理时使用短边为 1024 的图像。没有使用测试技巧或 DCN。我们以学习率 0.01 训练模型 18k 次迭代，并以学习率 0.001 训练 6k 次迭代。每个图像批次中有 8 张图像(每个 GPU 1 张图像)。</p>
<p>ResNet-50 是此数据集上的初始模型。</p>
<p><strong>方法和结果</strong></p>
<p>我们在测试子集上与最先进的技术进行了比较，结果如表 8 所示。在“fine-only”数据上训练，我们的方法比使用“fine-only”数据的 Mask R-CNN 的性能提高了 5.6 个百分点。它甚至可以与在 COCO 上预训练的 Mask R-CNN 相媲美。通过在 COCO 上进行预训练，我们比使用相同设置的 Mask R-CNN 的性能提高了 4.4 个百分点。我们在图 5 中展示了视觉结果。</p>
<p><img src="/img/panet/image-20240729160910818.png" alt="image-20240729160910818"></p>
<p><code>表8. 结果在cityscape val子集上，记为AP [val]，在cityscape测试子集上，记为AP。</code></p>
<p>我们对 val 子集上的改进进行的消融研究如表 9 所示。基于我们重新实现的基线，我们添加了多 GPU 同步批归一化，以帮助网络更好地收敛。它将精度提高了 1.5 个百分点。使用我们的完整 PANet，性能又提高了 1.9 个百分点。</p>
<p><img src="/img/panet/image-20240729160645261.png" alt="image-20240729160645261"></p>
<p><code>表9. 城市景观val子集消融研究结果。只有精细的注释用于训练。MBN是多gpu同步批处理规范化的缩写。</code></p>
<h3 id="4-4-在-MVD-上的实验">4.4. 在 MVD 上的实验</h3>
<p>MVD [45] 是一个相对较新且规模较大的实例分割数据集。它提供了 25,000 张街景图像，带有 37 个语义类别的精细实例级注释。它们是从几个国家使用不同的设备捕获的。内容和分辨率差异很大。我们在训练子集上使用 ResNet-50 作为初始模型训练我们的模型，并报告了在 val 和 secret 测试子集上根据 AP 和 AP50 的性能。</p>
<p>我们在表 10 中展示了我们的结果。与 LSUN 2017 实例分割挑战赛中的冠军 UCenter [40] 相比，我们的 PANet 使用一个在单尺度图像上测试的 ResNet-50 已经与在 COCO 上进行预训练的集成结果相当。通过使用 UCenter 也采用的多种尺度测试和水平翻转测试，我们的方法表现得甚至更好。定性的结果在图 5 中展示。</p>
<p><img src="/img/panet/image-20240729160743160.png" alt="image-20240729160743160"></p>
<p><code>表10. MVD值子集和测试子集的结果。</code></p>
<h2 id="5-结论">5. 结论</h2>
<p>我们提出了用于实例分割的 PANet。我们设计了一些简单而有效的组件，以增强代表性管道中的信息传播。我们从所有特征级别池化特征，并缩短了低层和顶层特征之间的距离，以进行可靠的信息传递。互补路径被增强以丰富每个候选框的特征。产生了令人印象深刻的结果。我们的未来工作是将我们的方法扩展到视频和 RGBD 数据。</p>
<h2 id="附录">附录</h2>
<h3 id="A-Cityscapes-和-MVD-的训练细节和生成锚点的策略">A. Cityscapes 和 MVD 的训练细节和生成锚点的策略</h3>
<p>在 Cityscapes [7] 上，我们采用了 Mask R-CNN [21] 中的训练超参数，并已在第 4.3 节中描述。RPN 锚点跨越 5 个尺度和 3 个宽高比，遵循 [21, 35]。而在 MVD [45] 上，我们采用了获胜者 [40] 中的训练超参数。我们以学习率 0.02 训练模型 60k 次迭代，并以学习率 0.002 训练 20k 次迭代。我们使用 16 张图像进行训练。我们设置输入图像的长边为 2400 像素，而另一个则从 600 到 2000 像素进行多尺度训练。我们采用了 {1600, 1800, 2000} 尺度进行多尺度测试。RPN 锚点跨越 7 个尺度，即 {82, 162, 322, 642, 1282, 2562, 5122}，和 5 个宽高比，即 {0.2, 0.5, 1, 2, 5}。RPN 使用与目标检测/实例分割网络训练相同的尺度进行训练。</p>
<h3 id="B-实现-多-GPU-同步批归一化的细节">B. 实现 多 GPU 同步批归一化的细节</h3>
<p>我们在 Caffe [29] 和 OpenMPI 上实现了多 GPU 批归一化。给定 n 个 GPU 和训练批次中的样本 B，我们首先将训练样本均匀地分割成 n 个子批次，每个子批次用 bi 表示，分配给一个 GPU。在每个 GPU 上，我们根据 bi 中的样本计算均值 µi。然后对所有 GPU 应用 AllReduce 操作来收集所有 µi，以获得整个批次 B 的均值 µB。µB 被广播到所有 GPU。然后我们独立地在每个 GPU 上计算临时统计数据，并应用 AllReduce 操作来生成整个批次 B 的方差 σ2 B。σ2 B 也被广播到所有 GPU。因此，每个 GPU 都具有在 B 上的所有训练样本上计算的统计数据。然后我们对每个训练样本执行归一化 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>m</mi></msub><mo>=</mo><mi>γ</mi><mfrac><mrow><msub><mi>x</mi><mi>m</mi></msub><mo>−</mo><msub><mtext>µ</mtext><mi>B</mi></msub></mrow><msqrt><mrow><msubsup><mi>σ</mi><mi>B</mi><mn>2</mn></msubsup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">y_m = γ\frac{x_m−µ_B}{\sqrt{σ^2_B+ϵ}} + β</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.6482em;vertical-align:-0.8296em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8186em;"><span style="top:-2.4761em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0198em;"><span class="svg-align" style="top:-3.4286em;"><span class="pstrut" style="height:3.4286em;"></span><span class="mord mtight" style="padding-left:1.19em;"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8051em;"><span style="top:-2.1607em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3393em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">ϵ</span></span></span><span style="top:-2.9918em;"><span class="pstrut" style="height:3.4286em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.5429em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.5429em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4368em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4103em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mtight">µ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8296em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span>，如 [28] 中所示。在反向操作中，我们同样应用 AllReduce 操作来收集来自所有 GPU 的信息，以便进行梯度计算。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://blog.sunlingzhang.com">神火不知灭</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://blog.sunlingzhang.com/2024/07/29/Paper/Path%20Aggregation%20Network%20for%20Instance%20Segmentation/">http://blog.sunlingzhang.com/2024/07/29/Paper/Path%20Aggregation%20Network%20for%20Instance%20Segmentation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CS/">CS</a><a class="post-meta__tags" href="/tags/paper/">paper</a><a class="post-meta__tags" href="/tags/CVPR-2018/">CVPR 2018</a></div><div class="post-share"><div class="social-share" data-image="/admin_head.jpg" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/08/05/Paper/Hierarchical%20Graph%20Pooling%20with%20Structure%20Learning/" title="Hierarchical Graph Pooling with Structure Learning"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">Hierarchical Graph Pooling with Structure Learning</div></div><div class="info-2"><div class="info-item-1">Hierarchical Graph Pooling with Structure Learning 会议: AAAI 2020(疑似撤稿) 论文地址：https://arxiv.org/abs/1911.05954 github: https://github.com/cszhangzhen/HGP-SL DGL开源库：https://github.com/dmlc/dgl/tree/master/examples/pytorch/hgp_sl [TOC] 摘要 图神经网络 (GNN) 将深度神经网络扩展到图结构数据，在许多图相关任务中取得了最先进的性能。然而，现有的 GNN 模型主要关注设计图卷积操作。图池化 (或下采样) 操作在分层表示学习中发挥着重要作用，通常被忽视。在这篇论文中，我们提出了一种新的图池化操作符，称为具有结构学习的分层图池化 (HGP-SL)，它可以集成到各种图神经网络架构中。HGP-SL 将图池化和结构学习集成到一个统一的模块中，以生成图的分层表示。具体来说，图池化操作根据我们定义的节点信息分数自适应地选择一组节点来形成一个诱导子图，用于后续层。为了保留...</div></div></div></a><a class="pagination-related" href="/2024/07/17/Work/%E6%A1%A9%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/" title="桩检测算法-自监督学习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">桩检测算法-自监督学习</div></div><div class="info-2"><div class="info-item-1">桩检测算法 总体流程  基于规则的桩检测  采用基于规则的方法检测桩，检测时不区分桩类别。这一步保证绝对的准确率（100%），较高的召回率（&gt;60%）。   生成桩检测数据集  使用规则检测的结果作为标注生成桩检测数据集【pile_v0.1】。   模型训练  使用YOLO算法在桩检测数据集上训练一个初步的桩检测模型【model_v0.1】。   自监督训练  使用训练好的模型对基于规则无法检测的图纸进行检测，将置信度较高的结果加入到训练集继续训练。     基于规则的桩检测   过滤干扰线  长度 &lt; 1500 直径 &lt; 1500 面积 &lt; 1500×1500 只保留直线，多段线，圆，椭圆，圆弧，实体，填充    桩检测  将去除干扰线后的实体导出图像求连通域 在每个连通域内进行桩检测，提高效率 圆检测：圆实体，多段线实体（只包含曲线，且构成360度），圆弧实体（构成360度），椭圆实体（长轴=短轴） 矩形检测：基于最近邻算法，遍历直线和它最近邻的直线是否组成矩形 交叉线检测：基于扫描线算法检测线是否交叉，判断交点是否在上述圆或矩形中心附近 填充检测：检...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2021/04/22/Paper/Learning%20Event-Based%20Motion%20Deblurring/" title="Learning Event-Based Motion Deblurring"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-22</div><div class="info-item-2">Learning Event-Based Motion Deblurring</div></div><div class="info-2"><div class="info-item-1">学习基于事件的运动去模糊 会议：CVPR 2020 地址：https://arxiv.org/abs/2004.05794 摘要 由于模糊过程中丢失了大量的运动信息，从运动模糊图像中恢复清晰的视频序列是一个高度不适定问题。然而，对于基于事件的相机，快速运动可以在高时间率上作为事件被捕捉，从而提出了探索有效解决方案的新机遇。在本文中，我们从基于事件的运动去模糊的序列表述开始，然后说明如何使用新颖的端到端深度架构来实现其优化。所提出的架构是一个卷积循环神经网络，有原则的整合了全局和局部尺度的视觉和时间知识。为了进一步改进(图像的)重建，我们提出了一种可微的定向事件过滤模块，可以有效地从事件流中提取丰富的先验边界。我们在合成的GoPro数据集和新引入的使用DAVIS240C相机捕获的大型数据集上进行了大量的实验。我们提出的方法达到了最先进的重建质量，并更好地处理现实世界的运动模糊。  1. 概述 运动模糊通常是由于现代相机传感器需要曝光时间，在此期间，场景被记录在不同的时间戳，并累积成平均(模糊)信号。这个过程的反问题被称为“去模糊”，它揭示了运动模糊图像背后的场景动态，并生成一系列复...</div></div></div></a><a class="pagination-related" href="/2025/04/22/Paper/ABINet_Autonomous,%20Bidirectional%20and%20Iterative%20Language%20Modeling%20for%20Scene%20Text%20Recognition/" title="ABINet: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-22</div><div class="info-item-2">ABINet: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition</div></div><div class="info-2"><div class="info-item-1">[TOC]  名称：Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition 论文：https://arxiv.org/abs/2103.06495 会议：AAAI2020 Github: https://github.com/FangShancheng/ABINet  ABINet（Attention-based Bidirectional Network）是一种用于场景文本识别（Scene Text Recognition, STR）的深度学习模型。它在处理复杂背景、噪声干扰以及弯曲或倾斜文本时表现出色。ABINet 的核心创新点是引入了 双向注意力机制 和 迭代优化策略 ，从而显著提升了文本识别的准确性和鲁棒性。 以下是 ABINet 的详细解析，包括其架构设计、工作原理、优势和实现细节。  1. ABINet 的背景 问题  自然场景中的文本通常具有复杂的形状（如弯曲、倾斜等），并且背景可能包含大量噪声。 传统的基于分类的方法...</div></div></div></a><a class="pagination-related" href="/2024/08/27/Paper/Rethinking%20Table%20Recognition%20using%20%20GNN/" title="Rethinking Table Recognitionusing Graph Neural Networks"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-27</div><div class="info-item-2">Rethinking Table Recognitionusing Graph Neural Networks</div></div><div class="info-2"><div class="info-item-1">Rethinking Table Recognitionusing Graph Neural Networks 会议: ICDAR 2019 论文地址：https://arxiv.org/abs/1905.13391 github: https://github.com/shahrukhqasim/TIES-2.0 [TOC] 摘要 文档结构分析，例如区域分割和表格识别，是文档处理中的复杂问题，并且是一个活跃的研究领域。深度学习在解决各种计算机视觉和机器学习问题方面的近期成功尚未反映在文档结构分析中，因为传统的神经网络不适合该问题的输入结构。本文提出了一种基于图网络的架构作为标准神经网络更好的替代方案来识别表格。我们主张图网络对于这些问题是一种更自然的选择，并探索了两种基于梯度的图神经网络。我们的提出的架构结合了卷积神经网络用于视觉特征提取以及图网络用于处理问题结构的好处。我们在实验上证明，与基线相比，我们的方法具有显著的优势。此外，我们还指出大规模数据集缺乏是结构分析领域深度学习研究的主要障碍，并提出了一个针对表格识别的新大规模合成数据集。最后，我们开源了我们的数据生成和图网络...</div></div></div></a><a class="pagination-related" href="/2024/08/21/Paper/GCN_Semi-Supervised%20Classification%20with%20Graph%20Convolutional%20Networks/" title="Semi-Supervised Classification with Graph Convolutional Networks"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-21</div><div class="info-item-2">Semi-Supervised Classification with Graph Convolutional Networks</div></div><div class="info-2"><div class="info-item-1">基于图卷积网络的半监督分类（GCN） 会议: ICLR 2017 论文地址：https://arxiv.org/abs/1609.02907 github: https://github.com/tkipf/pygcn  [TOC] 摘要 本文提出了一种可扩展的方法来处理图结构数据上的半监督学习，该方法基于一种高效的卷积神经网络变体，它直接在图上操作。本文通过局部一阶近似谱图卷积，优化我们的卷积架构的选择。我们的模型与图中边的数量线性相关，并且可以学习编码了图的局部结构和节点特征的隐藏层表示。我们在引用网络和知识图数据库上的一系列实验中展示了我们的方法相比其他相关方法具有显著优势。  1 简介 我们考虑在图（如文献引用网络）中对节点（如文档）进行分类的问题，其中仅有一小部分节点有标签。这个问题可以被看作基于图的半监督学习，通过某种显式的基于图的正则化形式来平滑（迁移）标签信息到图中，例如，在损失函数中使用图拉普拉斯正则化项：  式中，$L_0 $ 表示与图中带标签部分相关的监督损失。f(⋅)f(·)f(⋅)可以是类似于神经网络的可微函数，λλλ 是一个权重因子，XXX 是节点特征...</div></div></div></a><a class="pagination-related" href="/2023/06/07/JAVA/Mybatis/" title="Mybatis"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-07</div><div class="info-item-2">Mybatis</div></div><div class="info-2"><div class="info-item-1">环境：   JDK1.8   Mysql 5.7   maven 3.6.1   IDEA   回顾：   JDBC   Mysql   Java基础   Maven   Junit：单元测试   1. 简介 1.1 什么是Mybatis MyBatis 是一款优秀的持久层框架，它支持自定义 SQL、存储过程以及高级映射。MyBatis 免除了几乎所有的 JDBC 代码以及设置参数和获取结果集的工作。MyBatis 可以通过简单的 XML 或注解来配置和映射原始类型、接口和 Java POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。 框架：配置文件最好直接看官网 MyBatis官方文档：https://mybatis.org/mybatis-3/zh/index.html GitHub主页：https://github.com/mybatis/mybatis-3 Maven地址：https://mvnrepository.com/artifact/org.mybatis/mybatis 123456&lt;!-- https:/...</div></div></div></a><a class="pagination-related" href="/2026/02/04/Paper/YOLOv10/" title="YOLOv10技术文档"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-02-04</div><div class="info-item-2">YOLOv10技术文档</div></div><div class="info-2"><div class="info-item-1">[TOC] 核心摘要   端到端革命: 通过无NMS训练策略，彻底消除后处理，实现真正的端到端实时检测，显著降低延迟。   效率与精度: 通过轻量化模型设计，在保持高精度的同时，参数量和计算量显著降低，实现双赢。   硬件适配: 提供从边缘设备到高性能GPU服务器的全系列预训练模型，满足不同硬件的部署需求。   1. 概述 YOLOv10（You Only Look Once v10）是清华大学THU-MIG团队与Ultralytics合作开发的新一代实时端到端目标检测框架，于2024年5月正式发布。作为YOLO系列的里程碑式迭代，YOLOv10通过创新性的无NMS（非极大值抑制）训练策略和效率-精度驱动的模型设计，在保持高精度的同时显著降低了计算复杂度，实现了真正的端到端实时检测。 YOLOv10的核心突破在于：  完全消除NMS后处理：通过&quot;一致双分配&quot;策略，使模型在训练时利用多标签监督，推理时直接输出最终检测框，无需依赖后处理 全面优化计算路径：通过轻量化分类头、空间-通道解耦下采样、基于秩的块设计等技术，实现参数与计算量的显著降低 多硬件适配：从边缘设...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/admin_head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">神火不知灭</div><div class="author-info-description">日常记录学习用博客，仅用来练习使用</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">64</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">64</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/dollarser"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">技术笔记，日常记录</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E4%BA%8E%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E7%9A%84%E8%B7%AF%E5%BE%84%E8%81%9A%E5%90%88%E7%BD%91%E7%BB%9C"><span class="toc-number">1.</span> <span class="toc-text">用于实例分割的路径聚合网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">2.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%BC%95%E8%A8%80"><span class="toc-number">3.</span> <span class="toc-text">1. 引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">4.</span> <span class="toc-text">2. 相关工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6"><span class="toc-number">5.</span> <span class="toc-text">3 模型框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E8%87%AA%E5%BA%95%E5%90%91%E4%B8%8A%E7%9A%84%E8%B7%AF%E5%BE%84%E5%A2%9E%E5%BC%BA"><span class="toc-number">5.1.</span> <span class="toc-text">3.1. 自底向上的路径增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E8%87%AA%E9%80%82%E5%BA%94%E7%89%B9%E5%BE%81%E6%B1%A0%E5%8C%96"><span class="toc-number">5.2.</span> <span class="toc-text">3.2. 自适应特征池化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%85%A8%E8%BF%9E%E6%8E%A5%E8%9E%8D%E5%90%88"><span class="toc-number">5.3.</span> <span class="toc-text">3.3. 全连接融合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%AE%9E%E9%AA%8C"><span class="toc-number">6.</span> <span class="toc-text">4. 实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="toc-number">6.1.</span> <span class="toc-text">4.1. 实现细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%9C%A8-COCO-%E4%B8%8A%E7%9A%84%E5%AE%9E%E9%AA%8C"><span class="toc-number">6.2.</span> <span class="toc-text">4.2. 在 COCO 上的实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%9C%A8-Cityscapes-%E4%B8%8A%E7%9A%84%E5%AE%9E%E9%AA%8C"><span class="toc-number">6.3.</span> <span class="toc-text">4.3. 在 Cityscapes 上的实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%9C%A8-MVD-%E4%B8%8A%E7%9A%84%E5%AE%9E%E9%AA%8C"><span class="toc-number">6.4.</span> <span class="toc-text">4.4. 在 MVD 上的实验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E7%BB%93%E8%AE%BA"><span class="toc-number">7.</span> <span class="toc-text">5. 结论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-number">8.</span> <span class="toc-text">附录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Cityscapes-%E5%92%8C-MVD-%E7%9A%84%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82%E5%92%8C%E7%94%9F%E6%88%90%E9%94%9A%E7%82%B9%E7%9A%84%E7%AD%96%E7%95%A5"><span class="toc-number">8.1.</span> <span class="toc-text">A. Cityscapes 和 MVD 的训练细节和生成锚点的策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-%E5%AE%9E%E7%8E%B0-%E5%A4%9A-GPU-%E5%90%8C%E6%AD%A5%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E7%BB%86%E8%8A%82"><span class="toc-number">8.2.</span> <span class="toc-text">B. 实现 多 GPU 同步批归一化的细节</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/12/Python/Claude_Code/" title="Claude Code应用指南">Claude Code应用指南</a><time datetime="2026-02-12T09:30:00.000Z" title="Created 2026-02-12 17:30:00">2026-02-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/12/Python/MuSGD/" title="MuSGD优化器">MuSGD优化器</a><time datetime="2026-02-12T09:00:00.000Z" title="Created 2026-02-12 17:00:00">2026-02-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/11/Python/pytorch-%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" title="深度学习常用损失函数">深度学习常用损失函数</a><time datetime="2026-02-11T10:00:00.000Z" title="Created 2026-02-11 18:00:00">2026-02-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/10/Python/%E4%BD%8E%E7%A7%A9%E8%BF%91%E4%BC%BC/" title="矩阵相关">矩阵相关</a><time datetime="2026-02-10T10:00:00.000Z" title="Created 2026-02-10 18:00:00">2026-02-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/10/Python/Muon%E4%BC%98%E5%8C%96%E5%99%A8/" title="Muon优化器">Muon优化器</a><time datetime="2026-02-10T09:00:00.000Z" title="Created 2026-02-10 17:00:00">2026-02-10</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By 神火不知灭</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 8.1.1</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex@0.16.28/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex@0.16.28/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>